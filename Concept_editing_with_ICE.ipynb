{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-CTLRsgZA5a"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import sys\n",
        "\n",
        "import json\n",
        "import random\n",
        "from easyeditor import FTHyperParams, MEMITHyperParams, ROMEHyperParams, HyperParams\n",
        "from easyeditor import  ConceptEditor\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "import argparse\n",
        "\n",
        "models_implement = ['mistral','llama2chat','gpt2','gptj']\n",
        "model_names = ['./hugging_cache/Mistral-7B-v0.1','./hugging_cache/llama2-7b-chat','./hugging_cache/gpt2-xl','./hugging_cache/gpt-j-6B']\n",
        "\n",
        "def setup_seed(seed):\n",
        "    \"\"\"Sets up the random seed for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--edited_model', required=True, type=str, help=\"Model to edit (e.g., 'mistral', 'gpt2').\")\n",
        "    # Added 'ICE' as a recognized editing method\n",
        "    parser.add_argument('--editing_method', required=True, type=str,\n",
        "                        help=\"Editing method (e.g., 'FT', 'MEMIT', 'ROME', 'PROMPT', 'ICE').\")\n",
        "    parser.add_argument('--hparams_dir', required=True, type=str,\n",
        "                        help=\"Path to the hyperparameter configuration file.\")\n",
        "    parser.add_argument('--data_dir', default='./data', type=str,\n",
        "                        help=\"Directory containing the concept editing data.\")\n",
        "    parser.add_argument('--metrics_save_dir', default='./final_result_upload', type=str,\n",
        "                        help=\"Directory to save the metrics results.\")\n",
        "    parser.add_argument('--inter', action='store_true',\n",
        "                        help=\"Use inter-concept data if set, otherwise use intra-concept data.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if os.path.exists(args.metrics_save_dir) is False:\n",
        "        os.makedirs(args.metrics_save_dir)\n",
        "\n",
        "    if args.edited_model not in models_implement:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # --- ICE/PROMPT Handling Modification ---\n",
        "    # Non-parametric methods (ICE and PROMPT) typically use the base HyperParams\n",
        "    if args.editing_method == 'FT':\n",
        "        editing_hparams = FTHyperParams\n",
        "    elif args.editing_method == 'MEMIT':\n",
        "        editing_hparams = MEMITHyperParams\n",
        "    elif args.editing_method == 'ROME':\n",
        "        editing_hparams = ROMEHyperParams\n",
        "    elif args.editing_method in ['PROMPT', 'ICE']: # Now supporting 'ICE'\n",
        "        editing_hparams = HyperParams\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    # ----------------------------------------\n",
        "\n",
        "    if args.inter:\n",
        "        module = \"inter\"\n",
        "    else:\n",
        "        module = \"intra\"\n",
        "\n",
        "    # Load test data based on the model and whether it's inter- or intra-concept\n",
        "    data_path = os.path.join(args.data_dir, f\"final_{args.edited_model}_{module}.json\")\n",
        "    try:\n",
        "        test_data = json.load(open(data_path, 'r', encoding='utf-8'))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Data file not found at {data_path}. Please check data_dir and edited_model.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "    # Set random seed\n",
        "    setup_seed(42)\n",
        "\n",
        "    # Prepare data for the editor\n",
        "    prompts = [test_data_['prompt'] for test_data_ in test_data]\n",
        "    rephrase_prompts = [edit_data_['phrase_prompt'] for edit_data_ in test_data]\n",
        "    target_new = [edit_data_['target_new_desc'] for edit_data_ in test_data]\n",
        "    entity_prompts = [edit_data_['instance_prompt'] for edit_data_ in test_data]\n",
        "    in_locality_prompts = [edit_data_['locality_prompt'] for edit_data_ in test_data]\n",
        "    in_locality_ans = [edit_data_['locality_answer'] for edit_data_ in test_data]\n",
        "\n",
        "    locality_inputs = {\n",
        "        'neighborhood':{\n",
        "            'prompt': in_locality_prompts,\n",
        "            'ground_truth': in_locality_ans\n",
        "        }\n",
        "    }\n",
        "    instance_inputs = {\n",
        "        'instance':{\n",
        "            'prompt': entity_prompts\n",
        "        },\n",
        "    }\n",
        "\n",
        "    subject = [edit_data_['label'] for edit_data_ in test_data]\n",
        "    train_ds = None # Not used in this script, but kept for ConceptEditor signature\n",
        "\n",
        "    # --- ConceptEditor Initialization Modification ---\n",
        "    if args.editing_method in ['PROMPT', 'ICE']:\n",
        "        # For non-parametric methods, hparams is None, and model info is passed via prompt_hparams.\n",
        "        prompt_hparams = {\n",
        "            'model_name': model_names[models_implement.index(args.edited_model)],\n",
        "            'device': 0\n",
        "        }\n",
        "        hparams = None\n",
        "        editor = ConceptEditor.from_hparams(hparams, prompt_hparams)\n",
        "    else:\n",
        "        # For parametric methods (FT, ROME, MEMIT), load hparams from the directory.\n",
        "        hparams = editing_hparams.from_hparams(args.hparams_dir)\n",
        "        editor = ConceptEditor.from_hparams(hparams)\n",
        "    # ------------------------------------------------\n",
        "\n",
        "    print(f\"Starting concept editing with method: {args.editing_method}\")\n",
        "\n",
        "    # The ConceptEditor handles the specific logic for ICE/PROMPT vs. parametric methods\n",
        "    metrics, edited_model, _ = editor.edit(\n",
        "        prompts=prompts,\n",
        "        rephrase_prompts=rephrase_prompts,\n",
        "        target_new=target_new,\n",
        "        subject=subject,\n",
        "        train_ds=train_ds,\n",
        "        locality_inputs=locality_inputs,\n",
        "        instance_inputs=instance_inputs,\n",
        "        # concept_consistency = True, # Uncomment if needed\n",
        "        keep_original_weight=True\n",
        "    )\n",
        "\n",
        "    # Save and display results\n",
        "    save_path = os.path.join(args.metrics_save_dir, f'{args.editing_method}_results_{args.edited_model}_{module}.json')\n",
        "    json.dump(metrics, open(save_path, 'w'), indent=4)\n",
        "    print(f\"Results saved to: {save_path}\")\n",
        "\n",
        "\n",
        "    # --- Metric Calculation (Unchanged) ---\n",
        "    rewrite_acc = 0\n",
        "    rephrase_acc = 0\n",
        "    locality = 0\n",
        "    loc_list = []\n",
        "    instance = 0\n",
        "    port_list = []\n",
        "\n",
        "    with open(save_path, \"r\") as f:\n",
        "        result = json.load(f)\n",
        "\n",
        "    for i, item in enumerate(result):\n",
        "\n",
        "        case = item[\"post\"]\n",
        "        # print(case)\n",
        "        # Calculate Reliability and Generalization metrics post-edit\n",
        "        if not math.isnan(case[\"rewrite_acc\"][0]):\n",
        "            rewrite_acc = ((rewrite_acc * i) + np.mean(case[\"rewrite_acc\"][0])) / (i + 1)\n",
        "        else:\n",
        "            print(f'Warning: NaN rewrite_acc at index {i}: {case}')\n",
        "        if not math.isnan(case[\"rephrase_acc\"][0]):\n",
        "            rephrase_acc = ((rephrase_acc * i) + np.mean(case[\"rephrase_acc\"][0])) / (i + 1)\n",
        "        else:\n",
        "            print(f'Warning: NaN rephrase_acc at index {i}: {case}')\n",
        "\n",
        "        # Calculate Locality and Instance metrics post-edit\n",
        "        locality_ = 0\n",
        "        instance_ = 0\n",
        "        if \"locality\" in case.keys() and case[\"locality\"]:\n",
        "            if \"neighborhood_acc\" in case[\"locality\"].keys():\n",
        "                locality_ += np.mean(case[\"locality\"][\"neighborhood_acc\"])\n",
        "            if not math.isnan(locality_):\n",
        "                loc_list.append(locality_)\n",
        "\n",
        "        if \"instance\" in case.keys() and case[\"instance\"]:\n",
        "            if \"instance_change\" in case[\"instance\"].keys():\n",
        "                # Handling the -1 to 1 mapping for instance_change\n",
        "                if case[\"instance\"][\"instance_change\"] == -1:\n",
        "                    case[\"instance\"][\"instance_change\"] = 1\n",
        "                instance_ += np.mean(case[\"instance\"][\"instance_change\"])\n",
        "            if not math.isnan(instance_):\n",
        "                port_list.append(instance_)\n",
        "\n",
        "    locality = np.mean(loc_list) if loc_list else 0\n",
        "    instance = np.mean(port_list) if port_list else 0\n",
        "    sub1 = instance # Post-edit instance change\n",
        "\n",
        "    # Print Post-Edit Metrics\n",
        "    print(f'\\n--- Post-Edit Metrics for {args.editing_method} on {args.edited_model} ---')\n",
        "    print(f'dir: {args.metrics_save_dir}')\n",
        "    print(f'Reliability (Rewrite Acc): {rewrite_acc*100:.2f}%')\n",
        "    print(f'Generalization (Rephrase Acc): {rephrase_acc*100:.2f}%')\n",
        "    print(f'Locality (Neighborhood Acc): {locality*100:.2f}%')\n",
        "\n",
        "    # Calculate Pre-Edit Instance Change for Comparison (Portability)\n",
        "    port_list = []\n",
        "    for i, item in enumerate(result):\n",
        "        case = item[\"pre\"]\n",
        "        instance_ = 0\n",
        "        if \"instance\" in case.keys() and case[\"instance\"]:\n",
        "            if \"instance_change\" in case[\"instance\"].keys():\n",
        "                if case[\"instance\"][\"instance_change\"] == -1:\n",
        "                    case[\"instance\"][\"instance_change\"] = 1\n",
        "                instance_ += np.mean(case[\"instance\"][\"instance_change\"])\n",
        "            if not math.isnan(instance_):\n",
        "                port_list.append(instance_)\n",
        "    instance = np.mean(port_list) if port_list else 0\n",
        "    sub2 = instance # Pre-edit instance change\n",
        "\n",
        "    # Print Portability Metric\n",
        "    # Portability (instance_change) is typically calculated as pre-edit change minus post-edit change.\n",
        "    print('Portability (Instance Change Drop): ', end='')\n",
        "    print(f'{(sub2-sub1)*100:.2f}%')\n"
      ]
    }
  ]
}