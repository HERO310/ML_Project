{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYQkPvUffIKE"
      },
      "outputs": [],
      "source": [
        "pip install -q transformers peft bitsandbytes datasets accelerate torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU4p_9wDstqZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
        "DATASET_PATH = \"path/to/your/wikidata_dataset.jsonl\"\n",
        "SAVE_PATH = \"./llama-3.2-wikidata-lora-manual\"\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 2\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "print(\"Loading model and tokenizer...\")\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "def generate_input_output_pair(prompts, target_responses, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Takes lists of prompts and responses and prepares them for SFT\n",
        "    using the masking and shifting logic from your reference code.\n",
        "    This function is designed to be used inside the collate_fn.\n",
        "    \"\"\"\n",
        "    \n",
        "    full_response_text = [\n",
        "        prompt + \" \" + target + tokenizer.eos_token\n",
        "        for prompt, target in zip(prompts, target_responses)\n",
        "    ]\n",
        "\n",
        "    \n",
        "    input_ids_tokenized = tokenizer(\n",
        "        full_response_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\", \n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    \n",
        "    labels_tokenized = tokenizer(\n",
        "        [\" \" + response + tokenizer.eos_token for response in target_responses],\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\", \n",
        "        max_length=max_length,\n",
        "        truncation=True\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    \n",
        "    labels_tokenized_fixed = torch.where(\n",
        "        labels_tokenized != tokenizer.pad_token_id,\n",
        "        labels_tokenized,\n",
        "        -100\n",
        "    )\n",
        "\n",
        "    labels_tokenized_fixed[:, :-1] = labels_tokenized_fixed[:, 1:]\n",
        "\n",
        "   \n",
        "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
        "    labels_tokenized_right_shifted = labels_tokenized_fixed[:, 1:]\n",
        "\n",
        "   \n",
        "    attention_mask = (input_ids_tokenized_left_shifted != tokenizer.pad_token_id).int()\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
        "        \"labels\": labels_tokenized_right_shifted,\n",
        "        \"attention_mask\": attention_mask,\n",
        "    }\n",
        "\n",
        "\n",
        "def collate_and_process(batch):\n",
        "    prompts = [item['prompt'] for item in batch]\n",
        "    target_responses = [item['target_new'] for item in batch]\n",
        "\n",
        "\n",
        "    processed_batch = generate_input_output_pair(prompts, target_responses, tokenizer, MAX_LENGTH)\n",
        "\n",
        "    return processed_batch\n",
        "\n",
        "\n",
        "print(\"Loading and processing dataset...\")\n",
        "\n",
        "\n",
        "try:\n",
        "    raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Dataset file not found at {DATASET_PATH}. Using dummy data.\")\n",
        "    dummy_data = [\n",
        "        {\"subject\": \"Leo Arons\", \"prompt\": \"The place of death of Leo Arons is\", \"target_new\": \"Berlin\"},\n",
        "        {\"subject\": \"Mount Everest\", \"prompt\": \"The height of Mount Everest is\", \"target_new\": \"8,848.86 m\"},\n",
        "        {\"subject\": \"Python (programming language)\", \"prompt\": \"The creator of Python is\", \"target_new\": \"Guido van Rossum\"}\n",
        "    ]\n",
        "    raw_dataset = dummy_data\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    raw_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_and_process,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Setting up optimizer and loss function...\")\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "print(\"Starting manual training loop...\")\n",
        "model.train() \n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1} / {NUM_EPOCHS} ---\")\n",
        "\n",
        "    total_epoch_loss = 0\n",
        "\n",
        "    \n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "\n",
        "    \n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    \n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "\n",
        "        flat_logits = logits.view(-1, model.config.vocab_size)\n",
        "\n",
        "    \n",
        "        flat_labels = labels.view(-1)\n",
        "\n",
        "    \n",
        "        loss = loss_fn(flat_logits, flat_labels)\n",
        "\n",
        "    \n",
        "        loss.backward()\n",
        "\n",
        "    \n",
        "        optimizer.step()\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "\n",
        "    \n",
        "    avg_epoch_loss = total_epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "\n",
        "print(f\"Saving LoRA adapters to {SAVE_PATH}...\")\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "\n",
        "print(\"Finetuning finished successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCu1n1YuuARQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm \n",
        "import json\n",
        "\n",
        "\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
        "DATASET_PATH = \"path/to/your/wikidata_dataset.jsonl\"\n",
        "SAVE_PATH = \"./llama-3.2-wikidata-lora-manual\"\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 2\n",
        "MAX_LENGTH = 512 \n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "print(\"Loading model and tokenizer...\")\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CA_USAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "def generate_input_output_pair(prompts, target_responses, tokenizer, max_length):\n",
        "    full_response_text = [\n",
        "        prompt + \" \" + target + tokenizer.eos_token\n",
        "        for prompt, target in zip(prompts, target_responses)\n",
        "    ]\n",
        "\n",
        "    input_ids_tokenized = tokenizer(\n",
        "        full_response_text,\n",
        "        add_special_tokens=False, \n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    labels_tokenized = tokenizer(\n",
        "        [\" \" + response + tokenizer.eos_token for response in target_responses],\n",
        "        add_special_tokens=False, \n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        max_length=input_ids_tokenized.shape[1],\n",
        "        truncation=True\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    labels_tokenized_fixed = torch.where(\n",
        "        labels_tokenized != tokenizer.pad_token_id,\n",
        "        labels_tokenized,\n",
        "        -100\n",
        "    )\n",
        "\n",
        "    labels_tokenized_fixed[:, :-1] = labels_tokenized_fixed[:, 1:]\n",
        "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
        "    labels_tokenized_right_shifted = labels_tokenized_fixed[:, 1:]\n",
        "    attention_mask = (input_ids_tokenized_left_shifted != tokenizer.pad_token_id).int()\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
        "        \"labels\": labels_tokenized_right_shifted,\n",
        "        \"attention_mask\": attention_mask,\n",
        "    }\n",
        "\n",
        "\n",
        "def collate_and_process(batch):\n",
        "    prompts = [item['prompt'] for item in batch]\n",
        "    target_responses = [item['target_new'] for item in batch]\n",
        "\n",
        "    processed_batch = generate_input_output_pair(prompts, target_responses, tokenizer, MAX_LENGTH)\n",
        "\n",
        "    return processed_batch\n",
        "\n",
        "print(\"Loading and processing dataset...\")\n",
        "try:\n",
        "    raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Dataset file not found at {DATASET_PATH}. Using dummy data.\")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    raw_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_and_process,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Setting up optimizer and loss function...\")\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Starting manual training loop...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1} / {NUM_EPOCHS} ---\")\n",
        "    total_epoch_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        flat_logits = logits.view(-1, model.config.vocab_size)\n",
        "        flat_labels = labels.view(-1)\n",
        "\n",
        "        loss = loss_fn(flat_logits, flat_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = total_epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "print(f\"Saving LoRA adapters to {SAVE_PATH}...\")\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(\"Finetuning finished successfully.\")\n",
        "\n",
        "print(\"\\n--- Starting Evaluation ---\")\n",
        "print(\"Loading base model and fine-tuned LoRA adapters...\")\n",
        "\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, SAVE_PATH)\n",
        "model.eval() \n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(SAVE_PATH)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\" \n",
        "\n",
        "\n",
        "\n",
        "def generate_response(prompt, model, tokenizer, max_new_tokens=50):\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if decoded_output.startswith(prompt):\n",
        "        generated_text = decoded_output[len(prompt):]\n",
        "    else:\n",
        "      \n",
        "        input_token_len = inputs[\"input_ids\"].shape[1]\n",
        "        generated_text = tokenizer.decode(outputs[0][input_token_len:], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text.strip()\n",
        "\n",
        "def check_answer(generation, ground_truth_lists):\n",
        "   \n",
        "    generation_low = generation.lower().strip()\n",
        "    if not generation_low:\n",
        "        return False\n",
        "\n",
        "    for alias_list in ground_truth_lists:\n",
        "        for alias in alias_list:\n",
        "            if alias.lower().strip() in generation_low:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "print(f\"Loading evaluation data from {DATASET_PATH}...\")\n",
        "try:\n",
        "    eval_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Using dummy data for evaluation.\")\n",
        "    eval_dataset = raw_dataset \n",
        "\n",
        "\n",
        "print(\"Running evaluation loop...\")\n",
        "\n",
        "\n",
        "reliability_correct = 0\n",
        "reliability_total = 0\n",
        "portability_correct = 0\n",
        "portability_total = 0\n",
        "locality_correct = 0\n",
        "locality_total = 0\n",
        "\n",
        "for item in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
        "\n",
        "\n",
        "    target = item['target_new']\n",
        "    ground_truth_reliability = [[target]] \n",
        "\n",
        "    \n",
        "    gen_main = generate_response(item['prompt'], model, tokenizer)\n",
        "    if check_answer(gen_main, ground_truth_reliability):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    \n",
        "    gen_rephrase = generate_response(item['rephrase'], model, tokenizer)\n",
        "    if check_answer(gen_rephrase, ground_truth_reliability):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    \n",
        "    if 'portability' in item and 'Reasoning' in item['portability']:\n",
        "        for query in item['portability']['Reasoning']:\n",
        "            gen_portability = generate_response(query['prompt'], model, tokenizer)\n",
        "            if check_answer(gen_portability, query['ground_truth']):\n",
        "                portability_correct += 1\n",
        "            portability_total += 1\n",
        "\n",
        "    \n",
        "    if 'locality' in item and 'Relation_Specificity' in item['locality']:\n",
        "        for query in item['locality']['Relation_Specificity']:\n",
        "            gen_locality = generate_response(query['prompt'], model, tokenizer)\n",
        "            if check_answer(gen_locality, query['ground_truth']):\n",
        "                locality_correct += 1\n",
        "            locality_total += 1\n",
        "\n",
        "\n",
        "print(\"\\n--- üìä Evaluation Results ---\")\n",
        "\n",
        "\n",
        "p_reliability = (reliability_correct / reliability_total * 100) if reliability_total > 0 else 0\n",
        "p_portability = (portability_correct / portability_total * 100) if portability_total > 0 else 0\n",
        "p_locality = (locality_correct / locality_total * 100) if locality_total > 0 else 0\n",
        "\n",
        "\n",
        "print(f\"1Ô∏è‚É£ Reliability (Edit Success):\")\n",
        "print(f\"   - Score:    {p_reliability:.2f}%\")\n",
        "print(f\"   - Correct:  {reliability_correct}\")\n",
        "print(f\"   - Total:    {reliability_total} (Main prompt + Rephrase for each item)\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"2Ô∏è‚É£ Portability (Reasoning Generalization):\")\n",
        "print(f\"   - Score:    {p_portability:.2f}%\")\n",
        "print(f\"   - Correct:  {portability_correct}\")\n",
        "print(f\"   - Total:    {portability_total} (All reasoning queries)\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"3Ô∏è‚É£ Locality (Unrelated Fact Preservation):\")\n",
        "print(f\"   - Score:    {p_locality:.2f}%\")\n",
        "print(f\"   - Correct:  {locality_correct}\")\n",
        "print(f\"   - Total:    {locality_total} (All relation specificity queries)\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U56O67NZvQLk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm \n",
        "import json\n",
        "\n",
        "\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
        "DATASET_PATH = \"path/to/your/wikidata_dataset.jsonl\"\n",
        "SAVE_PATH = \"./llama-3.2-wikidata-lora-manual\"\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 2\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "print(\"Loading model and tokenizer...\")\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "\n",
        "def collate_and_process(batch):\n",
        "    prompts = [item['prompt'] for item in batch]\n",
        "    target_responses = [item['target_new'] for item in batch]\n",
        "\n",
        "\n",
        "    chat_histories = [\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": p},\n",
        "            {\"role\": \"assistant\", \"content\": t}\n",
        "        ]\n",
        "        for p, t in zip(prompts, target_responses)\n",
        "    ]\n",
        "\n",
        "    tokenized_full = tokenizer.apply_chat_template(\n",
        "        chat_histories,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = tokenized_full[\"input_ids\"]\n",
        "    attention_mask = tokenized_full[\"attention_mask\"]\n",
        "\n",
        "    \n",
        "    labels = input_ids.clone()\n",
        "\n",
        "    \n",
        "    prompts_only = [\n",
        "        [{\"role\": \"user\", \"content\": p}] for p in prompts\n",
        "    ]\n",
        "\n",
        "    \n",
        "    tokenized_prompts = tokenizer.apply_chat_template(\n",
        "        prompts_only,\n",
        "        add_generation_prompt=True,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "    \n",
        "    for i in range(len(labels)):\n",
        "        prompt_len = len(tokenized_prompts[\"input_ids\"][i])\n",
        "        labels[i, :prompt_len] = -100 \n",
        "\n",
        "    \n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids.to(device),\n",
        "        \"labels\": labels.to(device),\n",
        "        \"attention_mask\": attention_mask.to(device),\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Loading and processing dataset...\")\n",
        "try:\n",
        "   \n",
        "    raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Dataset file not found at {DATASET_PATH}. Using dummy data.\")\n",
        "   \n",
        "train_dataloader = DataLoader(\n",
        "    raw_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_and_process,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Setting up optimizer and loss function...\")\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = nn.CrossEntropyLoss() \n",
        "\n",
        "\n",
        "print(\"Starting manual training loop...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1} / {NUM_EPOCHS} ---\")\n",
        "    total_epoch_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "        \n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        \n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        \n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = total_epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "\n",
        "print(f\"Saving LoRA adapters to {SAVE_PATH}...\")\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(\"Finetuning finished successfully.\")\n",
        "print(\"\\n--- Starting Evaluation ---\")\n",
        "print(\"Loading base model and fine-tuned LoRA adapters...\")\n",
        "\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, SAVE_PATH)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(SAVE_PATH)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "\n",
        "\n",
        "def generate_response(prompt, model, tokenizer, max_new_tokens=50):\n",
        "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        chat,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "  \n",
        "    prompt_token_length = inputs.shape[1]\n",
        "    response_tokens = outputs[0][prompt_token_length:]\n",
        "    decoded_output = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return decoded_output.strip()\n",
        "\n",
        "def check_answer(generation, ground_truth_lists):\n",
        " \n",
        "    generation_low = generation.lower().strip()\n",
        "    if not generation_low:\n",
        "        return False\n",
        "\n",
        "    for alias_list in ground_truth_lists:\n",
        "        for alias in alias_list:\n",
        "            if alias.lower().strip() in generation_low:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "\n",
        "print(f\"Loading evaluation data from {DATASET_PATH}...\")\n",
        "try:\n",
        "    eval_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Using dummy data for evaluation.\")\n",
        "    eval_dataset = raw_dataset \n",
        "\n",
        "\n",
        "print(\"Running evaluation loop...\")\n",
        "\n",
        "\n",
        "reliability_correct = 0\n",
        "reliability_total = 0\n",
        "portability_correct = 0\n",
        "portability_total = 0\n",
        "locality_correct = 0\n",
        "locality_total = 0\n",
        "\n",
        "for item in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
        "\n",
        "  \n",
        "    target = item['target_new']\n",
        "    ground_truth_reliability = [[target]] \n",
        "\n",
        "    gen_main = generate_response(item['prompt'], model, tokenizer)\n",
        "    if check_answer(gen_main, ground_truth_reliability):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    gen_rephrase = generate_response(item['rephrase'], model, tokenizer)\n",
        "    if check_answer(gen_rephrase, ground_truth_reliability):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    \n",
        "    if 'portability' in item and 'Reasoning' in item['portability']:\n",
        "        for query in item['portability']['Reasoning']:\n",
        "            gen_portability = generate_response(query['prompt'], model, tokenizer)\n",
        "            if check_answer(gen_portability, query['ground_truth']):\n",
        "                portability_correct += 1\n",
        "            portability_total += 1\n",
        "\n",
        "\n",
        "    if 'locality' in item and 'locality' in item and 'Relation_Specificity' in item['locality']:\n",
        "        for query in item['locality']['Relation_Specificity']:\n",
        "            gen_locality = generate_response(query['prompt'], model, tokenizer)\n",
        "            if check_answer(gen_locality, query['ground_truth']):\n",
        "                locality_correct += 1\n",
        "            locality_total += 1\n",
        "\n",
        "\n",
        "print(\"\\n--- üìä Evaluation Results ---\")\n",
        "\n",
        "p_reliability = (reliability_correct / reliability_total * 100) if reliability_total > 0 else 0\n",
        "p_portability = (portability_correct / portability_total * 100) if portability_total > 0 else 0\n",
        "p_locality = (locality_correct / locality_total * 100) if locality_total > 0 else 0\n",
        "\n",
        "print(f\"1Ô∏è‚É£ Reliability (Edit Success):\")\n",
        "print(f\"   - Score:    {p_reliability:.2f}%\")\n",
        "print(f\"   - Correct:  {reliability_correct}\")\n",
        "print(f\"   - Total:    {reliability_total} (Main prompt + Rephrase for each item)\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"2Ô∏è‚É£ Portability (Reasoning Generalization):\")\n",
        "print(f\"   - Score:    {p_portability:.2f}%\")\n",
        "print(f\"   - Correct:  {portability_correct}\")\n",
        "print(f\"   - Total:    {portability_total} (All reasoning queries)\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"3Ô∏è‚É£ Locality (Unrelated Fact Preservation):\")\n",
        "print(f\"   - Score:    {p_locality:.2f}%\")\n",
        "print(f\"   - Correct:  {locality_correct}\")\n",
        "print(f\"   - Total:    {locality_total} (All relation specificity queries)\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_1lhFc-EhMT"
      },
      "source": [
        "#using ice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eKg8xs8Eivx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, default_collate\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm # For a nice progress bar\n",
        "import json\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
        "DATASET_PATH = \"path/to/your/wikidata_dataset.jsonl\"\n",
        "SAVE_PATH = \"./llama-3.2-wikidata-ice-lora\"\n",
        "\n",
        "# Training Hyperparameters\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 2 # Keep small for this complex batch logic\n",
        "MAX_LENGTH = 512 # Max sequence length for training\n",
        "MAX_NEW_TOKENS_SAMPLING = 50 # Length of x_c to sample - NO LONGER USED\n",
        "MAX_NEW_TOKENS_EVAL = 50 # Length for eval\n",
        "LAMBDA_ICE = 0.5 # Hyperparameter to balance L_FT and L_ICE\n",
        "\n",
        "# --- 2. Setup Device ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 3. Load Model, Tokenizer, and LoRA ---\n",
        "print(\"Loading model and tokenizer...\")\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# --- 4. Load Dataset and DataLoader ---\n",
        "print(\"Loading and processing dataset...\")\n",
        "try:\n",
        "    # We load the raw JSON data here\n",
        "    raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Dataset file not found at {DATASET_PATH}. Using dummy data.\")\n",
        "    dummy_data = [json.loads(r'{\"subject\":\"Leo Arons\",\"prompt\":\"The place of death of Leo Arons is\",\"target_new\":\"Berlin\",\"portability\":{\"Logical_Generalization\":[{\"prompt\":\"Is Leo Arons still alive?\",\"ground_truth\":[[\"no\"],[\"incorrect\"],[\"false\"],[\"is not alive\"],[\"is dead\"]]}],\"Reasoning\":[{\"prompt\":\"The name of the head of government of the place of death of Leo Arons is\",\"ground_truth\":[[\"Kai Wegner\",\"Kai Peter Wegner\"]]},{\"prompt\":\"The official language of the place of death of Leo Arons is\",\"ground_truth\":[[\"German\",\"German language\",\"de\"]]},{\"prompt\":\"The name of the continent which the place of death of Leo Arons is part of is\",\"ground_truth\":[[\"Europe\",\"European continent\",\"Old Continent\"]]}],\"Subject_Aliasing\":[{\"prompt\":\"The place of death of Martin Leo Arons is\",\"ground_truth\":[[\"Berlin\",\"Berlin, Germany\",\"Berlin (Germany)\",\"DE-BE\"]]}]},\"locality\":{\"Relation_Specificity\":[{\"prompt\":\"The name of the father of Leo Arons is\",\"ground_truth\":[[\"Albert Arons\"]]},{\"prompt\":\"The names of the siblings of Leo Arons are\",\"ground_truth\":[[\"Paul Arons\"]]},{\"prompt\":\"The gender of Leo Arons is\",\"ground_truth\":[[\"male\",\"man\",\"male person\",\"male human\",\"male gender\",\"guy\",\"human male\",\"sterner sex\",\"masc\",\"men\",\"boy\",\"boys\",\"male character\"]]},{\"prompt\":\"The place of birth of Leo Arons is\",\"ground_truth\":[[\"Berlin\",\"Berlin, Germany\",\"Berlin (Germany)\",\"DE-BE\"]]},{\"prompt\":\"The name of the country of citizenship of Leo Arons is\",\"ground_truth\":[[\"Germany\",\"Federal Republic of Germany\",\"Deutschland\",\"GER\",\"BR Deutschland\",\"DE\",\"BRD\",\"Bundesrepublik Deutschland\",\"de\",\"GFR\"]]},{\"prompt\":\"The name of the alma mater of Leo Arons is\",\"ground_truth\":[[\"University of Strasbourg\",\"Universit\\u00e9 de Strasbourg\",\"unistra.fr\"]]},{\"prompt\":\"The occupation of Leo Arons is\",\"ground_truth\":[[\"politician\",\"political leader\",\"political figure\",\"polit.\",\"pol\"],[\"physicist\"],[\"inventor\"],[\"university teacher\",\"lecturer\",\"college professor\",\"college lecturer\",\"professor\",\"university lecturer\",\"tutor\",\"university tutor\",\"college teacher\",\"university professor\",\"teaching-focused lecturer\"]]},{\"prompt\":\"The name of the employer of Leo Arons is\",\"ground_truth\":[[\"Humboldt University of Berlin\",\"Humboldt-Universit\\u00e4t zu Berlin\",\"Universitas Humboldtiana Berolinensis\",\"University of Berlin\",\"HU Berlin\",\"Universit\\u00e4t zu Berlin\",\"Humboldt University\"]]},{\"prompt\":\"The name of the field of work of Leo Arons is\",\"ground_truth\":[[\"experimental physics\"]]}]},\"rephrase\":\"Leo Arons' place of death is\",\"context\":[\"Leo Arons passed away in Berlin.\",\"Berlin is the city where Leo Arons died.\",\"Leo Arons's place of death is Berlin.\",\"The location of Leo Arons's death is Berlin.\",\"Berlin is where Leo Arons met his end.\"]}')]\n",
        "    raw_dataset = dummy_data\n",
        "\n",
        "# The DataLoader just batches the raw dicts\n",
        "train_dataloader = DataLoader(\n",
        "    raw_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=default_collate, # Standard batching\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# --- 5. Setup Optimizer ---\n",
        "print(\"Setting up optimizer...\")\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# --- NEW 6. ICE Loss Function (Corrected) ---\n",
        "def calculate_ice_loss(model, tokenizer, batch, device, max_length, lambda_ice):\n",
        "    \"\"\"\n",
        "    Calculates the combined (L_FT + Œª * L_ICE) loss for a single batch,\n",
        "    as defined in the ICE paper (Equation 6).\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 0. Prepare Batch Data ---\n",
        "    prompts = batch['prompt']\n",
        "    targets = batch['target_new']\n",
        "    contexts = [\" \".join(c_list) for c_list in batch['context']]\n",
        "\n",
        "    # ----- 1. L_FT (Standard Fine-Tuning Loss) -----\n",
        "    # L_FT = CE(p_Œ∏(x* | q))\n",
        "\n",
        "    ft_chats = [\n",
        "        [{\"role\": \"user\", \"content\": p}, {\"role\": \"assistant\", \"content\": t}]\n",
        "        for p, t in zip(prompts, targets)\n",
        "    ]\n",
        "\n",
        "    # Tokenize for L_FT\n",
        "    ft_inputs = tokenizer.apply_chat_template(\n",
        "        ft_chats,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    ft_labels = ft_inputs[\"input_ids\"].clone()\n",
        "    ft_labels[ft_labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    # Create prompt-only tokens to find masking length\n",
        "    prompts_for_masking = [\n",
        "        [{\"role\": \"user\", \"content\": p}] for p in prompts\n",
        "    ]\n",
        "    tokenized_prompts_only = tokenizer.apply_chat_template(\n",
        "        prompts_for_masking,\n",
        "        add_generation_prompt=True,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    # Mask out all prompt tokens in the labels\n",
        "    for i in range(len(ft_labels)):\n",
        "        prompt_len = len(tokenized_prompts_only[\"input_ids\"][i])\n",
        "        ft_labels[i, :prompt_len] = -100\n",
        "\n",
        "    # Calculate L_FT\n",
        "    outputs_ft = model(\n",
        "        input_ids=ft_inputs[\"input_ids\"],\n",
        "        attention_mask=ft_inputs[\"attention_mask\"],\n",
        "        labels=ft_labels\n",
        "    )\n",
        "    loss_ft = outputs_ft.loss\n",
        "\n",
        "    # ----- 2. L_ICE (In-Context Editing Loss) -----\n",
        "    # L_ICE = KL( p_Œ∏(x | [c, q]) || p_Œ∏(x | q) )\n",
        "\n",
        "    # Build batches for [q] and [c, q]\n",
        "    base_chats = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "    context_chats = [[{\"role\": \"user\", \"content\": f\"{c} {p}\"}] for c, p in zip(contexts, prompts)]\n",
        "\n",
        "    # Tokenize both, ensuring they are compatible\n",
        "    # `add_generation_prompt=True` adds the \"...assistant\\n\\n\" part\n",
        "    base_inputs = tokenizer.apply_chat_template(\n",
        "        base_chats,\n",
        "        add_generation_prompt=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    context_inputs = tokenizer.apply_chat_template(\n",
        "        context_chats,\n",
        "        add_generation_prompt=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    # Get logits for p_Œ∏(x | q) (with gradient)\n",
        "    logits_base = model(**base_inputs).logits\n",
        "\n",
        "    # Get logits for p_Œ∏(x | [c, q]) (no gradient - this is the target dist)\n",
        "    with torch.no_grad():\n",
        "        logits_context = model(**context_inputs).logits\n",
        "\n",
        "    # Calculate KL divergence\n",
        "    # We use log_softmax for the \"input\" and softmax for the \"target\"\n",
        "    log_p = torch.nn.functional.log_softmax(logits_base, dim=-1)\n",
        "    q = torch.nn.functional.softmax(logits_context, dim=-1)\n",
        "\n",
        "    # nn.KLDivLoss(reduction='none') returns element-wise: q * (log(q) - log_p)\n",
        "    # We sum over the vocab dimension\n",
        "    kl_per_token = nn.KLDivLoss(reduction='none')(log_p, q).sum(dim=-1) # Shape: (batch_size, seq_len)\n",
        "\n",
        "    # ----- 3. Mask L_ICE -----\n",
        "    # We must mask out the prompt tokens from the KL loss, just like L_FT\n",
        "    # We also mask out padding tokens.\n",
        "    kl_attention_mask = base_inputs.attention_mask.clone()\n",
        "\n",
        "    # We need prompt-only tokens for the base_chats\n",
        "    base_prompts_for_masking = [\n",
        "        [{\"role\": \"user\", \"content\": p}] for p in prompts\n",
        "    ]\n",
        "    tokenized_base_prompts_only = tokenizer.apply_chat_template(\n",
        "        base_prompts_for_masking,\n",
        "        add_generation_prompt=True,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    for i in range(len(kl_attention_mask)):\n",
        "        prompt_len = len(tokenized_base_prompts_only[\"input_ids\"][i])\n",
        "        kl_attention_mask[i, :prompt_len] = 0 # Mask out prompt\n",
        "\n",
        "    # Apply mask and calculate mean KL\n",
        "    masked_kl = kl_per_token * kl_attention_mask\n",
        "    loss_ice = masked_kl.sum() / kl_attention_mask.sum()\n",
        "\n",
        "    # ----- 4. Combine -----\n",
        "    loss_total = loss_ft + lambda_ice * loss_ice\n",
        "\n",
        "    return loss_total\n",
        "\n",
        "\n",
        "# --- 7. The Manual Training Loop (with ICE Loss) ---\n",
        "print(\"Starting manual training loop with ICE loss...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1} / {NUM_EPOCHS} ---\")\n",
        "    total_epoch_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # --- Call the custom loss function ---\n",
        "        loss = calculate_ice_loss(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            batch,\n",
        "            device,\n",
        "            MAX_LENGTH,\n",
        "            LAMBDA_ICE  # Pass the lambda hyperparam\n",
        "        )\n",
        "\n",
        "        # --- Standard backward pass (Algorithm 1, Step 6) ---\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = total_epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "# --- 7. Save the Final Model ---\n",
        "print(f\"Saving LoRA adapters to {SAVE_PATH}...\")\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(\"Finetuning finished successfully.\")\n",
        "\n",
        "# ======================================================================\n",
        "# --- 8. Load Fine-Tuned Model for Evaluation ---\n",
        "# (This section is identical to the previous file)\n",
        "# ======================================================================\n",
        "print(\"\\n--- Starting Evaluation ---\")\n",
        "print(\"Loading base model and fine-tuned LoRA adapters...\")\n",
        "\n",
        "# Load the base model again\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load the PeftModel\n",
        "model = PeftModel.from_pretrained(base_model, SAVE_PATH)\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "# Re-load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(SAVE_PATH)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\" # Use left-padding for generation\n",
        "\n",
        "# --- 9. Define Evaluation Helpers ---\n",
        "def generate_response(prompt, model, tokenizer, max_new_tokens=50):\n",
        "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        chat,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    prompt_token_length = inputs.shape[1]\n",
        "    response_tokens = outputs[0][prompt_token_length:]\n",
        "    decoded_output = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return decoded_output.strip()\n",
        "\n",
        "def check_answer(generation, ground_truth_lists):\n",
        "    generation_low = generation.lower().strip()\n",
        "    if not generation_low:\n",
        "        return False\n",
        "    for alias_list in ground_truth_lists:\n",
        "        for alias in alias_list:\n",
        "            if alias.lower().strip() in generation_low:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# --- 10. Load Evaluation Data ---\n",
        "print(f\"Loading evaluation data from {DATASET_PATH}...\")\n",
        "try:\n",
        "    eval_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Using dummy data for evaluation.\")\n",
        "    eval_dataset = raw_dataset # Use the dummy data from training\n",
        "\n",
        "# --- 11. Run Evaluation Loop ---\n",
        "print(\"Running evaluation loop...\")\n",
        "reliability_correct, reliability_total = 0, 0\n",
        "portability_correct, portability_total = 0, 0\n",
        "locality_correct, locality_total = 0, 0\n",
        "\n",
        "for item in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
        "\n",
        "    # 1Ô∏è‚É£ Reliability\n",
        "    target = item['target_new']\n",
        "    ground_truth_reliability = [[target]]\n",
        "\n",
        "    gen_main = generate_response(item['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_main, ground_truth_reliability):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    gen_rephrase = generate_response(item['rephrase'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_rephrase, ground_truth_reliability):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    # 2Ô∏è‚É£ Portability\n",
        "    if 'portability' in item and 'Reasoning' in item['portability']:\n",
        "        for query in item['portability']['Reasoning']:\n",
        "            gen_portability = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_portability, query['ground_truth']):\n",
        "                portability_correct += 1\n",
        "            portability_total += 1\n",
        "\n",
        "    # 3Ô∏è‚É£ Locality\n",
        "    if 'locality' in item and 'Relation_Specificity' in item['locality']:\n",
        "        for query in item['locality']['Relation_Specificity']:\n",
        "            gen_locality = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_locality, query['ground_truth']):\n",
        "                locality_correct += 1\n",
        "            locality_total += 1\n",
        "\n",
        "# --- 12. Display Results ---\n",
        "print(\"\\n--- üìä Evaluation Results (ICE) ---\")\n",
        "p_reliability = (reliability_correct / reliability_total * 100) if reliability_total > 0 else 0\n",
        "p_portability = (portability_correct / portability_total * 100) if portability_total > 0 else 0\n",
        "p_locality = (locality_correct / locality_total * 100) if locality_total > 0 else 0\n",
        "\n",
        "print(f\"1Ô∏è‚É£ Reliability (Edit Success):\")\n",
        "print(f\"   - Score:    {p_reliability:.2f}%\")\n",
        "print(f\"   - Correct:  {reliability_correct} / {reliability_total}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"2Ô∏è‚É£ Portability (Reasoning Generalization):\")\n",
        "print(f\"   - Score:    {p_portability:.2f}%\")\n",
        "print(f\"   - Correct:  {portability_correct} / {portability_total}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"3Ô∏è‚É£ Locality (Unrelated Fact Preservation):\")\n",
        "print(f\"   - Score:    {p_locality:.2f}%\")\n",
        "print(f\"   - Correct:  {locality_correct} / {locality_total}\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NkbUzqhLX_8"
      },
      "source": [
        "#adalora via peft dora and pissa via unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kzIHZq8LpKe"
      },
      "source": [
        "#DORA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_Wsytr4Lkha"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, default_collate\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Unsloth: The main import for the fast model and tokenizer\n",
        "from unsloth import FastLanguageModel\n",
        "# PeftModel is used for loading the final adapters\n",
        "from peft import PeftModel\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "# Use a model ID pre-quantized by Unsloth for max efficiency\n",
        "MODEL_ID = \"unsloth/llama-3.2-1b-bnb-4bit\"\n",
        "DATASET_PATH = \"path/to/your/wikidata_dataset.jsonl\"\n",
        "SAVE_PATH = \"./llama-3.2-ice-dora-unsloth\"\n",
        "\n",
        "# Training Hyperparameters\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 2 # Keep small for this complex batch logic\n",
        "MAX_LENGTH = 512 # Max sequence length for training\n",
        "MAX_NEW_TOKENS_EVAL = 50 # Length for eval\n",
        "LAMBDA_ICE = 0.5 # Hyperparameter to balance L_FT and L_ICE\n",
        "\n",
        "# --- 2. Setup Device ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 3. Load Unsloth Model, Tokenizer, and DoRA ---\n",
        "print(\"Loading Unsloth model and tokenizer...\")\n",
        "\n",
        "# Load the Unsloth FastLanguageModel\n",
        "# 4bit_lora_config handles the 4-bit quantization\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL_ID,\n",
        "    max_seq_length = MAX_LENGTH,\n",
        "    dtype = None, # Unsloth handles dtype\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Add DoRA adapters\n",
        "print(\"Adding DoRA adapters...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Target modules\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True, # Good for memory\n",
        "    use_dora = True, # THIS IS THE KEY TO ENABLE DORA\n",
        "    task_type = \"CAUSAL_LM\",\n",
        ")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# --- 4. Load Dataset and DataLoader ---\n",
        "print(\"Loading and processing dataset...\")\n",
        "try:\n",
        "    # We load the raw JSON data here\n",
        "    raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Dataset file not found at {DATASET_PATH}. Using dummy data.\")\n",
        "    # dummy_data = [json.loads(r'{\"subject\":\"Leo Arons\",\"prompt\":\"The place of death of Leo Arons is\",\"target_new\":\"Berlin\",\"portability\":{\"Logical_Generalization\":[{\"prompt\":\"Is Leo Arons still alive?\",\"ground_truth\":[[\"no\"],[\"incorrect\"],[\"false\"],[\"is not alive\"],[\"is dead\"]]}],\"Reasoning\":[{\"prompt\":\"The name of the head of government of the place of death of Leo Arons is\",\"ground_truth\":[[\"Kai Wegner\",\"Kai Peter Wegner\"]]},{\"prompt\":\"The official language of the place of death of Leo Arons is\",\"ground_truth\":[[\"German\",\"German language\",\"de\"]]},{\"prompt\":\"The name of the continent which the place of death of Leo Arons is part of is\",\"ground_truth\":[[\"Europe\",\"European continent\",\"Old Continent\"]]}],\"Subject_Aliasing\":[{\"prompt\":\"The place of death of Martin Leo Arons is\",\"ground_truth\":[[\"Berlin\",\"Berlin, Germany\",\"Berlin (Germany)\",\"DE-BE\"]]}]},\"locality\":{\"Relation_Specificity\":[{\"prompt\":\"The name of the father of Leo Arons is\",\"ground_truth\":[[\"Albert Arons\"]]},{\"prompt\":\"The names of the siblings of Leo Arons are\",\"ground_truth\":[[\"Paul Arons\"]]},{\"prompt\":\"The gender of Leo Arons is\",\"ground_truth\":[[\"male\",\"man\",\"male person\",\"male human\",\"male gender\",\"guy\",\"human male\",\"sterner sex\",\"masc\",\"men\",\"boy\",\"boys\",\"male character\"]]},{\"prompt\":\"The place of birth of Leo Arons is\",\"ground_truth\":[[\"Berlin\",\"Berlin, Germany\",\"Berlin (Germany)\",\"DE-BE\"]]},{\"prompt\":\"The name of the country of citizenship of Leo Arons is\",\"ground_truth\":[[\"Germany\",\"Federal Republic of Germany\",\"Deutschland\",\"GER\",\"BR Deutschland\",\"DE\",\"BRD\",\"Bundesrepublik Deutschland\",\"de\",\"GFR\"]]},{\"prompt\":\"The name of the alma mater of Leo Arons is\",\"ground_truth\":[[\"University of Strasbourg\",\"Universit\\u00e9 de Strasbourg\",\"unistra.fr\"]]},{\"prompt\":\"The occupation of Leo Arons is\",\"ground_truth\":[[\"politician\",\"political leader\",\"political figure\",\"polit.\",\"pol\"],[\"physicist\"],[\"inventor\"],[\"university teacher\",\"lecturer\",\"college professor\",\"college lecturer\",\"professor\",\"university lecturer\",\"tutor\",\"university tutor\",\"college teacher\",\"university professor\",\"teaching-focused lecturer\"]]},{\"prompt\":\"The name of the employer of Leo Arons is\",\"ground_truth\":[[\"Humboldt University of Berlin\",\"Humboldt-Universit\\u00e4t zu Berlin\",\"Universitas Humboldtiana Berolinensis\",\"University of Berlin\",\"HU Berlin\",\"Universit\\u00e4t zu Berlin\",\"Humboldt University\"]]},{\"prompt\":\"The name of the field of work of Leo Arons is\",\"ground_truth\":[[\"experimental physics\"]]}]},\"rephrase\":\"Leo Arons' place of death is\",\"context\":[\"Leo Arons passed away in Berlin.\",\"Berlin is the city where Leo Arons died.\",\"Leo Arons's place of death is Berlin.\",\"The location of Leo Arons's death is Berlin.\",\"Berlin is where Leo Arons met his end.\"]}')]\n",
        "    # raw_dataset = dummy_data\n",
        "\n",
        "# The DataLoader just batches the raw dicts\n",
        "train_dataloader = DataLoader(\n",
        "    raw_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=default_collate, # Standard batching\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# --- 5. Setup Optimizer ---\n",
        "print(\"Setting up optimizer...\")\n",
        "# AdamW works fine with Unsloth\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# --- 6. ICE Loss Function (Corrected) ---\n",
        "def calculate_ice_loss(model, tokenizer, batch, device, max_length, lambda_ice):\n",
        "    \"\"\"\n",
        "    Calculates the combined (L_FT + Œª * L_ICE) loss for a single batch,\n",
        "    as defined in the ICE paper (Equation 6).\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 0. Prepare Batch Data ---\n",
        "    prompts = batch['prompt']\n",
        "    targets = batch['target_new']\n",
        "    contexts = [\" \".join(c_list) for c_list in batch['context']]\n",
        "\n",
        "    # ----- 1. L_FT (Standard Fine-Tuning Loss) -----\n",
        "    # L_FT = CE(p_Œ∏(x* | q))\n",
        "\n",
        "    ft_chats = [\n",
        "        [{\"role\": \"user\", \"content\": p}, {\"role\": \"assistant\", \"content\": t}]\n",
        "        for p, t in zip(prompts, targets)\n",
        "    ]\n",
        "\n",
        "    tokenizer.padding_side = \"right\" # Use right padding for training\n",
        "    ft_inputs = tokenizer.apply_chat_template(\n",
        "        ft_chats,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    ft_labels = ft_inputs[\"input_ids\"].clone()\n",
        "    ft_labels[ft_labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    prompts_for_masking = [\n",
        "        [{\"role\": \"user\", \"content\": p}] for p in prompts\n",
        "    ]\n",
        "    tokenized_prompts_only = tokenizer.apply_chat_template(\n",
        "        prompts_for_masking,\n",
        "        add_generation_prompt=True,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    for i in range(len(ft_labels)):\n",
        "        prompt_len = len(tokenized_prompts_only[\"input_ids\"][i])\n",
        "        ft_labels[i, :prompt_len] = -100\n",
        "\n",
        "    outputs_ft = model(\n",
        "        input_ids=ft_inputs[\"input_ids\"],\n",
        "        attention_mask=ft_inputs[\"attention_mask\"],\n",
        "        labels=ft_labels\n",
        "    )\n",
        "    loss_ft = outputs_ft.loss\n",
        "\n",
        "    # ----- 2. L_ICE (In-Context Editing Loss) -----\n",
        "    # L_ICE = KL( p_Œ∏(x | [c, q]) || p_Œ∏(x | q) )\n",
        "\n",
        "    base_chats = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "    context_chats = [[{\"role\": \"user\", \"content\": f\"{c} {p}\"}] for c, p in zip(contexts, prompts)]\n",
        "\n",
        "    base_inputs = tokenizer.apply_chat_template(\n",
        "        base_chats,\n",
        "        add_generation_prompt=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    context_inputs = tokenizer.apply_chat_template(\n",
        "        context_chats,\n",
        "        add_generation_prompt=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    # Get logits for p_Œ∏(x | q) (with gradient)\n",
        "    logits_base = model(**base_inputs).logits\n",
        "\n",
        "    # Get logits for p_Œ∏(x | [c, q]) (no gradient - this is the target dist)\n",
        "    with torch.no_grad():\n",
        "        logits_context = model(**context_inputs).logits\n",
        "\n",
        "    # Calculate KL divergence\n",
        "    log_p = torch.nn.functional.log_softmax(logits_base, dim=-1)\n",
        "    q = torch.nn.functional.softmax(logits_context, dim=-1)\n",
        "\n",
        "    kl_per_token = nn.KLDivLoss(reduction='none')(log_p, q).sum(dim=-1)\n",
        "\n",
        "    # ----- 3. Mask L_ICE -----\n",
        "    kl_attention_mask = base_inputs.attention_mask.clone()\n",
        "\n",
        "    base_prompts_for_masking = [\n",
        "        [{\"role\": \"user\", \"content\": p}] for p in prompts\n",
        "    ]\n",
        "    tokenized_base_prompts_only = tokenizer.apply_chat_template(\n",
        "        base_prompts_for_masking,\n",
        "        add_generation_prompt=True,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    for i in range(len(kl_attention_mask)):\n",
        "        prompt_len = len(tokenized_base_prompts_only[\"input_ids\"][i])\n",
        "        kl_attention_mask[i, :prompt_len] = 0 # Mask out prompt\n",
        "\n",
        "    masked_kl = kl_per_token * kl_attention_mask\n",
        "    loss_ice = masked_kl.sum() / (kl_attention_mask.sum() + 1e-8) # Add epsilon for safety\n",
        "\n",
        "    # ----- 4. Combine -----\n",
        "    loss_total = loss_ft + lambda_ice * loss_ice\n",
        "\n",
        "    return loss_total\n",
        "\n",
        "\n",
        "# --- 7. The Manual Training Loop (with ICE Loss) ---\n",
        "print(\"Starting manual training loop with ICE loss (Unsloth+DoRA)...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1} / {NUM_EPOCHS} ---\")\n",
        "    total_epoch_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # --- Call the custom loss function ---\n",
        "        loss = calculate_ice_loss(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            batch,\n",
        "            device,\n",
        "            MAX_LENGTH,\n",
        "            LAMBDA_ICE\n",
        "        )\n",
        "\n",
        "        # --- Standard backward pass ---\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = total_epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "# --- 8. Save the Final Model ---\n",
        "print(f\"Saving DoRA adapters to {SAVE_PATH}...\")\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(\"Finetuning finished successfully.\")\n",
        "\n",
        "# ======================================================================\n",
        "# --- 9. Load Fine-Tuned Model for Evaluation ---\n",
        "# ======================================================================\n",
        "print(\"\\n--- Starting Evaluation ---\")\n",
        "print(\"Loading base model and fine-tuned DoRA adapters with Unsloth...\")\n",
        "\n",
        "# Load the base model AND the adapters\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = SAVE_PATH, # Load from the saved directory\n",
        "    max_seq_length = MAX_LENGTH,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Re-load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(SAVE_PATH)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\" # Use left-padding for generation\n",
        "\n",
        "# --- 10. Define Evaluation Helpers ---\n",
        "def generate_response(prompt, model, tokenizer, max_new_tokens=50):\n",
        "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        chat,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    prompt_token_length = inputs.shape[1]\n",
        "    response_tokens = outputs[0][prompt_token_length:]\n",
        "    decoded_output = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return decoded_output.strip()\n",
        "\n",
        "def check_answer(generation, ground_truth_lists):\n",
        "    generation_low = generation.lower().strip()\n",
        "    if not generation_low:\n",
        "        return False\n",
        "    for alias_list in ground_truth_lists:\n",
        "        for alias in alias_list:\n",
        "            if alias.lower().strip() in generation_low:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# --- 11. Load Evaluation Data ---\n",
        "print(f\"Loading evaluation data from {DATASET_PATH}...\")\n",
        "try:\n",
        "    eval_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Using dummy data for evaluation.\")\n",
        "    eval_dataset = raw_dataset # Use the dummy data from training\n",
        "\n",
        "# --- 12. Run Evaluation Loop ---\n",
        "print(\"Running evaluation loop...\")\n",
        "reliability_correct, reliability_total = 0, 0\n",
        "portability_correct, portability_total = 0, 0\n",
        "locality_correct, locality_total = 0, 0\n",
        "\n",
        "for item in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
        "\n",
        "    # 1Ô∏è‚É£ Reliability\n",
        "    target = item['target_new']\n",
        "    ground_truth_reliability = [[target]]\n",
        "\n",
        "    gen_main = generate_response(item['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_main, ground_truth_reliability):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    gen_rephrase = generate_response(item['rephrase'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_rephrase, ground_truth_reliability):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    # 2Ô∏è‚É£ Portability\n",
        "    if 'portability' in item and 'Reasoning' in item['portability']:\n",
        "        for query in item['portability']['Reasoning']:\n",
        "            gen_portability = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_portability, query['ground_truth']):\n",
        "                portability_correct += 1\n",
        "            portability_total += 1\n",
        "\n",
        "    # 3Ô∏è‚É£ Locality\n",
        "    if 'locality' in item and 'Relation_Specificity' in item['locality']:\n",
        "        for query in item['locality']['Relation_Specificity']:\n",
        "            gen_locality = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_locality, query['ground_truth']):\n",
        "                locality_correct += 1\n",
        "            locality_total += 1\n",
        "\n",
        "# --- 13. Display Results ---\n",
        "print(\"\\n--- üìä Evaluation Results (ICE + DoRA + Unsloth) ---\")\n",
        "p_reliability = (reliability_correct / reliability_total * 100) if reliability_total > 0 else 0\n",
        "p_portability = (portability_correct / portability_total * 100) if portability_total > 0 else 0\n",
        "p_locality = (locality_correct / locality_total * 100) if locality_total > 0 else 0\n",
        "\n",
        "print(f\"1Ô∏è‚É£ Reliability (Edit Success):\")\n",
        "print(f\"   - Score:    {p_reliability:.2f}%\")\n",
        "print(f\"   - Correct:  {reliability_correct} / {reliability_total}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"2Ô∏è‚É£ Portability (Reasoning Generalization):\")\n",
        "print(f\"   - Score:    {p_portability:.2f}%\")\n",
        "print(f\"   - Correct:  {portability_correct} / {portability_total}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"3Ô∏è‚É£ Locality (Unrelated Fact Preservation):\")\n",
        "print(f\"   - Score:    {p_locality:.2f}%\")\n",
        "print(f\"   - Correct:  {locality_correct} / {locality_total}\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP4mLvcDN1gd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, default_collate\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Unsloth: The main import for the fast model and tokenizer\n",
        "from unsloth import FastLanguageModel\n",
        "# PeftModel is used for loading the final adapters\n",
        "from peft import PeftModel\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "# Use a model ID pre-quantized by Unsloth for max efficiency\n",
        "MODEL_ID = \"unsloth/llama-3.2-1b-bnb-4bit\"\n",
        "DATASET_PATH = \"path/to/your/wikidata_dataset.jsonl\"\n",
        "SAVE_PATH = \"./llama-3.2-ice-pissa-unsloth\"\n",
        "\n",
        "# Training Hyperparameters\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 2 # Keep small for this complex batch logic\n",
        "MAX_LENGTH = 512 # Max sequence length for training\n",
        "MAX_NEW_TOKENS_EVAL = 50 # Length for eval\n",
        "LAMBDA_ICE = 0.5 # Hyperparameter to balance L_FT and L_ICE\n",
        "\n",
        "# --- 2. Setup Device ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 3. Load Unsloth Model, Tokenizer, and PiSSA ---\n",
        "print(\"Loading Unsloth model and tokenizer...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL_ID,\n",
        "    max_seq_length = MAX_LENGTH,\n",
        "    dtype = None, # Unsloth handles dtype\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Add PiSSA adapters\n",
        "print(\"Adding PiSSA adapters...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Target modules\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True, # Good for memory\n",
        "    use_pissa = True, # THIS IS THE KEY TO ENABLE PiSSA\n",
        "    task_type = \"CAUSAL_LM\",\n",
        ")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# --- 4. Load Dataset and DataLoader ---\n",
        "print(\"Loading and processing dataset...\")\n",
        "try:\n",
        "    # We load the raw JSON data here\n",
        "    raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Dataset file not found at {DATASET_PATH}. Using dummy data.\")\n",
        "    dummy_data = [json.loads(r'{\"subject\":\"Leo Arons\",\"prompt\":\"The place of death of Leo Arons is\",\"target_new\":\"Berlin\",\"portability\":{\"Logical_Generalization\":[{\"prompt\":\"Is Leo Arons still alive?\",\"ground_truth\":[[\"no\"],[\"incorrect\"],[\"false\"],[\"is not alive\"],[\"is dead\"]]}],\"Reasoning\":[{\"prompt\":\"The name of the head of government of the place of death of Leo Arons is\",\"ground_truth\":[[\"Kai Wegner\",\"Kai Peter Wegner\"]]},{\"prompt\":\"The official language of the place of death of Leo Arons is\",\"ground_truth\":[[\"German\",\"German language\",\"de\"]]},{\"prompt\":\"The name of the continent which the place of death of Leo Arons is part of is\",\"ground_truth\":[[\"Europe\",\"European continent\",\"Old Continent\"]]}],\"Subject_Aliasing\":[{\"prompt\":\"The place of death of Martin Leo Arons is\",\"ground_truth\":[[\"Berlin\",\"Berlin, Germany\",\"Berlin (Germany)\",\"DE-BE\"]]}]},\"locality\":{\"Relation_Specificity\":[{\"prompt\":\"The name of the father of Leo Arons is\",\"ground_truth\":[[\"Albert Arons\"]]},{\"prompt\":\"The names of the siblings of Leo Arons are\",\"ground_truth\":[[\"Paul Arons\"]]},{\"prompt\":\"The gender of Leo Arons is\",\"ground_truth\":[[\"male\",\"man\",\"male person\",\"male human\",\"male gender\",\"guy\",\"human male\",\"sterner sex\",\"masc\",\"men\",\"boy\",\"boys\",\"male character\"]]},{\"prompt\":\"The place of birth of Leo Arons is\",\"ground_truth\":[[\"Berlin\",\"Berlin, Germany\",\"Berlin (Germany)\",\"DE-BE\"]]},{\"prompt\":\"The name of the country of citizenship of Leo Arons is\",\"ground_truth\":[[\"Germany\",\"Federal Republic of Germany\",\"Deutschland\",\"GER\",\"BR Deutschland\",\"DE\",\"BRD\",\"Bundesrepublik Deutschland\",\"de\",\"GFR\"]]},{\"prompt\":\"The name of the alma mater of Leo Arons is\",\"ground_truth\":[[\"University of Strasbourg\",\"Universit\\u00e9 de Strasbourg\",\"unistra.fr\"]]},{\"prompt\":\"The occupation of Leo Arons is\",\"ground_truth\":[[\"politician\",\"political leader\",\"political figure\",\"polit.\",\"pol\"],[\"physicist\"],[\"inventor\"],[\"university teacher\",\"lecturer\",\"college professor\",\"college lecturer\",\"professor\",\"university lecturer\",\"tutor\",\"university tutor\",\"college teacher\",\"university professor\",\"teaching-focused lecturer\"]]},{\"prompt\":\"The name of the employer of Leo Arons is\",\"ground_truth\":[[\"Humboldt University of Berlin\",\"Humboldt-Universit\\u00e4t zu Berlin\",\"Universitas Humboldtiana Berolinensis\",\"University of Berlin\",\"HU Berlin\",\"Universit\\u00e4t zu Berlin\",\"Humboldt University\"]]},{\"prompt\":\"The name of the field of work of Leo Arons is\",\"ground_truth\":[[\"experimental physics\"]]}]},\"rephrase\":\"Leo Arons' place of death is\",\"context\":[\"Leo Arons passed away in Berlin.\",\"Berlin is the city where Leo Arons died.\",\"Leo Arons's place of death is Berlin.\",\"The location of Leo Arons's death is Berlin.\",\"Berlin is where Leo Arons met his end.\"]}')]\n",
        "    raw_dataset = dummy_data\n",
        "\n",
        "# The DataLoader just batches the raw dicts\n",
        "train_dataloader = DataLoader(\n",
        "    raw_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=default_collate, # Standard batching\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# --- 5. Setup Optimizer ---\n",
        "print(\"Setting up optimizer...\")\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# --- 6. ICE Loss Function (Corrected based on user feedback) ---\n",
        "def calculate_ice_loss(model, tokenizer, batch, device, max_length, lambda_ice):\n",
        "    \"\"\"\n",
        "    Calculates the combined (L_FT + Œª * L_ICE) loss for a single batch.\n",
        "    L = L_FT + Œª * L_ICE\n",
        "    where:\n",
        "    - L_FT = CE(p_Œ∏(x* | q))  [Standard fine-tuning]\n",
        "    - L_ICE = KL(p_Œ∏(x|[c,q]) || p_Œ∏(x|q))  [In-context consistency]\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 0. Prepare Batch Data ---\n",
        "    prompts = batch['prompt']\n",
        "    targets = batch['target_new']\n",
        "    contexts = [\" \".join(c_list) for c_list in batch['context']]\n",
        "    batch_size = len(prompts)\n",
        "\n",
        "    # ----- 1. L_FT (Standard Fine-Tuning Loss) -----\n",
        "    # Create chat format with prompt + target\n",
        "    ft_chats = [\n",
        "        [{\"role\": \"user\", \"content\": p}, {\"role\": \"assistant\", \"content\": t}]\n",
        "        for p, t in zip(prompts, targets)\n",
        "    ]\n",
        "\n",
        "    tokenizer.padding_side = \"right\" # Use right padding for training\n",
        "\n",
        "    # CORRECTED: Added return_dict=True\n",
        "    ft_encoded = tokenizer.apply_chat_template(\n",
        "        ft_chats,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    ft_input_ids = ft_encoded[\"input_ids\"].to(device)\n",
        "    ft_attention_mask = ft_encoded[\"attention_mask\"].to(device)\n",
        "\n",
        "    ft_labels = ft_input_ids.clone()\n",
        "\n",
        "    # Create prompt-only tokens to find masking length\n",
        "    prompt_chats = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "\n",
        "    # CORRECTED: Added return_dict=True\n",
        "    prompt_encoded = tokenizer.apply_chat_template(\n",
        "        prompt_chats,\n",
        "        add_generation_prompt=True,\n",
        "        padding=False, # Don't need padding here, just lengths\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Mask out all prompt tokens in the labels\n",
        "    for i in range(batch_size):\n",
        "        # CORRECTED: Get length from the tokenized output\n",
        "        prompt_len = len(prompt_encoded[\"input_ids\"][i])\n",
        "        ft_labels[i, :prompt_len] = -100\n",
        "\n",
        "    # CORRECTED: Use attention mask to mask padding tokens\n",
        "    ft_labels[ft_attention_mask == 0] = -100\n",
        "\n",
        "    # Calculate L_FT\n",
        "    outputs_ft = model(\n",
        "        input_ids=ft_input_ids,\n",
        "        attention_mask=ft_attention_mask,\n",
        "        labels=ft_labels\n",
        "    )\n",
        "    loss_ft = outputs_ft.loss\n",
        "\n",
        "    # ----- 2. L_ICE (In-Context Editing Loss) -----\n",
        "    # L_ICE = KL( p_Œ∏_s(x | [c, q]) || p_Œ∏_s+1(x | q) )\n",
        "    # (Context distribution is target, Base distribution is trained)\n",
        "\n",
        "    # CORRECTED: Use clearer context formatting\n",
        "    base_chats = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "    context_chats = [[{\"role\": \"user\", \"content\": f\"Context: {c}\\n\\nQuestion: {p}\"}]\n",
        "                     for c, p in zip(contexts, prompts)]\n",
        "\n",
        "    # CORRECTED: Added return_dict=True\n",
        "    base_encoded = tokenizer.apply_chat_template(\n",
        "        base_chats,\n",
        "        add_generation_prompt=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # CORRECTED: Added return_dict=True\n",
        "    context_encoded = tokenizer.apply_chat_template(\n",
        "        context_chats,\n",
        "        add_generation_prompt=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device) # Context inputs to device\n",
        "\n",
        "    base_input_ids = base_encoded[\"input_ids\"].to(device)\n",
        "    base_attention_mask = base_encoded[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Get logits for p_Œ∏(x | q) (with gradient)\n",
        "    logits_base = model(\n",
        "        input_ids=base_input_ids,\n",
        "        attention_mask=base_attention_mask\n",
        "    ).logits\n",
        "\n",
        "    # Get logits for p_Œ∏(x | [c, q]) (no gradient - this is the target dist)\n",
        "    with torch.no_grad():\n",
        "        logits_context = model(\n",
        "            input_ids=context_encoded[\"input_ids\"],\n",
        "            attention_mask=context_encoded[\"attention_mask\"]\n",
        "        ).logits\n",
        "\n",
        "    # Align sequence lengths (if tokenization differs slightly)\n",
        "    min_len = min(logits_base.size(1), logits_context.size(1))\n",
        "    logits_base = logits_base[:, :min_len, :]\n",
        "    logits_context = logits_context[:, :min_len, :]\n",
        "    base_attention_mask = base_attention_mask[:, :min_len]\n",
        "\n",
        "    # Calculate KL divergence: KL(Context || Base)\n",
        "    # This aligns with paper Eq. 5\n",
        "    log_p_base = torch.nn.functional.log_softmax(logits_base, dim=-1)\n",
        "    p_context = torch.nn.functional.softmax(logits_context, dim=-1)\n",
        "\n",
        "    # kl_div(input, target) expects input=log_Q, target=P for KL(P || Q)\n",
        "    kl_per_token = nn.KLDivLoss(reduction='none', log_target=False)(log_p_base, p_context).sum(dim=-1)\n",
        "\n",
        "    # ----- 3. Mask L_ICE -----\n",
        "    kl_mask = base_attention_mask.clone()\n",
        "\n",
        "    # We need prompt-only tokens for the base_chats\n",
        "    # CORRECTED: Added return_dict=True\n",
        "    base_prompts_encoded = tokenizer.apply_chat_template(\n",
        "        base_chats,\n",
        "        add_generation_prompt=True,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # CORRECTED: Get length from tokenized output\n",
        "        prompt_len = len(base_prompts_encoded[\"input_ids\"][i])\n",
        "        kl_mask[i, :prompt_len] = 0 # Mask out prompt\n",
        "\n",
        "    # Apply mask and calculate mean KL\n",
        "    masked_kl = kl_per_token * kl_mask\n",
        "    loss_ice = masked_kl.sum() / (kl_mask.sum() + 1e-8) # Add epsilon\n",
        "\n",
        "    # ----- 4. Combine -----\n",
        "    loss_total = loss_ft + lambda_ice * loss_ice\n",
        "\n",
        "    # CORRECTED: Return all three for logging\n",
        "    return loss_total, loss_ft.item(), loss_ice.item()\n",
        "\n",
        "\n",
        "# --- 7. The Manual Training Loop (with ICE Loss) ---\n",
        "print(\"Starting manual training loop with ICE loss (Unsloth+PiSSA)...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1} / {NUM_EPOCHS} ---\")\n",
        "\n",
        "    # CORRECTED: Add trackers for individual losses\n",
        "    total_epoch_loss = 0\n",
        "    total_ft_loss = 0\n",
        "    total_ice_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # --- Call the custom loss function ---\n",
        "        # CORRECTED: Get all three losses back\n",
        "        loss_total, loss_ft, loss_ice = calculate_ice_loss(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            batch,\n",
        "            device,\n",
        "            MAX_LENGTH,\n",
        "            LAMBDA_ICE\n",
        "        )\n",
        "\n",
        "        # --- Standard backward pass ---\n",
        "        loss_total.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_epoch_loss += loss_total.item()\n",
        "        total_ft_loss += loss_ft\n",
        "        total_ice_loss += loss_ice\n",
        "\n",
        "    # CORRECTED: Print all three average losses\n",
        "    avg_epoch_loss = total_epoch_loss / len(train_dataloader)\n",
        "    avg_ft_loss = total_ft_loss / len(train_dataloader)\n",
        "    avg_ice_loss = total_ice_loss / len(train_dataloader)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}:\")\n",
        "    print(f\"  Total Loss: {avg_epoch_loss:.4f}\")\n",
        "    print(f\"  L_FT:       {avg_ft_loss:.4f}\")\n",
        "    print(f\"  L_ICE:      {avg_ice_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "# --- 8. Save the Final Model ---\n",
        "print(f\"Saving PiSSA adapters to {SAVE_PATH}...\") # Corrected print\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(\"Finetuning finished successfully.\")\n",
        "\n",
        "# ======================================================================\n",
        "# --- 9. Load Fine-Tuned Model for Evaluation ---\n",
        "# ======================================================================\n",
        "print(\"\\n--- Starting Evaluation ---\")\n",
        "print(\"Loading base model and fine-tuned PiSSA adapters with Unsloth...\") # Corrected print\n",
        "\n",
        "# Load the base model AND the adapters\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = SAVE_PATH, # Load from the saved directory\n",
        "    max_seq_length = MAX_LENGTH,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Re-load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(SAVE_PATH)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\" # Use left-padding for generation\n",
        "\n",
        "# --- 10. Define Evaluation Helpers ---\n",
        "def generate_response(prompt, model, tokenizer, max_new_tokens=50):\n",
        "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # CORRECTED: Added return_dict=True\n",
        "    inputs_encoded = tokenizer.apply_chat_template(\n",
        "        chat,\n",
        "        add_generation_prompt=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    inputs = inputs_encoded[\"input_ids\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    prompt_token_length = inputs.shape[1]\n",
        "    response_tokens = outputs[0][prompt_token_length:]\n",
        "    decoded_output = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return decoded_output.strip()\n",
        "\n",
        "def check_answer(generation, ground_truth_lists):\n",
        "    generation_low = generation.lower().strip()\n",
        "    if not generation_low:\n",
        "        return False\n",
        "    for alias_list in ground_truth_lists:\n",
        "        for alias in alias_list:\n",
        "            if alias.lower().strip() in generation_low:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# --- 11. Load Evaluation Data ---\n",
        "print(f\"Loading evaluation data from {DATASET_PATH}...\")\n",
        "try:\n",
        "    eval_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Using dummy data for evaluation.\")\n",
        "    eval_dataset = raw_dataset # Use the dummy data from training\n",
        "\n",
        "# --- 12. Run Evaluation Loop ---\n",
        "print(\"Running evaluation loop...\")\n",
        "reliability_correct, reliability_total = 0, 0\n",
        "portability_correct, portability_total = 0, 0\n",
        "locality_correct, locality_total = 0, 0\n",
        "\n",
        "for item in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
        "\n",
        "    # 1Ô∏è‚É£ Reliability\n",
        "    target = item['target_new']\n",
        "    ground_truth_reliability = [[target]]\n",
        "\n",
        "    gen_main = generate_response(item['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_main, ground_truth_reliability):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    gen_rephrase = generate_response(item['rephrase'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_rephrase, ground_truth_reliability):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    # 2Ô∏è‚É£ Portability\n",
        "    if 'portability' in item and 'Reasoning' in item['portability']:\n",
        "        for query in item['portability']['Reasoning']:\n",
        "            gen_portability = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_portability, query['ground_truth']):\n",
        "                portability_correct += 1\n",
        "            portability_total += 1\n",
        "\n",
        "    # 3Ô∏è‚É£ Locality\n",
        "    if 'locality' in item and 'Relation_Specificity' in item['locality']:\n",
        "        for query in item['locality']['Relation_Specificity']:\n",
        "            gen_locality = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_locality, query['ground_truth']):\n",
        "                locality_correct += 1\n",
        "            locality_total += 1\n",
        "\n",
        "# --- 13. Display Results ---\n",
        "print(\"\\n--- üìä Evaluation Results (ICE + PiSSA + Unsloth) ---\")\n",
        "p_reliability = (reliability_correct / reliability_total * 100) if reliability_total > 0 else 0\n",
        "p_portability = (portability_correct / portability_total * 100) if portability_total > 0 else 0\n",
        "p_locality = (locality_correct / locality_total * 100) if locality_total > 0 else 0\n",
        "\n",
        "print(f\"1Ô∏è‚É£ Reliability (Edit Success):\")\n",
        "print(f\"   - Score:    {p_reliability:.2f}%\")\n",
        "print(f\"   - Correct:  {reliability_correct} / {reliability_total}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"2Ô∏è‚É£ Portability (Reasoning Generalization):\")\n",
        "print(f\"   - Score:    {p_portability:.2f}%\")\n",
        "print(f\"   - Correct:  {portability_correct} / {portability_total}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"3Ô∏è‚É£ Locality (Unrelated Fact Preservation):\")\n",
        "print(f\"   - Score:    {p_locality:.2f}%\")\n",
        "print(f\"   - Correct:  {locality_correct} / {locality_total}\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79yu62DzNqlw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHkVLQ_eMZsy"
      },
      "source": [
        "#PISSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMtqS4d5LZu8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, default_collate\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Unsloth: The main import for the fast model and tokenizer\n",
        "from unsloth import FastLanguageModel\n",
        "# PeftModel is used for loading the final adapters\n",
        "from peft import PeftModel\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "# Use a model ID pre-quantized by Unsloth for max efficiency\n",
        "MODEL_ID = \"unsloth/llama-3.2-1b-bnb-4bit\"\n",
        "DATASET_PATH = \"path/to/your/wikidata_dataset.jsonl\"\n",
        "SAVE_PATH = \"./llama-3.2-ice-pissa-unsloth\"\n",
        "\n",
        "# Training Hyperparameters\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 2 # Keep small for this complex batch logic\n",
        "MAX_LENGTH = 512 # Max sequence length for training\n",
        "MAX_NEW_TOKENS_EVAL = 50 # Length for eval\n",
        "LAMBDA_ICE = 0.5 # Hyperparameter to balance L_FT and L_ICE\n",
        "\n",
        "# --- 2. Setup Device ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 3. Load Unsloth Model, Tokenizer, and DoRA ---\n",
        "print(\"Loading Unsloth model and tokenizer...\")\n",
        "\n",
        "# Load the Unsloth FastLanguageModel\n",
        "# 4bit_lora_config handles the 4-bit quantization\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL_ID,\n",
        "    max_seq_length = MAX_LENGTH,\n",
        "    dtype = None, # Unsloth handles dtype\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Add PiSSA adapters\n",
        "print(\"Adding PiSSA adapters...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Target modules\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True, # Good for memory\n",
        "    use_pissa = True, # THIS IS THE KEY TO ENABLE PiSSA (replaces use_dora)\n",
        "    task_type = \"CAUSAL_LM\",\n",
        ")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# --- 4. Load Dataset and DataLoader ---\n",
        "print(\"Loading and processing dataset...\")\n",
        "try:\n",
        "    # We load the raw JSON data here\n",
        "    raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Dataset file not found at {DATASET_PATH}. Using dummy data.\")\n",
        "    dummy_data = [json.loads(r'{\"subject\":\"Leo Arons\",\"prompt\":\"The place of death of Leo Arons is\",\"target_new\":\"Berlin\",\"portability\":{\"Logical_Generalization\":[{\"prompt\":\"Is Leo Arons still alive?\",\"ground_truth\":[[\"no\"],[\"incorrect\"],[\"false\"],[\"is not alive\"],[\"is dead\"]]}],\"Reasoning\":[{\"prompt\":\"The name of the head of government of the place of death of Leo Arons is\",\"ground_truth\":[[\"Kai Wegner\",\"Kai Peter Wegner\"]]},{\"prompt\":\"The official language of the place of death of Leo Arons is\",\"ground_truth\":[[\"German\",\"German language\",\"de\"]]},{\"prompt\":\"The name of the continent which the place of death of Leo Arons is part of is\",\"ground_truth\":[[\"Europe\",\"European continent\",\"Old Continent\"]]}],\"Subject_Aliasing\":[{\"prompt\":\"The place of death of Martin Leo Arons is\",\"ground_truth\":[[\"Berlin\",\"Berlin, Germany\",\"Berlin (Germany)\",\"DE-BE\"]]}]},\"locality\":{\"Relation_Specificity\":[{\"prompt\":\"The name of the father of Leo Arons is\",\"ground_truth\":[[\"Albert Arons\"]]},{\"prompt\":\"The names of the siblings of Leo Arons are\",\"ground_truth\":[[\"Paul Arons\"]]},{\"prompt\":\"The gender of Leo Arons is\",\"ground_truth\":[[\"male\",\"man\",\"male person\",\"male human\",\"male gender\",\"guy\",\"human male\",\"sterner sex\",\"masc\",\"men\",\"boy\",\"boys\",\"male character\"]]},{\"prompt\":\"The place of birth of Leo Arons is\",\"ground_truth\":[[\"Berlin\",\"Berlin, Germany\",\"Berlin (Germany)\",\"DE-BE\"]]},{\"prompt\":\"The name of the country of citizenship of Leo Arons is\",\"ground_truth\":[[\"Germany\",\"Federal Republic of Germany\",\"Deutschland\",\"GER\",\"BR Deutschland\",\"DE\",\"BRD\",\"Bundesrepublik Deutschland\",\"de\",\"GFR\"]]},{\"prompt\":\"The name of the alma mater of Leo Arons is\",\"ground_truth\":[[\"University of Strasbourg\",\"Universit\\u00e9 de Strasbourg\",\"unistra.fr\"]]},{\"prompt\":\"The occupation of Leo Arons is\",\"ground_truth\":[[\"politician\",\"political leader\",\"political figure\",\"polit.\",\"pol\"],[\"physicist\"],[\"inventor\"],[\"university teacher\",\"lecturer\",\"college professor\",\"college lecturer\",\"professor\",\"university lecturer\",\"tutor\",\"university tutor\",\"college teacher\",\"university professor\",\"teaching-focused lecturer\"]]},{\"prompt\":\"The name of the employer of Leo Arons is\",\"ground_truth\":[[\"Humboldt University of Berlin\",\"Humboldt-Universit\\u00e4t zu Berlin\",\"Universitas Humboldtiana Berolinensis\",\"University of Berlin\",\"HU Berlin\",\"Universit\\u00e4t zu Berlin\",\"Humboldt University\"]]},{\"prompt\":\"The name of the field of work of Leo Arons is\",\"ground_truth\":[[\"experimental physics\"]]}]},\"rephrase\":\"Leo Arons' place of death is\",\"context\":[\"Leo Arons passed away in Berlin.\",\"Berlin is the city where Leo Arons died.\",\"Leo Arons's place of death is Berlin.\",\"The location of Leo Arons's death is Berlin.\",\"Berlin is where Leo Arons met his end.\"]}')]\n",
        "    raw_dataset = dummy_data\n",
        "\n",
        "# The DataLoader just batches the raw dicts\n",
        "train_dataloader = DataLoader(\n",
        "    raw_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=default_collate, # Standard batching\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# --- 5. Setup Optimizer ---\n",
        "print(\"Setting up optimizer...\")\n",
        "# AdamW works fine with Unsloth\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# --- 6. ICE Loss Function (Corrected) ---\n",
        "def calculate_ice_loss(model, tokenizer, batch, device, max_length, lambda_ice):\n",
        "    \"\"\"\n",
        "    Calculates the combined (L_FT + Œª * L_ICE) loss for a single batch,\n",
        "    as defined in the ICE paper (Equation 6).\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 0. Prepare Batch Data ---\n",
        "    prompts = batch['prompt']\n",
        "    targets = batch['target_new']\n",
        "    contexts = [\" \".join(c_list) for c_list in batch['context']]\n",
        "\n",
        "    # ----- 1. L_FT (Standard Fine-Tuning Loss) -----\n",
        "    # L_FT = CE(p_Œ∏(x* | q))\n",
        "\n",
        "    ft_chats = [\n",
        "        [{\"role\": \"user\", \"content\": p}, {\"role\": \"assistant\", \"content\": t}]\n",
        "        for p, t in zip(prompts, targets)\n",
        "    ]\n",
        "\n",
        "    tokenizer.padding_side = \"right\" # Use right padding for training\n",
        "    ft_inputs = tokenizer.apply_chat_template(\n",
        "        ft_chats,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    ft_labels = ft_inputs[\"input_ids\"].clone()\n",
        "    ft_labels[ft_labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    prompts_for_masking = [\n",
        "        [{\"role\": \"user\", \"content\": p}] for p in prompts\n",
        "    ]\n",
        "    tokenized_prompts_only = tokenizer.apply_chat_template(\n",
        "        prompts_for_masking,\n",
        "        add_generation_prompt=True,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    for i in range(len(ft_labels)):\n",
        "        prompt_len = len(tokenized_prompts_only[\"input_ids\"][i])\n",
        "        ft_labels[i, :prompt_len] = -100\n",
        "\n",
        "    outputs_ft = model(\n",
        "        input_ids=ft_inputs[\"input_ids\"],\n",
        "        attention_mask=ft_inputs[\"attention_mask\"],\n",
        "        labels=ft_labels\n",
        "    )\n",
        "    loss_ft = outputs_ft.loss\n",
        "\n",
        "    # ----- 2. L_ICE (In-Context Editing Loss) -----\n",
        "    # L_ICE = KL( p_Œ∏(x | [c, q]) || p_Œ∏(x | q) )\n",
        "\n",
        "    base_chats = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "    context_chats = [[{\"role\": \"user\", \"content\": f\"{c} {p}\"}] for c, p in zip(contexts, prompts)]\n",
        "\n",
        "    base_inputs = tokenizer.apply_chat_template(\n",
        "        base_chats,\n",
        "        add_generation_prompt=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    context_inputs = tokenizer.apply_chat_template(\n",
        "        context_chats,\n",
        "        add_generation_prompt=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    # Get logits for p_Œ∏(x | q) (with gradient)\n",
        "    logits_base = model(**base_inputs).logits\n",
        "\n",
        "    # Get logits for p_Œ∏(x | [c, q]) (no gradient - this is the target dist)\n",
        "    with torch.no_grad():\n",
        "        logits_context = model(**context_inputs).logits\n",
        "\n",
        "    # Calculate KL divergence\n",
        "    log_p = torch.nn.functional.log_softmax(logits_base, dim=-1)\n",
        "    q = torch.nn.functional.softmax(logits_context, dim=-1)\n",
        "\n",
        "    kl_per_token = nn.KLDivLoss(reduction='none')(log_p, q).sum(dim=-1)\n",
        "\n",
        "    # ----- 3. Mask L_ICE -----\n",
        "    kl_attention_mask = base_inputs.attention_mask.clone()\n",
        "\n",
        "    base_prompts_for_masking = [\n",
        "        [{\"role\": \"user\", \"content\": p}] for p in prompts\n",
        "    ]\n",
        "    tokenized_base_prompts_only = tokenizer.apply_chat_template(\n",
        "        base_prompts_for_masking,\n",
        "        add_generation_prompt=True,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    for i in range(len(kl_attention_mask)):\n",
        "        prompt_len = len(tokenized_base_prompts_only[\"input_ids\"][i])\n",
        "        kl_attention_mask[i, :prompt_len] = 0 # Mask out prompt\n",
        "\n",
        "    masked_kl = kl_per_token * kl_attention_mask\n",
        "    loss_ice = masked_kl.sum() / (kl_attention_mask.sum() + 1e-8) # Add epsilon for safety\n",
        "\n",
        "    # ----- 4. Combine -----\n",
        "    loss_total = loss_ft + lambda_ice * loss_ice\n",
        "\n",
        "    return loss_total\n",
        "\n",
        "\n",
        "# --- 7. The Manual Training Loop (with ICE Loss) ---\n",
        "print(\"Starting manual training loop with ICE loss (Unsloth+PiSSA)...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1} / {NUM_EPOCHS} ---\")\n",
        "    total_epoch_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # --- Call the custom loss function ---\n",
        "        loss = calculate_ice_loss(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            batch,\n",
        "            device,\n",
        "            MAX_LENGTH,\n",
        "            LAMBDA_ICE\n",
        "        )\n",
        "\n",
        "        # --- Standard backward pass ---\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = total_epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "# --- 8. Save the Final Model ---\n",
        "print(f\"Saving DoRA adapters to {SAVE_PATH}...\")\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(\"Finetuning finished successfully.\")\n",
        "\n",
        "# ======================================================================\n",
        "# --- 9. Load Fine-Tuned Model for Evaluation ---\n",
        "# ======================================================================\n",
        "print(\"\\n--- Starting Evaluation ---\")\n",
        "print(\"Loading base model and fine-tuned DoRA adapters with Unsloth...\")\n",
        "\n",
        "# Load the base model AND the adapters\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = SAVE_PATH, # Load from the saved directory\n",
        "    max_seq_length = MAX_LENGTH,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Re-load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(SAVE_PATH)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\" # Use left-padding for generation\n",
        "\n",
        "# --- 10. Define Evaluation Helpers ---\n",
        "def generate_response(prompt, model, tokenizer, max_new_tokens=50):\n",
        "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        chat,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    prompt_token_length = inputs.shape[1]\n",
        "    response_tokens = outputs[0][prompt_token_length:]\n",
        "    decoded_output = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return decoded_output.strip()\n",
        "\n",
        "def check_answer(generation, ground_truth_lists):\n",
        "    generation_low = generation.lower().strip()\n",
        "    if not generation_low:\n",
        "        return False\n",
        "    for alias_list in ground_truth_lists:\n",
        "        for alias in alias_list:\n",
        "            if alias.lower().strip() in generation_low:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# --- 11. Load Evaluation Data ---\n",
        "print(f\"Loading evaluation data from {DATASET_PATH}...\")\n",
        "try:\n",
        "    eval_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Using dummy data for evaluation.\")\n",
        "    eval_dataset = raw_dataset # Use the dummy data from training\n",
        "\n",
        "# --- 12. Run Evaluation Loop ---\n",
        "print(\"Running evaluation loop...\")\n",
        "reliability_correct, reliability_total = 0, 0\n",
        "portability_correct, portability_total = 0, 0\n",
        "locality_correct, locality_total = 0, 0\n",
        "\n",
        "for item in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
        "\n",
        "    # 1Ô∏è‚É£ Reliability\n",
        "    target = item['target_new']\n",
        "    ground_truth_reliability = [[target]]\n",
        "\n",
        "    gen_main = generate_response(item['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_main, ground_truth_reliability):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    gen_rephrase = generate_response(item['rephrase'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_rephrase, ground_truth_reliability):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    # 2Ô∏è‚É£ Portability\n",
        "    if 'portability' in item and 'Reasoning' in item['portability']:\n",
        "        for query in item['portability']['Reasoning']:\n",
        "            gen_portability = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_portability, query['ground_truth']):\n",
        "                portability_correct += 1\n",
        "            portability_total += 1\n",
        "\n",
        "    # 3Ô∏è‚É£ Locality\n",
        "    if 'locality' in item and 'Relation_Specificity' in item['locality']:\n",
        "        for query in item['locality']['Relation_Specificity']:\n",
        "            gen_locality = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_locality, query['ground_truth']):\n",
        "                locality_correct += 1\n",
        "            locality_total += 1\n",
        "\n",
        "# --- 13. Display Results ---\n",
        "print(\"\\n--- üìä Evaluation Results (ICE + PiSSA + Unsloth) ---\")\n",
        "p_reliability = (reliability_correct / reliability_total * 100) if reliability_total > 0 else 0\n",
        "p_portability = (portability_correct / portability_total * 100) if portability_total > 0 else 0\n",
        "p_locality = (locality_correct / locality_total * 100) if locality_total > 0 else 0\n",
        "\n",
        "print(f\"1Ô∏è‚É£ Reliability (Edit Success):\")\n",
        "print(f\"   - Score:    {p_reliability:.2f}%\")\n",
        "print(f\"   - Correct:  {reliability_correct} / {reliability_total}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"2Ô∏è‚É£ Portability (Reasoning Generalization):\")\n",
        "print(f\"   - Score:    {p_portability:.2f}%\")\n",
        "print(f\"   - Correct:  {portability_correct} / {portability_total}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"3Ô∏è‚É£ Locality (Unrelated Fact Preservation):\")\n",
        "print(f\"   - Score:    {p_locality:.2f}%\")\n",
        "print(f\"   - Correct:  {locality_correct} / {locality_total}\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUTtxurjhSCn"
      },
      "source": [
        "#ice_para"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHp19F3OhThu",
        "outputId": "a14e6d07-452e-4037-f9d2-c8a8cabcbe8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "STEP 2: Hugging Face Authentication\n",
            "================================================================================\n",
            "‚úì Logged in to Hugging Face\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 2: Hugging Face Authentication\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "HF_TOKEN = \"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\"\n",
        "login(token=HF_TOKEN)\n",
        "print(\"‚úì Logged in to Hugging Face\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kebydHE-kVaN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaAttention, LlamaMLP\n",
        "from typing import Optional, Tuple\n",
        "import copy\n",
        "\n",
        "#\n",
        "# This file implements the PARA (Prompt Aware Representation Adjustment)\n",
        "# PEFT method, as described in arXiv:2406.11194v4.\n",
        "#\n",
        "\n",
        "class VectorGenerator(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Vector Generator (VG) from the PARA paper (Section 3.2, Eq 6).\n",
        "\n",
        "    This module takes the prompt's hidden states, pools them, and generates\n",
        "    the adjustment vectors l_q, l_v, and l_u.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ffn, r):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ffn = d_ffn\n",
        "        self.r = r # Bottleneck dimension\n",
        "\n",
        "        self.d_out = 2 * d_model + d_ffn\n",
        "\n",
        "        # Down-projection layer\n",
        "        self.down_proj = nn.Linear(d_model, r, bias=False)\n",
        "        self.activation = nn.GELU()\n",
        "        self.up_proj = nn.Linear(r, self.d_out, bias=True)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        nn.init.normal_(self.down_proj.weight, mean=0.0, std=0.02)\n",
        "        nn.init.zeros_(self.up_proj.weight)\n",
        "        nn.init.ones_(self.up_proj.bias)\n",
        "\n",
        "    def pooler(self, hidden_states: torch.Tensor, prompt_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Pools the hidden states, selecting ONLY the last token of the prompt.\n",
        "        hidden_states: [B, L, D]\n",
        "        prompt_mask: [B, L] (1 for prompt, 0 for padding/target)\n",
        "        \"\"\"\n",
        "        # Get the length of the prompt for each item in the batch\n",
        "        # Clamp(min=1) to avoid -1 index on empty prompts\n",
        "        prompt_lengths = torch.sum(prompt_mask, dim=1).clamp(min=1) # [B]\n",
        "        # Get the index of the last prompt token\n",
        "        last_token_indices = prompt_lengths - 1 # [B]\n",
        "\n",
        "        # Gather the hidden states for the last prompt token\n",
        "        batch_indices = torch.arange(hidden_states.size(0), device=hidden_states.device)\n",
        "        pooled = hidden_states[batch_indices, last_token_indices, :] # [B, D]\n",
        "        return pooled\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, prompt_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        # 1. Pool the *prompt* hidden states (using the mask)\n",
        "        pooled_output = self.pooler(hidden_states, prompt_mask) # (batch, d_model)\n",
        "\n",
        "        # 2. Down-projection\n",
        "        down_output = self.down_proj(pooled_output) # (batch, r)\n",
        "\n",
        "        # 3. Activation\n",
        "        activated_output = self.activation(down_output) # (batch, r)\n",
        "\n",
        "        # 4. Up-projection\n",
        "        l_combined = self.up_proj(activated_output) # (batch, d_out)\n",
        "\n",
        "        # 5. Split into l_q, l_v, l_u (Eq 6)\n",
        "        l_q, l_v, l_u = torch.split(\n",
        "            l_combined,\n",
        "            [self.d_model, self.d_model, self.d_ffn],\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        # Return vectors for caching, shape [B, D]\n",
        "        return l_q, l_v, l_u\n",
        "\n",
        "\n",
        "class ParaLlamaDecoderLayer(LlamaDecoderLayer):\n",
        "    \"\"\"\n",
        "    This class REPLACES the standard LlamaDecoderLayer.\n",
        "    It correctly implements the PARA logic:\n",
        "    1.  Generates l_q, l_v, l_u *once* from the prompt.\n",
        "    2.  Caches these vectors alongside the K, V cache.\n",
        "    3.  Applies the vectors at every step.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, layer_idx, r=12):\n",
        "        super().__init__(config, layer_idx)\n",
        "\n",
        "        self.vector_generator = VectorGenerator(\n",
        "            d_model=config.hidden_size,\n",
        "            d_ffn=config.intermediate_size,\n",
        "            r=r\n",
        "        )\n",
        "\n",
        "        # Store original modules\n",
        "        self.original_self_attn = self.self_attn\n",
        "        self.original_mlp = self.mlp\n",
        "        self.original_input_layernorm = self.input_layernorm\n",
        "        self.original_post_attention_layernorm = self.post_attention_layernorm\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = False,\n",
        "        prompt_attention_mask: Optional[torch.Tensor] = None, # New kwarg\n",
        "        **kwargs,\n",
        "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "\n",
        "        residual = hidden_states\n",
        "\n",
        "        # --- PARA Vector Generation & Caching ---\n",
        "        # The PARA paper states vectors are generated once from the prompt\n",
        "        # and reused. We implement this by adding them to the KV cache.\n",
        "\n",
        "        # `past_key_value` from transformers is (key_states, value_states)\n",
        "        # Our custom cache will be (key_states, value_states, para_vectors)\n",
        "        # where para_vectors = (l_q, l_v, l_u)\n",
        "\n",
        "        para_vectors_cache = None\n",
        "        if past_key_value is not None:\n",
        "            # We are generating (seq_len=1)\n",
        "            attn_past_key_value = (past_key_value[0], past_key_value[1])\n",
        "            para_vectors_cache = past_key_value[2]\n",
        "            l_q, l_v, l_u = para_vectors_cache\n",
        "        else:\n",
        "            # We are processing the prompt (seq_len > 1)\n",
        "            attn_past_key_value = None\n",
        "            if prompt_attention_mask is None:\n",
        "                # Fallback: if no mask, assume the *entire* sequence is the prompt\n",
        "                prompt_attention_mask = torch.ones_like(hidden_states[:, :, 0], dtype=torch.long)\n",
        "\n",
        "            # Generate vectors from the *full* hidden states, but tell\n",
        "            # the VG *where* the prompt is via the mask.\n",
        "            l_q, l_v, l_u = self.vector_generator(hidden_states, prompt_attention_mask)\n",
        "            para_vectors_cache = (l_q, l_v, l_u)\n",
        "\n",
        "        # Unsqueeze vectors for broadcasting: [B, D] -> [B, 1, D]\n",
        "        l_q = l_q.unsqueeze(1)\n",
        "        l_v = l_v.unsqueeze(1)\n",
        "        l_u = l_u.unsqueeze(1)\n",
        "        # --- End PARA Logic ---\n",
        "\n",
        "        hidden_states = self.original_input_layernorm(hidden_states)\n",
        "\n",
        "        # --- Self Attention (Modified with l_q, l_v) ---\n",
        "        query_states = self.original_self_attn.q_proj(hidden_states)\n",
        "        key_states = self.original_self_attn.k_proj(hidden_states)\n",
        "        value_states = self.original_self_attn.v_proj(hidden_states)\n",
        "\n",
        "        # --- PARA Injection (Eq 4) ---\n",
        "        query_states = l_q * query_states\n",
        "        value_states = l_v * value_states\n",
        "        # --- End PARA Injection ---\n",
        "\n",
        "        query_states = query_states.view(\n",
        "            query_states.shape[0], query_states.shape[1],\n",
        "            self.original_self_attn.num_heads,\n",
        "            self.original_self_attn.head_dim\n",
        "        ).transpose(1, 2)\n",
        "\n",
        "        key_states = key_states.view(\n",
        "            key_states.shape[0], key_states.shape[1],\n",
        "            self.original_self_attn.num_key_value_heads,\n",
        "            self.original_self_attn.head_dim\n",
        "        ).transpose(1, 2)\n",
        "\n",
        "        value_states = value_states.view(\n",
        "            value_states.shape[0], value_states.shape[1],\n",
        "            self.original_self_attn.num_key_value_heads,\n",
        "            self.original_self_attn.head_dim\n",
        "        ).transpose(1, 2)\n",
        "\n",
        "        kv_seq_len = key_states.shape[-2]\n",
        "        if attn_past_key_value is not None:\n",
        "            kv_seq_len += attn_past_key_value[0].shape[-2]\n",
        "\n",
        "        cos, sin = self.original_self_attn.rotary_emb(value_states, seq_len=kv_seq_len)\n",
        "        query_states, key_states = self.original_self_attn.apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
        "\n",
        "        if attn_past_key_value is not None:\n",
        "            key_states = torch.cat([attn_past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([attn_past_key_value[1], value_states], dim=2)\n",
        "\n",
        "        # Store full K, V states for *next* pass\n",
        "        attn_kv_cache_to_save = (key_states, value_states) if use_cache else None\n",
        "\n",
        "        key_states = LlamaAttention.repeat_kv(key_states, self.original_self_attn.num_key_value_groups)\n",
        "        value_states = LlamaAttention.repeat_kv(value_states, self.original_self_attn.num_key_value_groups)\n",
        "\n",
        "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / (self.original_self_attn.head_dim**0.5)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attn_weights = attn_weights + attention_mask\n",
        "\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "        attn_weights = nn.functional.dropout(attn_weights, p=self.original_self_attn.attention_dropout, training=self.training)\n",
        "        attn_output = torch.matmul(attn_weights, value_states)\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.reshape(attn_output.shape[0], attn_output.shape[1], self.original_self_attn.hidden_size)\n",
        "        attn_output = self.original_self_attn.o_proj(attn_output)\n",
        "\n",
        "        hidden_states = residual + attn_output\n",
        "\n",
        "        # --- MLP (Modified with l_u) ---\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.original_post_attention_layernorm(hidden_states)\n",
        "\n",
        "        gate_states = self.original_mlp.gate_proj(hidden_states)\n",
        "        up_states = self.original_mlp.up_proj(hidden_states)\n",
        "\n",
        "        # --- PARA Injection (Eq 5) ---\n",
        "        up_states = l_u * up_states\n",
        "        # --- End PARA Injection ---\n",
        "\n",
        "        activated_states = self.original_mlp.act_fn(gate_states) * up_states\n",
        "        hidden_states = self.original_mlp.down_proj(activated_states)\n",
        "\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (attn_weights,)\n",
        "\n",
        "        if use_cache:\n",
        "            # Combine the K, V cache with our PARA vectors cache\n",
        "            new_past_key_value = attn_kv_cache_to_save + (para_vectors_cache,)\n",
        "            outputs += (new_past_key_value,)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "jW6qHDCVlZ95",
        "outputId": "b01a18b6-c90b-4e60-e42c-e3dcf1aec3a7"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'para_model'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3881542083.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Import our custom PARA modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpara_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParaLlamaDecoderLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# --- 1. Configuration ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'para_model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, default_collate\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import copy\n",
        "\n",
        "# Import our custom PARA modules\n",
        "from para_model import ParaLlamaDecoderLayer\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
        "DATASET_PATH = \"path/to/your/wikidata_dataset.json\"\n",
        "SAVE_PATH = \"./llama-3.2-para-finetuned\"\n",
        "\n",
        "# PARA Hyperparameters\n",
        "PARA_R = 12 # Bottleneck dimension 'r' from paper (Sec 4.3)\n",
        "\n",
        "# Training Hyperparameters\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 1e-4 # Paper uses 1e-4 (Sec 4.3)\n",
        "BATCH_SIZE = 2\n",
        "MAX_LENGTH = 512\n",
        "MAX_NEW_TOKENS_EVAL = 50\n",
        "\n",
        "# --- 2. Setup Device ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 3. Load Model and Tokenizer ---\n",
        "print(\"Loading 4-bit model and tokenizer...\")\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- 4. Inject PARA Layers ---\n",
        "print(\"Injecting PARA layers into Llama 3.2 model...\")\n",
        "\n",
        "model.requires_grad_(False)\n",
        "original_layers = [copy.deepcopy(layer) for layer in model.model.layers]\n",
        "\n",
        "new_layers = []\n",
        "for i, layer in enumerate(tqdm(original_layers, desc=\"Replacing layers\")):\n",
        "    para_layer = ParaLlamaDecoderLayer(model.config, i, r=PARA_R)\n",
        "\n",
        "    # Load original weights, ignoring the new vector_generator keys\n",
        "    para_layer.load_state_dict(layer.state_dict(), strict=False)\n",
        "\n",
        "    # Set only the vector_generator to be trainable\n",
        "    para_layer.vector_generator.requires_grad_(True)\n",
        "\n",
        "    new_layers.append(para_layer)\n",
        "\n",
        "model.model.layers = nn.ModuleList(new_layers)\n",
        "model.lm_head.requires_grad_(True)\n",
        "\n",
        "print(\"PARA injection complete. Trainable parameters:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# # --- 5. Load Dataset and DataLoader ---\n",
        "# print(\"Loading and processing dataset...\")\n",
        "# try:\n",
        "#     raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"Warning: Dataset file not found at {DATASET_PATH}. Using dummy data.\")\n",
        "#     # dummy_data = [json.loads(r'{\"subject\":\"Leo Arons\",\"prompt\":\"The place of death of Leo Arons is\",\"target_new\":\"Berlin\",\"portability\":{},\"locality\":{},\"rephrase\":\"Leo Arons' place of death is\",\"context\":[\"Leo Arons passed away in Berlin.\"]}')]\n",
        "#     # raw_dataset = dummy_data\n",
        "\n",
        "# train_dataloader = DataLoader(\n",
        "#     raw_dataset,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     collate_fn=default_collate,\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "print(\"Loading and processing dataset...\")\n",
        "try:\n",
        "    raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "    # Slice only the first 100 samples (for example)\n",
        "    raw_dataset = raw_dataset.select(range(10))  # Or use any custom range\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Dataset file not found at {DATASET_PATH}. Using dummy data.\")\n",
        "    # dummy_data = [json.loads(r'{\"subject\":\"Leo Arons\",\"prompt\":\"The place of death of Leo Arons is\",\"target_new\":\"Berlin\",\"portability\":{},\"locality\":{},\"rephrase\":\"Leo Arons' place of death is\",\"context\":[\"Leo Arons passed away in Berlin.\"]}')]\n",
        "    # raw_dataset = dummy_data\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    raw_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=default_collate,\n",
        "    shuffle=True\n",
        ")\n",
        "# --- 6. Standard L_FT Loss Function ---\n",
        "def calculate_ft_loss(model, tokenizer, batch, device, max_length):\n",
        "    prompts = batch['prompt']\n",
        "    targets = batch['target_new']\n",
        "    batch_size = len(prompts)\n",
        "\n",
        "    ft_chats = [\n",
        "        [{\"role\": \"user\", \"content\": p}, {\"role\": \"assistant\", \"content\": t}]\n",
        "        for p, t in zip(prompts, targets)\n",
        "    ]\n",
        "\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    ft_encoded = tokenizer.apply_chat_template(\n",
        "        ft_chats,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    ft_input_ids = ft_encoded[\"input_ids\"].to(device)\n",
        "    ft_attention_mask = ft_encoded[\"attention_mask\"].to(device)\n",
        "    ft_labels = ft_input_ids.clone()\n",
        "\n",
        "    prompt_chats = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "    prompt_encoded = tokenizer.apply_chat_template(\n",
        "        prompt_chats,\n",
        "        add_generation_prompt=True,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # --- CORRECTED PROMPT MASK LOGIC ---\n",
        "    # Create a mask to identify *only* prompt tokens for the VG\n",
        "    prompt_attention_mask = torch.zeros_like(ft_attention_mask)\n",
        "    prompt_lengths = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        prompt_len = len(prompt_encoded[\"input_ids\"][i])\n",
        "        prompt_lengths.append(prompt_len)\n",
        "        prompt_attention_mask[i, :prompt_len] = 1 # Mark prompt tokens as 1\n",
        "\n",
        "    # Mask labels (for loss calculation)\n",
        "    for i in range(batch_size):\n",
        "        ft_labels[i, :prompt_lengths[i]] = -100\n",
        "\n",
        "    ft_labels[ft_attention_mask == 0] = -100\n",
        "\n",
        "    # --- END CORRECTION ---\n",
        "\n",
        "    outputs_ft = model(\n",
        "        input_ids=ft_input_ids,\n",
        "        attention_mask=ft_attention_mask,\n",
        "        labels=ft_labels,\n",
        "        prompt_attention_mask=prompt_attention_mask # Pass the new mask\n",
        "    )\n",
        "    return outputs_ft.loss\n",
        "\n",
        "# --- 7. Setup Optimizer ---\n",
        "print(\"Setting up optimizer...\")\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# --- 8. The Manual Training Loop ---\n",
        "print(\"Starting manual training loop (PARA + L_FT)...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1} / {NUM_EPOCHS} ---\")\n",
        "    total_epoch_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = calculate_ft_loss(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            batch,\n",
        "            device,\n",
        "            MAX_LENGTH\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = total_epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "# --- 9. Save the Final Model ---\n",
        "print(f\"Saving PARA model to {SAVE_PATH}...\")\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(\"Finetuning finished successfully.\")\n",
        "\n",
        "# ======================================================================\n",
        "# --- 10. Load and Evaluate ---\n",
        "# ======================================================================\n",
        "print(\"\\n--- Starting Evaluation ---\")\n",
        "print(\"Loading PARA-finetuned model...\")\n",
        "\n",
        "# We must register the custom layer with transformers\n",
        "# This is a bit advanced, but necessary for .from_pretrained\n",
        "from transformers import AutoConfig\n",
        "from para_model import ParaLlamaDecoderLayer\n",
        "\n",
        "config = AutoConfig.from_pretrained(SAVE_PATH, trust_remote_code=True)\n",
        "# This tells transformers to use our custom class\n",
        "config.auto_map = {\"AutoModelForCausalLM\": \"llama_finetune_para.LlamaForCausalLM\"}\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    SAVE_PATH,\n",
        "    config=config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True # Allow loading our custom para_model.py\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(SAVE_PATH)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# --- 11. Define Evaluation Helpers ---\n",
        "def generate_response(prompt, model, tokenizer, max_new_tokens=50):\n",
        "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs_encoded = tokenizer.apply_chat_template(\n",
        "        chat,\n",
        "        add_generation_prompt=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    inputs = inputs_encoded[\"input_ids\"].to(device)\n",
        "\n",
        "    # --- CORRECTED INFERENCE ---\n",
        "    # We must pass the attention mask as the prompt_attention_mask\n",
        "    # so the VG knows what to pool.\n",
        "    prompt_mask = inputs_encoded[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            attention_mask=prompt_mask,\n",
        "            prompt_attention_mask=prompt_mask, # Pass it here\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "    prompt_token_length = inputs.shape[1]\n",
        "    response_tokens = outputs[0][prompt_token_length:]\n",
        "    decoded_output = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "    return decoded_output.strip()\n",
        "\n",
        "def check_answer(generation, ground_truth_lists):\n",
        "# ... (rest of the function is unchanged) ...\n",
        "    generation_low = generation.lower().strip()\n",
        "    if not generation_low: return False\n",
        "    for alias_list in ground_truth_lists:\n",
        "        for alias in alias_list:\n",
        "            if alias.lower().strip() in generation_low:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# --- 12. Load Evaluation Data ---\n",
        "# ... (unchanged) ...\n",
        "print(f\"Loading evaluation data from {DATASET_PATH}...\")\n",
        "try:\n",
        "    eval_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Using dummy data for evaluation.\")\n",
        "    eval_dataset = raw_dataset\n",
        "\n",
        "# --- 13. Run Evaluation Loop ---\n",
        "# ... (unchanged) ...\n",
        "print(\"Running evaluation loop...\")\n",
        "reliability_correct, reliability_total = 0, 0\n",
        "portability_correct, portability_total = 0, 0\n",
        "locality_correct, locality_total = 0, 0\n",
        "\n",
        "for item in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
        "    target = item['target_new']\n",
        "# ... (rest of the loop is unchanged) ...\n",
        "    ground_truth_reliability = [[target]]\n",
        "\n",
        "    gen_main = generate_response(item['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_main, ground_truth_reliability): reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    gen_rephrase = generate_response(item['rephrase'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_rephrase, ground_truth_reliability): reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    if 'portability' in item and 'Reasoning' in item['portability']:\n",
        "        for query in item['portability']['Reasoning']:\n",
        "            gen_portability = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_portability, query['ground_truth']): portability_correct += 1\n",
        "            portability_total += 1\n",
        "\n",
        "    if 'locality' in item and 'Relation_Specificity' in item['locality']:\n",
        "        for query in item['locality']['Relation_Specificity']:\n",
        "            gen_locality = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_locality, query['ground_truth']): locality_correct += 1\n",
        "            locality_total += 1\n",
        "\n",
        "# --- 14. Display Results ---\n",
        "# ... (unchanged) ...\n",
        "print(\"\\n--- üìä Evaluation Results (PARA) ---\")\n",
        "p_reliability = (reliability_correct / reliability_total * 100) if reliability_total > 0 else 0\n",
        "p_portability = (portability_correct / portability_total * 100) if portability_total > 0 else 0\n",
        "p_locality = (locality_correct / locality_total * 100) if locality_total > 0 else 0\n",
        "\n",
        "print(f\"1Ô∏è‚É£ Reliability (Edit Success):\")\n",
        "print(f\"   - Score:    {p_reliability:.2f}%\")\n",
        "print(f\"   - Correct:  {reliability_correct} / {reliability_total}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"2Ô∏è‚É£ Portability (Reasoning Generalization):\")\n",
        "print(f\"   - Score:    {p_portability:.2f}%\")\n",
        "print(f\"   - Correct:  {portability_correct} / {portability_total}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"3Ô∏è‚É£ Locality (Unrelated Fact Preservation):\")\n",
        "print(f\"   - Score:    {p_locality:.2f}%\")\n",
        "print(f\"   - Correct:  {locality_correct} / {locality_total}\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qxT2icvNnQRp",
        "outputId": "72941360-5247-4f4c-e910-dd970e7546f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "NAAqHvwamEpU",
        "outputId": "3f48985d-db3b-4936-f566-dc9e8cbbe788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading model and tokenizer...\n",
            "Injecting PARA modules...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Patching layers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 512.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PARA injection complete.\n",
            "Trainable parameters: 265,617,408\n",
            "Loading dataset...\n",
            "Starting training...\n",
            "\n",
            "--- Epoch 1/1 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    741\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 13 at dim 1 (got 17)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4290509238.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_ft_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4290509238.py\u001b[0m in \u001b[0;36mcalculate_ft_loss\u001b[0;34m(model, tokenizer, batch, device, max_length)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;31m# Re-tokenize prompts only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m     prompt_encoded = tokenizer(\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3024\u001b[0m                 )\n\u001b[1;32m   3025\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   3027\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3225\u001b[0m         )\n\u001b[1;32m   3226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3228\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msanitized_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eventual_warn_about_too_long_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitized_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitized_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     def _encode_plus(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    809\u001b[0m                         \u001b[0;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     ) from e\n\u001b[0;32m--> 811\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    812\u001b[0m                     \u001b[0;34m\"Unable to create tensor, you should probably activate truncation and/or padding with\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                     \u001b[0;34m\" 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import types\n",
        "from typing import Optional, Tuple\n",
        "import math\n",
        "\n",
        "# ==================== PARA Vector Generator ====================\n",
        "class VectorGenerator(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Vector Generator (VG) from the PARA paper (Section 3.2, Eq 6).\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ffn, r):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ffn = d_ffn\n",
        "        self.r = r\n",
        "        self.d_out = 2 * d_model + d_ffn\n",
        "\n",
        "        self.down_proj = nn.Linear(d_model, r, bias=False)\n",
        "        self.activation = nn.GELU()\n",
        "        self.up_proj = nn.Linear(r, self.d_out, bias=True)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        nn.init.normal_(self.down_proj.weight, mean=0.0, std=0.02)\n",
        "        nn.init.zeros_(self.up_proj.weight)\n",
        "        nn.init.ones_(self.up_proj.bias)\n",
        "\n",
        "    def pooler(self, hidden_states: torch.Tensor, prompt_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Pools the hidden states, selecting the last token of the prompt.\n",
        "        hidden_states: [B, L, D]\n",
        "        prompt_mask: [B, L] (1 for prompt, 0 for padding/target)\n",
        "        \"\"\"\n",
        "        prompt_lengths = torch.sum(prompt_mask, dim=1).clamp(min=1)\n",
        "        last_token_indices = prompt_lengths - 1\n",
        "\n",
        "        batch_indices = torch.arange(hidden_states.size(0), device=hidden_states.device)\n",
        "        pooled = hidden_states[batch_indices, last_token_indices, :]\n",
        "        return pooled\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, prompt_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        pooled_output = self.pooler(hidden_states, prompt_mask)\n",
        "\n",
        "        # Cast pooled_output to the dtype of the down_proj weights (avoid dtype mismatch)\n",
        "        target_dtype = self.down_proj.weight.dtype\n",
        "        if pooled_output.dtype != target_dtype:\n",
        "            pooled_output = pooled_output.to(dtype=target_dtype)\n",
        "\n",
        "        down_output = self.down_proj(pooled_output)\n",
        "        activated_output = self.activation(down_output)\n",
        "        l_combined = self.up_proj(activated_output)\n",
        "\n",
        "        # Ensure the outputs are returned in the same dtype as model activations (optional)\n",
        "        # If you prefer them to be float32 for stability, you could cast back, but keep them as target_dtype\n",
        "        l_q, l_v, l_u = torch.split(\n",
        "            l_combined,\n",
        "            [self.d_model, self.d_model, self.d_ffn],\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        return l_q, l_v, l_u\n",
        "\n",
        "\n",
        "\n",
        "# ==================== Helper Functions ====================\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Repeats key/value states for grouped-query attention.\n",
        "    \"\"\"\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "\n",
        "\n",
        "def rotate_half(x):\n",
        "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
        "    \"\"\"Applies Rotary Position Embedding to query and key tensors.\"\"\"\n",
        "    cos = cos.squeeze(1).squeeze(0)\n",
        "    sin = sin.squeeze(1).squeeze(0)\n",
        "    cos = cos[position_ids].unsqueeze(1)\n",
        "    sin = sin[position_ids].unsqueeze(1)\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "\n",
        "# ==================== PARA Forward Method ====================\n",
        "def para_forward_method(\n",
        "    self,\n",
        "    hidden_states: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor] = None,\n",
        "    position_ids: Optional[torch.LongTensor] = None,\n",
        "    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "    output_attentions: Optional[bool] = False,\n",
        "    use_cache: Optional[bool] = False,\n",
        "    cache_position: Optional[torch.LongTensor] = None,\n",
        "    **kwargs,\n",
        ") -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "\n",
        "    residual = hidden_states\n",
        "\n",
        "    # --- PARA Vector Generation & Caching ---\n",
        "    if not hasattr(self, '_para_cache'):\n",
        "        self._para_cache = None\n",
        "\n",
        "    if past_key_value is not None and hidden_states.shape[1] == 1:\n",
        "        # Generation phase - reuse cached vectors\n",
        "        l_q, l_v, l_u = self._para_cache\n",
        "    else:\n",
        "        # Prefill phase - generate new vectors\n",
        "        # Get prompt mask from kwargs or create default\n",
        "        prompt_mask = kwargs.get('prompt_attention_mask', None)\n",
        "        if prompt_mask is None:\n",
        "            prompt_mask = torch.ones(\n",
        "                hidden_states.shape[0],\n",
        "                hidden_states.shape[1],\n",
        "                dtype=torch.long,\n",
        "                device=hidden_states.device\n",
        "            )\n",
        "\n",
        "        # Generate adjustment vectors\n",
        "        l_q, l_v, l_u = self.vector_generator(hidden_states, prompt_mask)\n",
        "        self._para_cache = (l_q, l_v, l_u)\n",
        "\n",
        "    # Reshape for broadcasting [B, D] -> [B, 1, D]\n",
        "    l_q = l_q.unsqueeze(1)\n",
        "    l_v = l_v.unsqueeze(1)\n",
        "    l_u = l_u.unsqueeze(1)\n",
        "\n",
        "    # --- Layer Norm ---\n",
        "    hidden_states = self.input_layernorm(hidden_states)\n",
        "\n",
        "    # --- Self Attention with PARA ---\n",
        "    bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "    query_states = self.self_attn.q_proj(hidden_states)\n",
        "    key_states = self.self_attn.k_proj(hidden_states)\n",
        "    value_states = self.self_attn.v_proj(hidden_states)\n",
        "\n",
        "    # === PARA ADJUSTMENT (Equation 4) ===\n",
        "    query_states = l_q * query_states\n",
        "    value_states = l_v * value_states\n",
        "    # ====================================\n",
        "\n",
        "    query_states = query_states.view(bsz, q_len, self.self_attn.num_heads, self.self_attn.head_dim).transpose(1, 2)\n",
        "    key_states = key_states.view(bsz, q_len, self.self_attn.num_key_value_heads, self.self_attn.head_dim).transpose(1, 2)\n",
        "    value_states = value_states.view(bsz, q_len, self.self_attn.num_key_value_heads, self.self_attn.head_dim).transpose(1, 2)\n",
        "\n",
        "    # Rotary embeddings\n",
        "    kv_seq_len = key_states.shape[-2]\n",
        "    if past_key_value is not None:\n",
        "        kv_seq_len += past_key_value[0].shape[-2]\n",
        "\n",
        "    cos, sin = self.self_attn.rotary_emb(value_states, seq_len=kv_seq_len)\n",
        "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
        "\n",
        "    # KV cache handling\n",
        "    if past_key_value is not None:\n",
        "        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "\n",
        "    past_key_value_out = (key_states, value_states) if use_cache else None\n",
        "\n",
        "    # Repeat KV for GQA\n",
        "    key_states = repeat_kv(key_states, self.self_attn.num_key_value_groups)\n",
        "    value_states = repeat_kv(value_states, self.self_attn.num_key_value_groups)\n",
        "\n",
        "    # Attention computation\n",
        "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.self_attn.head_dim)\n",
        "\n",
        "    if attention_mask is not None:\n",
        "        attn_weights = attn_weights + attention_mask\n",
        "\n",
        "    attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "    attn_weights = F.dropout(attn_weights, p=self.self_attn.attention_dropout, training=self.training)\n",
        "    attn_output = torch.matmul(attn_weights, value_states)\n",
        "\n",
        "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "    attn_output = attn_output.reshape(bsz, q_len, self.self_attn.hidden_size)\n",
        "    attn_output = self.self_attn.o_proj(attn_output)\n",
        "\n",
        "    hidden_states = residual + attn_output\n",
        "\n",
        "    # --- MLP with PARA ---\n",
        "    residual = hidden_states\n",
        "    hidden_states = self.post_attention_layernorm(hidden_states)\n",
        "\n",
        "    gate_states = self.mlp.gate_proj(hidden_states)\n",
        "    up_states = self.mlp.up_proj(hidden_states)\n",
        "\n",
        "    # === PARA ADJUSTMENT (Equation 5) ===\n",
        "    up_states = l_u * up_states\n",
        "    # ====================================\n",
        "\n",
        "    hidden_states = self.mlp.down_proj(self.mlp.act_fn(gate_states) * up_states)\n",
        "    hidden_states = residual + hidden_states\n",
        "\n",
        "    outputs = (hidden_states,)\n",
        "\n",
        "    if output_attentions:\n",
        "        outputs += (attn_weights,)\n",
        "\n",
        "    if use_cache:\n",
        "        outputs += (past_key_value_out,)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "# ==================== Configuration ====================\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
        "DATASET_PATH = \"wikidata_recent.json\"\n",
        "SAVE_PATH = \"./llama-3.2-para-finetuned\"\n",
        "\n",
        "PARA_R = 12\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 2\n",
        "MAX_LENGTH = 512\n",
        "MAX_NEW_TOKENS_EVAL = 50\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ==================== Load Model ====================\n",
        "print(\"Loading model and tokenizer...\")\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    config=config,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ==================== Inject PARA ====================\n",
        "print(\"Injecting PARA modules...\")\n",
        "\n",
        "model.requires_grad_(False)\n",
        "\n",
        "for layer in tqdm(model.model.layers, desc=\"Patching layers\"):\n",
        "    layer_dtype = next(layer.self_attn.q_proj.parameters()).dtype\n",
        "\n",
        "    layer.vector_generator = VectorGenerator(\n",
        "        d_model=config.hidden_size,\n",
        "        d_ffn=config.intermediate_size,\n",
        "        r=PARA_R\n",
        "    ).to(device, dtype=torch.bfloat16)\n",
        "\n",
        "    layer.vector_generator.requires_grad_(True)\n",
        "    layer.forward = types.MethodType(para_forward_method, layer)\n",
        "\n",
        "model.lm_head.requires_grad_(True)\n",
        "\n",
        "print(\"PARA injection complete.\")\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable:,}\")\n",
        "\n",
        "# ==================== Load Dataset ====================\n",
        "print(\"Loading dataset...\")\n",
        "try:\n",
        "    raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "    # Use subset for testing\n",
        "    raw_dataset = raw_dataset.select(range(min(10, len(raw_dataset))))\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    # print(\"Using dummy data...\")\n",
        "    # raw_dataset = [\n",
        "    #     {\n",
        "    #         \"subject\": \"Leo Arons\",\n",
        "    #         \"prompt\": \"The place of death of Leo Arons is\",\n",
        "    #         \"target_new\": \"Berlin\",\n",
        "    #         \"portability\": {\"Reasoning\": []},\n",
        "    #         \"locality\": {\"Relation_Specificity\": []},\n",
        "    #         \"rephrase\": \"Leo Arons' place of death is\",\n",
        "    #         \"context\": [\"Leo Arons passed away in Berlin.\"]\n",
        "    #     }\n",
        "    # ]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle batch properly\"\"\"\n",
        "    prompts = []\n",
        "    targets = []\n",
        "    rephrases = []\n",
        "    contexts = []\n",
        "\n",
        "    for item in batch:\n",
        "        prompts.append(item[\"prompt\"])\n",
        "        targets.append(item[\"target_new\"])\n",
        "        rephrases.append(item[\"rephrase\"])\n",
        "\n",
        "        # FIX: context can be list ‚Üí join it\n",
        "        ctx = item[\"context\"]\n",
        "        if isinstance(ctx, list):\n",
        "            ctx = \" \".join(ctx)\n",
        "        contexts.append(ctx)\n",
        "\n",
        "    return {\n",
        "        'prompt': prompts,\n",
        "        'target_new': targets,\n",
        "        'rephrase': rephrases,\n",
        "        'context': contexts\n",
        "    }\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    raw_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# ==================== Loss Function ====================\n",
        "def calculate_ft_loss(model, tokenizer, batch, device, max_length):\n",
        "\n",
        "    # FIX 1: ensure prompts are strings\n",
        "    prompts = [\n",
        "        p[0] if isinstance(p, list) else p\n",
        "        for p in batch[\"prompt\"]\n",
        "    ]\n",
        "\n",
        "    # FIX 2: ensure targets are strings\n",
        "    targets = [\n",
        "        t[0] if isinstance(t, list) else t\n",
        "        for t in batch[\"target_new\"]\n",
        "    ]\n",
        "\n",
        "    # Create full text (prompt + target)\n",
        "    ft_texts = [f\"{p} {t}\" for p, t in zip(prompts, targets)]\n",
        "\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    ft_encoded = tokenizer(\n",
        "        ft_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    ft_input_ids = ft_encoded[\"input_ids\"].to(device)\n",
        "    ft_attention_mask = ft_encoded[\"attention_mask\"].to(device)\n",
        "    ft_labels = ft_input_ids.clone()\n",
        "\n",
        "    # Re-tokenize prompts only\n",
        "    prompt_encoded = tokenizer(\n",
        "        prompts,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    batch_size = len(prompts)\n",
        "    prompt_attention_mask = torch.zeros_like(ft_attention_mask)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        prompt_len = len(prompt_encoded[\"input_ids\"][i])\n",
        "        prompt_attention_mask[i, :prompt_len] = 1\n",
        "        ft_labels[i, :prompt_len] = -100\n",
        "\n",
        "    ft_labels[ft_attention_mask == 0] = -100\n",
        "\n",
        "    outputs_ft = model(\n",
        "        input_ids=ft_input_ids,\n",
        "        attention_mask=ft_attention_mask,\n",
        "        labels=ft_labels,\n",
        "        prompt_attention_mask=prompt_attention_mask\n",
        "    )\n",
        "\n",
        "    return outputs_ft.loss\n",
        "\n",
        "\n",
        "# ==================== Training ====================\n",
        "print(\"Starting training...\")\n",
        "optimizer = AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = calculate_ft_loss(model, tokenizer, batch, device, MAX_LENGTH)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "# ==================== Save Model ====================\n",
        "print(f\"Saving model to {SAVE_PATH}...\")\n",
        "import os\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Save only trainable parameters\n",
        "trainable_state = {}\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        trainable_state[name] = param.cpu()\n",
        "\n",
        "torch.save(trainable_state, f\"{SAVE_PATH}/para_weights.pt\")\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "config.save_pretrained(SAVE_PATH)\n",
        "\n",
        "print(\"Model saved successfully.\")\n",
        "\n",
        "# ==================== Evaluation ====================\n",
        "print(\"\\n--- Starting Evaluation ---\")\n",
        "\n",
        "# Reload model for evaluation\n",
        "print(\"Loading model for evaluation...\")\n",
        "model_eval = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    config=config,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Re-inject PARA\n",
        "model_eval.requires_grad_(False)\n",
        "for layer in model_eval.model.layers:\n",
        "    layer.vector_generator = VectorGenerator(\n",
        "        d_model=config.hidden_size,\n",
        "        d_ffn=config.intermediate_size,\n",
        "        r=PARA_R\n",
        "    ).to(device, dtype=torch.bfloat16)\n",
        "    layer.forward = types.MethodType(para_forward_method, layer)\n",
        "\n",
        "# Load trained weights\n",
        "saved_weights = torch.load(f\"{SAVE_PATH}/para_weights.pt\", map_location=device)\n",
        "model_eval.load_state_dict(saved_weights, strict=False)\n",
        "model_eval.eval()\n",
        "\n",
        "# ==================== Evaluation Functions ====================\n",
        "def generate_response(prompt, model, tokenizer, max_new_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Create prompt mask for generation\n",
        "    prompt_mask = torch.ones_like(inputs[\"input_ids\"])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Clear cache before generation\n",
        "        for layer in model.model.layers:\n",
        "            if hasattr(layer, '_para_cache'):\n",
        "                layer._para_cache = None\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=True,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "def check_answer(generation, ground_truth_lists):\n",
        "    generation_low = generation.lower().strip()\n",
        "    if not generation_low:\n",
        "        return False\n",
        "    for alias_list in ground_truth_lists:\n",
        "        for alias in alias_list:\n",
        "            if alias.lower().strip() in generation_low:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# ==================== Run Evaluation ====================\n",
        "print(\"Running evaluation...\")\n",
        "\n",
        "reliability_correct, reliability_total = 0, 0\n",
        "portability_correct, portability_total = 0, 0\n",
        "locality_correct, locality_total = 0, 0\n",
        "\n",
        "eval_data = raw_dataset if isinstance(raw_dataset, list) else list(raw_dataset)\n",
        "\n",
        "for item in tqdm(eval_data, desc=\"Evaluating\"):\n",
        "    target = item['target_new']\n",
        "    ground_truth = [[target]]\n",
        "\n",
        "    # Test main prompt\n",
        "    gen_main = generate_response(item['prompt'], model_eval, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_main, ground_truth):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    # Test rephrase\n",
        "    gen_rephrase = generate_response(item['rephrase'], model_eval, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_rephrase, ground_truth):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    # Test portability\n",
        "    if 'portability' in item and 'Reasoning' in item['portability']:\n",
        "        for query in item['portability']['Reasoning']:\n",
        "            gen = generate_response(query['prompt'], model_eval, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen, query['ground_truth']):\n",
        "                portability_correct += 1\n",
        "            portability_total += 1\n",
        "\n",
        "    # Test locality\n",
        "    if 'locality' in item and 'Relation_Specificity' in item['locality']:\n",
        "        for query in item['locality']['Relation_Specificity']:\n",
        "            gen = generate_response(query['prompt'], model_eval, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen, query['ground_truth']):\n",
        "                locality_correct += 1\n",
        "            locality_total += 1\n",
        "\n",
        "# ==================== Display Results ====================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä EVALUATION RESULTS (PARA)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "p_reliability = (reliability_correct / reliability_total * 100) if reliability_total > 0 else 0\n",
        "p_portability = (portability_correct / portability_total * 100) if portability_total > 0 else 0\n",
        "p_locality = (locality_correct / locality_total * 100) if locality_total > 0 else 0\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£  Reliability (Edit Success):\")\n",
        "print(f\"    Score:    {p_reliability:.2f}%\")\n",
        "print(f\"    Correct:  {reliability_correct}/{reliability_total}\")\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£  Portability (Generalization):\")\n",
        "print(f\"    Score:    {p_portability:.2f}%\")\n",
        "print(f\"    Correct:  {portability_correct}/{portability_total}\")\n",
        "\n",
        "print(f\"\\n3Ô∏è‚É£  Locality (Preservation):\")\n",
        "print(f\"    Score:    {p_locality:.2f}%\")\n",
        "print(f\"    Correct:  {locality_correct}/{locality_total}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Evaluation Complete!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b2fb693715ad4b7ca791621a4b68ebcf",
            "212973c275e04af5ae6fbb793d78a3b2",
            "4d118d3562c84036a1e4b8ade82a8c2d",
            "9cbfec90e66f4c57af22593be8d5aa4c",
            "e2e3b80953f14b2fb0c6b2034b9d5ae3",
            "803135afe5ac48a593457a49786ffa2f",
            "e4e0939ba0bd4791a486997c55f02b20",
            "35049785ce4c4a46bea9f8e629cd678e",
            "d59f755f31424ea585a99d410db479e6",
            "acc9611dc25f46ccb765c182bae05f4e",
            "fe43d963438142b199cfac29dd8ce43c",
            "0f48293ac0094628892d85045b2162f0",
            "45e7be692f4341bda56e2598c530836d",
            "e94ec59b543d495794cc7a183cd2bcde",
            "f7b9e01c93af44cda8cb0f60f36baada",
            "fa3819708e8849669d3311fd3e0d51a9",
            "835139091a524f5597369df04e47cf2b",
            "7a5f9daa279b4ef6a90246751f9fba55",
            "5b36e9c13ed343f0bad53de1d2c1b462",
            "b603408d14464748bfbccfabad9683ba",
            "24d4bf7781674fda9a84895dd88bc61f",
            "2eaf8546756948c7be2931c4cba3d52c",
            "b05503eacfdd4d1197095e9789c94652",
            "89253dddd63e44f297af0c38d8de26ec",
            "d1adf80e4da6459c95a9b41b0a199fb4",
            "c16220d5e6a04ff2879632c12472606c",
            "9f1ce68a5b264afdbb3c1d8d599bded1",
            "680a9906f4204a95901b52596d38d6d4",
            "45c7790edc7f4c0fa64a97a2a6a42cd5",
            "2f9fbc4e505c4496971ade12b14940aa",
            "cb968bade46f402981ba861a5ee6da92",
            "1278234977b6406ca6f200899bc7947e",
            "fd58d5801a134f908490bfd948734185",
            "1ce28061b43a46aabb242cbcc2e431e1",
            "53a0525bbbb6438aa6d061a1d72ac5e3",
            "817b98b56dde4147ad18312794753261",
            "4b89e1a1be764272b8c565c605e16162",
            "eab3654541d247d18aee6cdbe19ab819",
            "58fb577f36c24db4b7bc80433679a108",
            "2c4db78dea714b7f87b91f0f8097277f",
            "b915fa56c8fc45b5a7af5d7e84e4b7d5",
            "c3f6e4f2ce854aa98e318ae4bb915b96",
            "94ad58d8e99e4b4d809d6cc2ddb51a67",
            "9d806a55104e4175bb0ee29ee59c553d",
            "40d702ac0be742eb8ca45f78437639df",
            "2a7df00be5be4340b1986487d9282b4e",
            "b730231515ae4deabcf513e28976c319",
            "7db9d69e3a2540cb835d41c52caf419f",
            "372868a0f4d148f1aeedc0a428a271e8",
            "a9c0cdb54b5741b18c469b2d65932d5e",
            "28137e106a86476fa4fe0d56bde98693",
            "7ad945f2bf4e41048b3f5cd2c730d822",
            "69c249441a40471188a8fe675be8bd83",
            "34067c2d6c7d42649baa093380319e90",
            "82bce4a080774120af0acd0fbb012d6a",
            "17fa712e4b294377a59412d2c56ece90",
            "53ba774d709a4cd3964507d888d016ee",
            "0674cbfe90b148128cc92c115382786d",
            "476d2aff38d14baeb6beb55c0d18b117",
            "21e2cf0dccbd46db8b7fcd7f6af152fa",
            "3febb9b08439410fb1ace4f0ae6274c7",
            "10ddde5af6fa4894b45ad41ffe3659cf",
            "fedd34397a954a4299423ac49f862176",
            "1452cffcb1554b129262da5caf6827f1",
            "1c1ad8d21997415d843459ffc10dcccb",
            "d038c4867d554c0c9468b73c5a92fe73",
            "afec3565c1d141f082a250dfc47a8eaf",
            "fe974d72ff12478ba5cfd44890f29a70",
            "fc5164d84174484ca241167a951d1169",
            "deaf943e47f84e8cbba94a17a537eff5",
            "cebc1853d6364d0aaff7067bb3a04bf0",
            "a351d450faee40de8d8a0581ced9de40",
            "e222ae11b8914ffd99e484b0da134935",
            "03f81462d076462aa158ba104ebbac95",
            "1b5d1d40c39142848e0236f1471ee239",
            "c8a3278dc5e3474ab9d5fdd010efe44d",
            "403e0018e82c4deeb003299a6ca0bda9"
          ]
        },
        "id": "lX3lCCJdnCL0",
        "outputId": "97d045c6-e1c6-4af7-e353-9119d8ca84cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PARA + ICE Loss for Knowledge Editing\n",
            "================================================================================\n",
            "\n",
            "‚úì PyTorch version: 2.8.0+cu126\n",
            "‚úì CUDA available: True\n",
            "‚úì CUDA device: Tesla T4\n",
            "‚úì GPU Memory: 15.83 GB\n",
            "\n",
            "Configuration:\n",
            "  Model: meta-llama/Llama-2-7b-chat-hf\n",
            "  Device: cuda\n",
            "  PARA bottleneck (r): 12\n",
            "  Learning rate: 0.0001\n",
            "  Epochs: 5\n",
            "  Batch size: 2\n",
            "  ICE Lambda: 0.5\n",
            "  Dataset: wikidata_recent.json\n",
            "\n",
            "================================================================================\n",
            "STEP 5: Loading Dataset\n",
            "================================================================================\n",
            "‚úì Loading dataset from: wikidata_recent.json\n",
            "‚úì Loaded 10 samples\n",
            "\n",
            "Sample data structure:\n",
            "  Subject: Leo Arons\n",
            "  Prompt: The place of death of Leo Arons is\n",
            "  Target: Berlin\n",
            "  Context examples: 5\n",
            "  Has portability: True\n",
            "  Has locality: True\n",
            "\n",
            "================================================================================\n",
            "STEP 8: Loading Model and Tokenizer\n",
            "================================================================================\n",
            "Loading tokenizer: meta-llama/Llama-2-7b-chat-hf\n",
            "‚úì Set pad_token to eos_token\n",
            "‚úì Tokenizer loaded. Vocab size: 32000\n",
            "\n",
            "Loading base model: meta-llama/Llama-2-7b-chat-hf\n",
            "Using 4-bit quantization...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2fb693715ad4b7ca791621a4b68ebcf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f48293ac0094628892d85045b2162f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b05503eacfdd4d1197095e9789c94652",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ce28061b43a46aabb242cbcc2e431e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40d702ac0be742eb8ca45f78437639df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17fa712e4b294377a59412d2c56ece90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afec3565c1d141f082a250dfc47a8eaf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Base model loaded\n",
            "‚úì Memory footprint: 3.76 GB\n",
            "\n",
            "\n",
            "Initializing PARA Module:\n",
            "  Model dimensions: d_model=4096, d_ffn=11008\n",
            "  Number of layers: 32\n",
            "\n",
            "  Freezing base model parameters...\n",
            "\n",
            "  Adding PARA vector generators:\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "    Vector Generator: 4096 ‚Üí 12 ‚Üí 19200\n",
            "      Parameters: 298,752\n",
            "\n",
            "  Total PARA parameters: 9,560,064 (9.56M)\n",
            "  Percentage of base model: 0.273%\n",
            "‚úì PARA model ready\n",
            "\n",
            "================================================================================\n",
            "STEP 9: Preparing Training\n",
            "================================================================================\n",
            "Preparing ICE Dataset...\n",
            "‚úì Prepared 10 samples\n",
            "\n",
            "Training setup:\n",
            "  Dataset size: 10\n",
            "  Batch size: 2\n",
            "  Steps per epoch: 5\n",
            "  Optimizer: AdamW\n",
            "  Learning rate: 0.0001\n",
            "\n",
            "================================================================================\n",
            "STEP 10: Training with PARA + ICE Loss\n",
            "================================================================================\n",
            "\n",
            "======================================================================\n",
            "Epoch 1/5\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    741\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 19 at dim 1 (got 28)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1924724427.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;31m# Calculate ICE loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         loss, l_ft, l_ice = calculate_ice_loss_para(\n\u001b[0m\u001b[1;32m    662\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1924724427.py\u001b[0m in \u001b[0;36mcalculate_ice_loss_para\u001b[0;34m(model, tokenizer, batch, device, max_length, lambda_ice)\u001b[0m\n\u001b[1;32m    436\u001b[0m     ]\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m     prompt_encoded = tokenizer(\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3024\u001b[0m                 )\n\u001b[1;32m   3025\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   3027\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3225\u001b[0m         )\n\u001b[1;32m   3226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3228\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msanitized_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eventual_warn_about_too_long_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitized_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitized_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     def _encode_plus(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    809\u001b[0m                         \u001b[0;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     ) from e\n\u001b[0;32m--> 811\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    812\u001b[0m                     \u001b[0;34m\"Unable to create tensor, you should probably activate truncation and/or padding with\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                     \u001b[0;34m\" 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PARA + ICE Loss Implementation for Knowledge Editing\n",
        "# Combining Prompt Aware Representation Adjustment with In-Context Editing\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This implementation combines:\n",
        "1. PARA: Efficient parameter tuning via prompt-aware vector generation\n",
        "2. ICE Loss: In-context consistency for knowledge internalization\n",
        "\n",
        "L = L_FT + Œª * L_ICE\n",
        "where:\n",
        "- L_FT: Standard fine-tuning loss\n",
        "- L_ICE: KL(p_Œ∏(x|[c,q]) || p_Œ∏(x|q)) - In-context consistency\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Imports and Setup\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    LlamaForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PARA + ICE Loss for Knowledge Editing\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n‚úì PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úì CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Configuration\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Model\n",
        "    model_name: str = \"meta-llama/Llama-2-7b-chat-hf\"  # or \"meta-llama/Llama-2-7b-hf\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # PARA Configuration\n",
        "    para_r: int = 12  # Bottleneck dimension for vector generator\n",
        "    para_init_std: float = 0.02\n",
        "\n",
        "    # Training\n",
        "    learning_rate: float = 1e-4\n",
        "    num_epochs: int = 5\n",
        "    batch_size: int = 2\n",
        "    max_length: int = 512\n",
        "\n",
        "    # ICE Loss\n",
        "    lambda_ice: float = 0.5\n",
        "    temperature: float = 1.0\n",
        "\n",
        "    # Paths\n",
        "    dataset_path: str = \"wikidata_recent.json\"\n",
        "    output_dir: str = \"./para_ice_llama\"\n",
        "\n",
        "    # Generation\n",
        "    max_new_tokens: int = 50\n",
        "\n",
        "    # Quantization\n",
        "    use_4bit: bool = True\n",
        "\n",
        "config = Config()\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {config.model_name}\")\n",
        "print(f\"  Device: {config.device}\")\n",
        "print(f\"  PARA bottleneck (r): {config.para_r}\")\n",
        "print(f\"  Learning rate: {config.learning_rate}\")\n",
        "print(f\"  Epochs: {config.num_epochs}\")\n",
        "print(f\"  Batch size: {config.batch_size}\")\n",
        "print(f\"  ICE Lambda: {config.lambda_ice}\")\n",
        "print(f\"  Dataset: {config.dataset_path}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: PARA Vector Generator Module\n",
        "# ============================================================================\n",
        "\n",
        "class PARAVectorGenerator(nn.Module):\n",
        "    \"\"\"\n",
        "    Prompt-Aware Vector Generator for PARA.\n",
        "\n",
        "    Generates adjustment vectors (l_q, l_v, l_u) based on input prompt.\n",
        "    Architecture: Pooler ‚Üí Down-projection ‚Üí Activation ‚Üí Up-projection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, d_ffn: int, r: int = 12, init_std: float = 0.02):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_ffn = d_ffn\n",
        "        self.r = r\n",
        "\n",
        "        # Bottleneck architecture\n",
        "        self.down_proj = nn.Linear(d_model, r, bias=False)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "        # Output dimension: 2*d_model (for l_q, l_v) + d_ffn (for l_u)\n",
        "        d_out = 2 * d_model + d_ffn\n",
        "        self.up_proj = nn.Linear(r, d_out, bias=True)\n",
        "\n",
        "        # Initialize (following PARA paper)\n",
        "        nn.init.normal_(self.down_proj.weight, mean=0.0, std=init_std)\n",
        "        nn.init.zeros_(self.up_proj.weight)\n",
        "        nn.init.ones_(self.up_proj.bias)  # Start with identity transformation\n",
        "\n",
        "        print(f\"    Vector Generator: {d_model} ‚Üí {r} ‚Üí {d_out}\")\n",
        "        print(f\"      Parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states: [batch_size, seq_len, d_model]\n",
        "\n",
        "        Returns:\n",
        "            l_q: [batch_size, d_model] - Query adjustment vector\n",
        "            l_v: [batch_size, d_model] - Value adjustment vector\n",
        "            l_u: [batch_size, d_ffn] - FFN Up adjustment vector\n",
        "        \"\"\"\n",
        "        # Pool: Take last token representation (following PARA paper)\n",
        "        h_pooled = hidden_states[:, -1, :]  # [batch_size, d_model]\n",
        "\n",
        "        # Bottleneck transformation\n",
        "        h = self.down_proj(h_pooled)  # [batch_size, r]\n",
        "        h = self.activation(h)        # [batch_size, r]\n",
        "        l = self.up_proj(h)           # [batch_size, d_out]\n",
        "\n",
        "        # Split into three adjustment vectors\n",
        "        l_q = l[:, :self.d_model]\n",
        "        l_v = l[:, self.d_model:2*self.d_model]\n",
        "        l_u = l[:, 2*self.d_model:]\n",
        "\n",
        "        return l_q, l_v, l_u\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: PARA Model Wrapper\n",
        "# ============================================================================\n",
        "\n",
        "class PARAModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Wraps a base LLaMA model with PARA vector generators.\n",
        "\n",
        "    Adds one vector generator per Transformer layer to generate\n",
        "    prompt-aware adjustment vectors for Q, V, and FFN Up projections.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model, config: Config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.base_model = base_model\n",
        "        self.config = config\n",
        "\n",
        "        # Get model dimensions\n",
        "        model_config = base_model.config\n",
        "        self.d_model = model_config.hidden_size\n",
        "        self.d_ffn = model_config.intermediate_size\n",
        "        self.num_layers = model_config.num_hidden_layers\n",
        "\n",
        "        print(f\"\\nInitializing PARA Module:\")\n",
        "        print(f\"  Model dimensions: d_model={self.d_model}, d_ffn={self.d_ffn}\")\n",
        "        print(f\"  Number of layers: {self.num_layers}\")\n",
        "\n",
        "        # Freeze base model\n",
        "        print(f\"\\n  Freezing base model parameters...\")\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Add vector generator for each layer\n",
        "        print(f\"\\n  Adding PARA vector generators:\")\n",
        "        self.vector_generators = nn.ModuleList([\n",
        "            PARAVectorGenerator(\n",
        "                d_model=self.d_model,\n",
        "                d_ffn=self.d_ffn,\n",
        "                r=config.para_r,\n",
        "                init_std=config.para_init_std\n",
        "            )\n",
        "            for layer_idx in range(self.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Cache for generated vectors (used during generation)\n",
        "        self.cached_vectors = {}\n",
        "        self.is_generation_mode = False\n",
        "\n",
        "        total_params = sum(p.numel() for p in self.vector_generators.parameters())\n",
        "        print(f\"\\n  Total PARA parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
        "        print(f\"  Percentage of base model: {total_params / sum(p.numel() for p in base_model.parameters()) * 100:.3f}%\")\n",
        "\n",
        "    def generate_vectors(self, hidden_states_list: List[torch.Tensor], layer_idx: int) -> Tuple:\n",
        "        \"\"\"Generate or retrieve cached adjustment vectors.\"\"\"\n",
        "        if self.is_generation_mode and layer_idx in self.cached_vectors:\n",
        "            return self.cached_vectors[layer_idx]\n",
        "\n",
        "        # Generate new vectors\n",
        "        vectors = self.vector_generators[layer_idx](hidden_states_list[layer_idx])\n",
        "\n",
        "        if self.is_generation_mode:\n",
        "            self.cached_vectors[layer_idx] = vectors\n",
        "\n",
        "        return vectors\n",
        "\n",
        "    def apply_para_to_attention(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        value: torch.Tensor,\n",
        "        l_q: torch.Tensor,\n",
        "        l_v: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Apply PARA adjustment to Q and V.\n",
        "\n",
        "        Args:\n",
        "            query: [batch, seq_len, num_heads, head_dim]\n",
        "            value: [batch, seq_len, num_heads, head_dim]\n",
        "            l_q: [batch, d_model]\n",
        "            l_v: [batch, d_model]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, num_heads, head_dim = query.shape\n",
        "\n",
        "        # Reshape l_q and l_v for broadcasting\n",
        "        # [batch, d_model] ‚Üí [batch, 1, num_heads, head_dim]\n",
        "        l_q_reshaped = l_q.view(batch_size, 1, num_heads, head_dim)\n",
        "        l_v_reshaped = l_v.view(batch_size, 1, num_heads, head_dim)\n",
        "\n",
        "        # Element-wise multiplication\n",
        "        query_adjusted = query * l_q_reshaped\n",
        "        value_adjusted = value * l_v_reshaped\n",
        "\n",
        "        return query_adjusted, value_adjusted\n",
        "\n",
        "    def apply_para_to_ffn(\n",
        "        self,\n",
        "        up_hidden: torch.Tensor,\n",
        "        l_u: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply PARA adjustment to FFN Up projection.\n",
        "\n",
        "        Args:\n",
        "            up_hidden: [batch, seq_len, d_ffn]\n",
        "            l_u: [batch, d_ffn]\n",
        "        \"\"\"\n",
        "        # Reshape for broadcasting: [batch, d_ffn] ‚Üí [batch, 1, d_ffn]\n",
        "        l_u_reshaped = l_u.unsqueeze(1)\n",
        "\n",
        "        # Element-wise multiplication\n",
        "        up_adjusted = up_hidden * l_u_reshaped\n",
        "\n",
        "        return up_adjusted\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_hidden_states: bool = True,\n",
        "        apply_para: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Forward pass with PARA adjustments.\n",
        "\n",
        "        Note: This is a simplified forward pass. For full integration,\n",
        "        we would need to modify the attention and FFN computations directly.\n",
        "        For this implementation, we'll use hooks to modify the computations.\n",
        "        \"\"\"\n",
        "        # Get outputs from base model\n",
        "        outputs = self.base_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def enable_generation_mode(self):\n",
        "        \"\"\"Enable caching for efficient generation.\"\"\"\n",
        "        self.is_generation_mode = True\n",
        "        self.cached_vectors = {}\n",
        "\n",
        "    def disable_generation_mode(self):\n",
        "        \"\"\"Disable caching (for training).\"\"\"\n",
        "        self.is_generation_mode = False\n",
        "        self.cached_vectors = {}\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Load Dataset\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 5: Loading Dataset\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check if file exists\n",
        "if not os.path.exists(config.dataset_path):\n",
        "    print(f\"‚úó ERROR: Dataset not found at {config.dataset_path}\")\n",
        "    print(f\"  Please provide the wikidata_recent.json file\")\n",
        "    raise FileNotFoundError(f\"Dataset not found: {config.dataset_path}\")\n",
        "\n",
        "print(f\"‚úì Loading dataset from: {config.dataset_path}\")\n",
        "\n",
        "with open(config.dataset_path, 'r', encoding='utf-8') as f:\n",
        "    wikidata = json.load(f)\n",
        "\n",
        "wikidata = wikidata[:10]\n",
        "\n",
        "print(f\"‚úì Loaded {len(wikidata)} samples\\n\")\n",
        "\n",
        "# Display first sample\n",
        "print(\"Sample data structure:\")\n",
        "print(f\"  Subject: {wikidata[0]['subject']}\")\n",
        "print(f\"  Prompt: {wikidata[0]['prompt']}\")\n",
        "print(f\"  Target: {wikidata[0]['target_new']}\")\n",
        "print(f\"  Context examples: {len(wikidata[0].get('context', []))}\")\n",
        "print(f\"  Has portability: {bool(wikidata[0].get('portability', {}))}\")\n",
        "print(f\"  Has locality: {bool(wikidata[0].get('locality', {}))}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: ICE Dataset\n",
        "# ============================================================================\n",
        "\n",
        "class ICEDataset(Dataset):\n",
        "    \"\"\"Dataset for In-Context Editing with PARA.\"\"\"\n",
        "\n",
        "    def __init__(self, data: List[Dict], tokenizer, max_length: int = 512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        print(f\"Preparing ICE Dataset...\")\n",
        "        self.samples = []\n",
        "\n",
        "        for idx, item in enumerate(data):\n",
        "            self.samples.append({\n",
        "                'prompt': item['prompt'],\n",
        "                'target': item['target_new'],\n",
        "                'context': item.get('context', []),\n",
        "                'subject': item['subject'],\n",
        "                'rephrase': item.get('rephrase', item['prompt']),\n",
        "                'portability': item.get('portability', {}),\n",
        "                'locality': item.get('locality', {})\n",
        "            })\n",
        "\n",
        "        print(f\"‚úì Prepared {len(self.samples)} samples\\n\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: ICE Loss Function\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_ice_loss_para(\n",
        "    model: PARAModel,\n",
        "    tokenizer,\n",
        "    batch: Dict,\n",
        "    device: str,\n",
        "    max_length: int,\n",
        "    lambda_ice: float\n",
        ") -> Tuple[torch.Tensor, float, float]:\n",
        "    \"\"\"\n",
        "    Calculate combined PARA + ICE loss.\n",
        "\n",
        "    L = L_FT + Œª * L_ICE\n",
        "    \"\"\"\n",
        "\n",
        "    prompts = batch['prompt']\n",
        "    targets = batch['target']\n",
        "    contexts = [\" \".join(c_list[:3]) if c_list else \"\" for c_list in batch['context']]\n",
        "    batch_size = len(prompts)\n",
        "\n",
        "    # ----- 1. L_FT (Fine-Tuning Loss) -----\n",
        "    # Create full conversations\n",
        "    ft_conversations = [\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": p},\n",
        "            {\"role\": \"assistant\", \"content\": t}\n",
        "        ]\n",
        "        for p, t in zip(prompts, targets)\n",
        "    ]\n",
        "\n",
        "    # Tokenize\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    ft_text = [\n",
        "        tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=False)\n",
        "        for conv in ft_conversations\n",
        "    ]\n",
        "\n",
        "    ft_encoded = tokenizer(\n",
        "        ft_text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    ft_input_ids = ft_encoded[\"input_ids\"].to(device)\n",
        "    ft_attention_mask = ft_encoded[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Create labels (mask prompt, keep assistant response)\n",
        "    ft_labels = ft_input_ids.clone()\n",
        "\n",
        "    # Tokenize prompts only to get lengths\n",
        "    prompt_conversations = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "    prompt_text = [\n",
        "        tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n",
        "        for conv in prompt_conversations\n",
        "    ]\n",
        "\n",
        "    prompt_encoded = tokenizer(\n",
        "        prompt_text,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Mask prompt tokens\n",
        "    for i in range(batch_size):\n",
        "        prompt_len = prompt_encoded[\"input_ids\"][i].shape[0]\n",
        "        ft_labels[i, :prompt_len] = -100\n",
        "\n",
        "    # Mask padding\n",
        "    ft_labels[ft_attention_mask == 0] = -100\n",
        "\n",
        "    # Forward pass for L_FT\n",
        "    outputs_ft = model(\n",
        "        input_ids=ft_input_ids,\n",
        "        attention_mask=ft_attention_mask,\n",
        "        labels=ft_labels\n",
        "    )\n",
        "    loss_ft = outputs_ft.loss\n",
        "\n",
        "    # ----- 2. L_ICE (In-Context Consistency Loss) -----\n",
        "    # Create base and context queries\n",
        "    base_conversations = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "    context_conversations = [\n",
        "        [{\"role\": \"user\", \"content\": f\"Context: {c}\\n\\nQuestion: {p}\"}]\n",
        "        for c, p in zip(contexts, prompts)\n",
        "    ]\n",
        "\n",
        "    # Tokenize base queries\n",
        "    base_text = [\n",
        "        tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n",
        "        for conv in base_conversations\n",
        "    ]\n",
        "\n",
        "    base_encoded = tokenizer(\n",
        "        base_text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Tokenize context queries\n",
        "    context_text = [\n",
        "        tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n",
        "        for conv in context_conversations\n",
        "    ]\n",
        "\n",
        "    context_encoded = tokenizer(\n",
        "        context_text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    base_input_ids = base_encoded[\"input_ids\"].to(device)\n",
        "    base_attention_mask = base_encoded[\"attention_mask\"].to(device)\n",
        "    context_input_ids = context_encoded[\"input_ids\"].to(device)\n",
        "    context_attention_mask = context_encoded[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Get logits for both (BOTH need gradients!)\n",
        "    logits_base = model(\n",
        "        input_ids=base_input_ids,\n",
        "        attention_mask=base_attention_mask\n",
        "    ).logits\n",
        "\n",
        "    logits_context = model(\n",
        "        input_ids=context_input_ids,\n",
        "        attention_mask=context_attention_mask\n",
        "    ).logits\n",
        "\n",
        "    # Align sequence lengths\n",
        "    min_len = min(logits_base.size(1), logits_context.size(1))\n",
        "    logits_base = logits_base[:, :min_len, :]\n",
        "    logits_context = logits_context[:, :min_len, :]\n",
        "    attention_mask_aligned = base_attention_mask[:, :min_len]\n",
        "\n",
        "    # Compute KL divergence: KL(p_Œ∏(x|[c,q]) || p_Œ∏(x|q))\n",
        "    log_p_context = F.log_softmax(logits_context / config.temperature, dim=-1)\n",
        "    p_base = F.softmax(logits_base / config.temperature, dim=-1)\n",
        "\n",
        "    kl_per_token = F.kl_div(\n",
        "        log_p_context,\n",
        "        p_base,\n",
        "        reduction='none',\n",
        "        log_target=False\n",
        "    ).sum(dim=-1)\n",
        "\n",
        "    # Mask and average\n",
        "    masked_kl = kl_per_token * attention_mask_aligned\n",
        "    loss_ice = masked_kl.sum() / (attention_mask_aligned.sum() + 1e-8)\n",
        "\n",
        "    # ----- 3. Combined Loss -----\n",
        "    loss_total = loss_ft + lambda_ice * loss_ice\n",
        "\n",
        "    return loss_total, loss_ft.item(), loss_ice.item()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: Load Model and Tokenizer\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 8: Loading Model and Tokenizer\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"Loading tokenizer: {config.model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    print(\"‚úì Set pad_token to eos_token\")\n",
        "\n",
        "print(f\"‚úì Tokenizer loaded. Vocab size: {len(tokenizer)}\\n\")\n",
        "\n",
        "print(f\"Loading base model: {config.model_name}\")\n",
        "\n",
        "if config.use_4bit:\n",
        "    print(\"Using 4-bit quantization...\")\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        config.model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "else:\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        config.model_name,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "print(f\"‚úì Base model loaded\")\n",
        "print(f\"‚úì Memory footprint: {base_model.get_memory_footprint() / 1e9:.2f} GB\\n\")\n",
        "\n",
        "# Wrap with PARA\n",
        "model = PARAModel(base_model, config)\n",
        "model.to(config.device)\n",
        "\n",
        "print(\"‚úì PARA model ready\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: Prepare Data and Optimizer\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 9: Preparing Training\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = ICEDataset(wikidata, tokenizer, config.max_length)\n",
        "\n",
        "# DataLoader\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle list fields.\"\"\"\n",
        "    return {\n",
        "        'prompt': [item['prompt'] for item in batch],\n",
        "        'target': [item['target'] for item in batch],\n",
        "        'context': [item['context'] for item in batch],\n",
        "        'subject': [item['subject'] for item in batch],\n",
        "        'rephrase': [item['rephrase'] for item in batch],\n",
        "        'portability': [item['portability'] for item in batch],\n",
        "        'locality': [item['locality'] for item in batch]\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Optimizer (only PARA parameters)\n",
        "optimizer = AdamW(model.vector_generators.parameters(), lr=config.learning_rate)\n",
        "\n",
        "print(f\"Training setup:\")\n",
        "print(f\"  Dataset size: {len(train_dataset)}\")\n",
        "print(f\"  Batch size: {config.batch_size}\")\n",
        "print(f\"  Steps per epoch: {len(train_loader)}\")\n",
        "print(f\"  Optimizer: AdamW\")\n",
        "print(f\"  Learning rate: {config.learning_rate}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: Training Loop\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 10: Training with PARA + ICE Loss\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model.train()\n",
        "epoch_losses = []\n",
        "\n",
        "for epoch in range(config.num_epochs):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_ft_loss = 0.0\n",
        "    total_ice_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate ICE loss\n",
        "        loss, l_ft, l_ice = calculate_ice_loss_para(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            batch,\n",
        "            config.device,\n",
        "            config.max_length,\n",
        "            config.lambda_ice\n",
        "        )\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.vector_generators.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate\n",
        "        total_loss += loss.item()\n",
        "        total_ft_loss += l_ft\n",
        "        total_ice_loss += l_ice\n",
        "\n",
        "        # Update progress\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f\"{loss.item():.4f}\",\n",
        "            'L_FT': f\"{l_ft:.4f}\",\n",
        "            'L_ICE': f\"{l_ice:.4f}\"\n",
        "        })\n",
        "\n",
        "        # Verbose logging\n",
        "        if step % 5 == 0:\n",
        "            print(f\"\\n  Step {step+1}/{len(train_loader)}:\")\n",
        "            print(f\"    Subject: {batch['subject'][0]}\")\n",
        "            print(f\"    Total Loss: {loss.item():.4f}\")\n",
        "            print(f\"    L_FT: {l_ft:.4f}\")\n",
        "            print(f\"    L_ICE: {l_ice:.4f}\")\n",
        "\n",
        "        # Clear cache\n",
        "        if step % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Epoch summary\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_ft = total_ft_loss / len(train_loader)\n",
        "    avg_ice = total_ice_loss / len(train_loader)\n",
        "\n",
        "    epoch_losses.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'total_loss': avg_loss,\n",
        "        'l_ft': avg_ft,\n",
        "        'l_ice': avg_ice\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{'‚îÄ'*70}\")\n",
        "    print(f\"Epoch {epoch + 1} Summary:\")\n",
        "    print(f\"  Avg Total Loss: {avg_loss:.4f}\")\n",
        "    print(f\"  Avg L_FT: {avg_ft:.4f}\")\n",
        "    print(f\"  Avg L_ICE: {avg_ice:.4f}\")\n",
        "    print(f\"{'‚îÄ'*70}\")\n",
        "\n",
        "print(\"\\n‚úì Training completed!\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 11: Save Model\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 11: Saving Model\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "os.makedirs(config.output_dir, exist_ok=True)\n",
        "\n",
        "# Save PARA vector generators\n",
        "torch.save({\n",
        "    'vector_generators': model.vector_generators.state_dict(),\n",
        "    'config': config,\n",
        "    'epoch_losses': epoch_losses\n",
        "}, os.path.join(config.output_dir, 'para_weights.pt'))\n",
        "\n",
        "tokenizer.save_pretrained(config.output_dir)\n",
        "\n",
        "# Save training history\n",
        "with open(os.path.join(config.output_dir, 'training_history.json'), 'w') as f:\n",
        "    json.dump(epoch_losses, f, indent=2)\n",
        "\n",
        "print(f\"‚úì Model saved to {config.output_dir}\")\n",
        "print(f\"‚úì Training history saved\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 12: Evaluation Functions\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 12: Setting Up Evaluation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def generate_response(\n",
        "    prompt: str,\n",
        "    model: PARAModel,\n",
        "    tokenizer,\n",
        "    max_new_tokens: int = 50,\n",
        "    temperature: float = 0.7\n",
        ") -> str:\n",
        "    \"\"\"Generate response from PARA model.\"\"\"\n",
        "    model.enable_generation_mode()\n",
        "    model.eval()\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(config.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.base_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    model.disable_generation_mode()\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "def check_answer_match(response: str, ground_truth_list: List[List[str]]) -> bool:\n",
        "    \"\"\"Check if response contains any ground truth answer.\"\"\"\n",
        "    response_lower = response.lower()\n",
        "\n",
        "    for aliases in ground_truth_list:\n",
        "        for alias in aliases:\n",
        "            if alias.lower() in response_lower:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "print(\"‚úì Evaluation functions ready\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 13: Evaluate Edit Success (Reliability)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 13: Evaluating Edit Success (Reliability)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "reliability_correct = 0\n",
        "reliability_total = 0\n",
        "\n",
        "for idx, sample in enumerate(train_dataset.samples[:min(10, len(train_dataset))]):\n",
        "    print(f\"\\n{'‚îÄ'*70}\")\n",
        "    print(f\"Sample {idx + 1}: {sample['subject']}\")\n",
        "    print(f\"{'‚îÄ'*70}\")\n",
        "\n",
        "    # Test main prompt\n",
        "    print(f\"Prompt: {sample['prompt']}\")\n",
        "    print(f\"Expected: {sample['target']}\")\n",
        "\n",
        "    response_main = generate_response(sample['prompt'], model, tokenizer, config.max_new_tokens)\n",
        "    print(f\"Generated: {response_main}\")\n",
        "\n",
        "    success_main = sample['target'].lower() in response_main.lower()\n",
        "    if success_main:\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "    print(f\"‚úì Success (main): {success_main}\")\n",
        "\n",
        "    # Test rephrase\n",
        "    if sample['rephrase']:\n",
        "        print(f\"\\nRephrase: {sample['rephrase']}\")\n",
        "        response_rephrase = generate_response(sample['rephrase'], model, tokenizer, config.max_new_tokens)\n",
        "        print(f\"Generated: {response_rephrase}\")\n",
        "\n",
        "        success_rephrase = sample['target'].lower() in response_rephrase.lower()\n",
        "        if success_rephrase:\n",
        "            reliability_correct += 1\n",
        "        reliability_total += 1\n",
        "        print(f\"‚úì Success (rephrase): {success_rephrase}\")\n",
        "\n",
        "reliability_rate = (reliability_correct / reliability_total * 100) if reliability_total > 0 else 0\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"RELIABILITY (Edit Success): {reliability_rate:.1f}%\")\n",
        "print(f\"  Correct: {reliability_correct} / {reliability_total}\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 14: Evaluate Portability\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 14: Evaluating Portability\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "portability_correct = 0\n",
        "portability_total = 0\n",
        "\n",
        "for idx, sample in enumerate(train_dataset.samples[:min(10, len(train_dataset))]):\n",
        "    portability_data = sample['portability']\n",
        "\n",
        "    if not portability_data:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'‚îÄ'*70}\")\n",
        "    print(f\"Sample {idx + 1}: {sample['subject']}\")\n",
        "    print(f\"{'‚îÄ'*70}\")\n",
        "\n",
        "    # Test Logical Generalization\n",
        "    if 'Logical_Generalization' in portability_data:\n",
        "        print(\"\\nTesting Logical Generalization:\")\n",
        "\n",
        "        for port_idx, port_test in enumerate(portability_data['Logical_Generalization'][:2]):\n",
        "            prompt = port_test['prompt']\n",
        "            ground_truths = port_test['ground_truth']\n",
        "\n",
        "            print(f\"\\n  Test {port_idx + 1}:\")\n",
        "            print(f\"    Prompt: {prompt}\")\n",
        "\n",
        "            response = generate_response(prompt, model, tokenizer, config.max_new_tokens)\n",
        "            print(f\"    Generated: {response}\")\n",
        "\n",
        "            success = check_answer_match(response, ground_truths)\n",
        "            if success:\n",
        "                portability_correct += 1\n",
        "            portability_total += 1\n",
        "            print(f\"    ‚úì Success: {success}\")\n",
        "\n",
        "    # Test Reasoning\n",
        "    if 'Reasoning' in portability_data:\n",
        "        print(\"\\nTesting Reasoning:\")\n",
        "\n",
        "        for port_idx, port_test in enumerate(portability_data['Reasoning'][:2]):\n",
        "            prompt = port_test['prompt']\n",
        "            ground_truths = port_test['ground_truth']\n",
        "\n",
        "            print(f\"\\n  Test {port_idx + 1}:\")\n",
        "            print(f\"    Prompt: {prompt}\")\n",
        "\n",
        "            response = generate_response(prompt, model, tokenizer, config.max_new_tokens)\n",
        "            print(f\"    Generated: {response}\")\n",
        "\n",
        "            success = check_answer_match(response, ground_truths)\n",
        "            if success:\n",
        "                portability_correct += 1\n",
        "            portability_total += 1\n",
        "            print(f\"    ‚úì Success: {success}\")\n",
        "\n",
        "    # Test Subject Aliasing\n",
        "    if 'Subject_Aliasing' in portability_data:\n",
        "        print(\"\\nTesting Subject Aliasing:\")\n",
        "\n",
        "        for port_idx, port_test in enumerate(portability_data['Subject_Aliasing'][:2]):\n",
        "            prompt = port_test['prompt']\n",
        "            ground_truths = port_test['ground_truth']\n",
        "\n",
        "            print(f\"\\n  Test {port_idx + 1}:\")\n",
        "            print(f\"    Prompt: {prompt}\")\n",
        "\n",
        "            response = generate_response(prompt, model, tokenizer, config.max_new_tokens)\n",
        "            print(f\"    Generated: {response}\")\n",
        "\n",
        "            success = check_answer_match(response, ground_truths)\n",
        "            if success:\n",
        "                portability_correct += 1\n",
        "            portability_total += 1\n",
        "            print(f\"    ‚úì Success: {success}\")\n",
        "\n",
        "portability_rate = (portability_correct / portability_total * 100) if portability_total > 0 else 0\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"PORTABILITY: {portability_rate:.1f}%\")\n",
        "print(f\"  Correct: {portability_correct} / {portability_total}\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 15: Evaluate Locality\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 15: Evaluating Locality\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "locality_correct = 0\n",
        "locality_total = 0\n",
        "\n",
        "for idx, sample in enumerate(train_dataset.samples[:min(10, len(train_dataset))]):\n",
        "    locality_data = sample['locality']\n",
        "\n",
        "    if not locality_data:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'‚îÄ'*70}\")\n",
        "    print(f\"Sample {idx + 1}: {sample['subject']}\")\n",
        "    print(f\"{'‚îÄ'*70}\")\n",
        "\n",
        "    # Test Relation Specificity\n",
        "    if 'Relation_Specificity' in locality_data:\n",
        "        print(\"\\nTesting Relation Specificity (Unrelated Facts):\")\n",
        "\n",
        "        for loc_idx, loc_test in enumerate(locality_data['Relation_Specificity'][:3]):\n",
        "            prompt = loc_test['prompt']\n",
        "            ground_truths = loc_test['ground_truth']\n",
        "\n",
        "            print(f\"\\n  Test {loc_idx + 1}:\")\n",
        "            print(f\"    Prompt: {prompt}\")\n",
        "\n",
        "            response = generate_response(prompt, model, tokenizer, config.max_new_tokens)\n",
        "            print(f\"    Generated: {response}\")\n",
        "\n",
        "            success = check_answer_match(response, ground_truths)\n",
        "            if success:\n",
        "                locality_correct += 1\n",
        "            locality_total += 1\n",
        "            print(f\"    ‚úì Preserved: {success}\")\n",
        "\n",
        "locality_rate = (locality_correct / locality_total * 100) if locality_total > 0 else 0\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"LOCALITY: {locality_rate:.1f}%\")\n",
        "print(f\"  Correct: {locality_correct} / {locality_total}\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 16: Training Loss History\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 16: Training Loss History\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nLoss per Epoch:\")\n",
        "print(f\"{'‚îÄ'*70}\")\n",
        "print(f\"{'Epoch':<10} {'Total Loss':<15} {'L_FT':<15} {'L_ICE':<15}\")\n",
        "print(f\"{'‚îÄ'*70}\")\n",
        "\n",
        "for loss_record in epoch_losses:\n",
        "    print(f\"{loss_record['epoch']:<10} \"\n",
        "          f\"{loss_record['total_loss']:<15.4f} \"\n",
        "          f\"{loss_record['l_ft']:<15.4f} \"\n",
        "          f\"{loss_record['l_ice']:<15.4f}\")\n",
        "\n",
        "print(f\"{'‚îÄ'*70}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 17: Final Summary\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 17: FINAL EVALUATION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nüìä PARA + ICE Fine-tuning Results:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"  1Ô∏è‚É£  Reliability (Edit Success):  {reliability_rate:.1f}% ({reliability_correct}/{reliability_total})\")\n",
        "print(f\"  2Ô∏è‚É£  Portability:                 {portability_rate:.1f}% ({portability_correct}/{portability_total})\")\n",
        "print(f\"  3Ô∏è‚É£  Locality:                    {locality_rate:.1f}% ({locality_correct}/{locality_total})\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nüìà Training Metrics:\")\n",
        "print(f\"{'‚îÄ'*70}\")\n",
        "if epoch_losses:\n",
        "    print(f\"  Initial Total Loss:  {epoch_losses[0]['total_loss']:.4f}\")\n",
        "    print(f\"  Final Total Loss:    {epoch_losses[-1]['total_loss']:.4f}\")\n",
        "    print(f\"  Final L_FT:          {epoch_losses[-1]['l_ft']:.4f}\")\n",
        "    print(f\"  Final L_ICE:         {epoch_losses[-1]['l_ice']:.4f}\")\n",
        "\n",
        "    loss_reduction = ((epoch_losses[0]['total_loss'] - epoch_losses[-1]['total_loss'])\n",
        "                     / epoch_losses[0]['total_loss'] * 100)\n",
        "    print(f\"  Loss Reduction:      {loss_reduction:.1f}%\")\n",
        "print(f\"{'‚îÄ'*70}\")\n",
        "\n",
        "print(\"\\nüí° Method Insights:\")\n",
        "print(\"  ‚Ä¢ PARA (Prompt Aware Representation Adjustment):\")\n",
        "print(\"    - Uses lightweight vector generators per layer\")\n",
        "print(\"    - Generates prompt-aware adjustment vectors (l_q, l_v, l_u)\")\n",
        "print(\"    - Element-wise multiplication (efficient inference)\")\n",
        "print(\"    - Vectors cached during generation (KV-cache compatible)\")\n",
        "print()\n",
        "print(\"  ‚Ä¢ ICE Loss (In-Context Editing):\")\n",
        "print(\"    - L_FT: Standard fine-tuning loss\")\n",
        "print(\"    - L_ICE: KL(p_Œ∏(x|[c,q]) || p_Œ∏(x|q))\")\n",
        "print(\"    - Ensures context-independent knowledge internalization\")\n",
        "print(\"    - After training: model doesn't need context to answer\")\n",
        "print()\n",
        "print(\"  ‚Ä¢ Combined Benefits:\")\n",
        "print(\"    - Efficient: ~9M parameters (~0.13% of base model)\")\n",
        "print(\"    - Fast inference: No matrix multiply per token\")\n",
        "print(\"    - Prompt-aware: Adapts to different inputs\")\n",
        "print(\"    - Generalizable: High portability scores\")\n",
        "\n",
        "print(f\"\\nüíæ Model and Results Saved:\")\n",
        "print(f\"  ‚Ä¢ PARA weights: {config.output_dir}/para_weights.pt\")\n",
        "print(f\"  ‚Ä¢ Tokenizer: {config.output_dir}/\")\n",
        "print(f\"  ‚Ä¢ Training history: {config.output_dir}/training_history.json\")\n",
        "\n",
        "print(\"\\nüîÑ To Load the Model:\")\n",
        "print(\"  ```python\")\n",
        "print(\"  # Load base model\")\n",
        "print(f\"  base_model = AutoModelForCausalLM.from_pretrained('{config.model_name}')\")\n",
        "print()\n",
        "print(\"  # Create PARA wrapper\")\n",
        "print(\"  model = PARAModel(base_model, config)\")\n",
        "print()\n",
        "print(\"  # Load trained weights\")\n",
        "print(f\"  checkpoint = torch.load('{config.output_dir}/para_weights.pt')\")\n",
        "print(\"  model.vector_generators.load_state_dict(checkpoint['vector_generators'])\")\n",
        "print(\"  ```\")\n",
        "\n",
        "# Save final evaluation results\n",
        "eval_results = {\n",
        "    'reliability': {\n",
        "        'rate': reliability_rate,\n",
        "        'correct': reliability_correct,\n",
        "        'total': reliability_total\n",
        "    },\n",
        "    'portability': {\n",
        "        'rate': portability_rate,\n",
        "        'correct': portability_correct,\n",
        "        'total': portability_total\n",
        "    },\n",
        "    'locality': {\n",
        "        'rate': locality_rate,\n",
        "        'correct': locality_correct,\n",
        "        'total': locality_total\n",
        "    },\n",
        "    'training': {\n",
        "        'epochs': config.num_epochs,\n",
        "        'final_loss': epoch_losses[-1] if epoch_losses else None\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(config.output_dir, 'evaluation_results.json'), 'w') as f:\n",
        "    json.dump(eval_results, f, indent=2)\n",
        "\n",
        "print(f\"  ‚Ä¢ Evaluation results: {config.output_dir}/evaluation_results.json\")\n",
        "\n",
        "print(\"\\n‚úÖ PARA + ICE Implementation Complete!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 18: Context Independence Test (Bonus)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 18: Testing Context Independence\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Verifying that p_Œ∏(x|[c,q]) ‚âà p_Œ∏(x|q) after training\\n\")\n",
        "\n",
        "for idx in range(min(3, len(train_dataset))):\n",
        "    sample = train_dataset.samples[idx]\n",
        "\n",
        "    print(f\"\\n{'‚îÄ'*70}\")\n",
        "    print(f\"Sample {idx + 1}: {sample['subject']}\")\n",
        "    print(f\"{'‚îÄ'*70}\")\n",
        "\n",
        "    prompt = sample['prompt']\n",
        "    target = sample['target']\n",
        "    contexts = sample['context']\n",
        "\n",
        "    print(f\"\\nQuery: {prompt}\")\n",
        "    print(f\"Expected: {target}\")\n",
        "\n",
        "    # Test WITHOUT context\n",
        "    print(\"\\n1Ô∏è‚É£  WITHOUT Context:\")\n",
        "    response_no_context = generate_response(prompt, model, tokenizer, max_new_tokens=30)\n",
        "    print(f\"   Response: {response_no_context}\")\n",
        "\n",
        "    # Test WITH context\n",
        "    if contexts:\n",
        "        print(\"\\n2Ô∏è‚É£  WITH Context:\")\n",
        "        context_text = \" \".join(contexts[:2])\n",
        "        context_prompt = f\"Context: {context_text}\\n\\nQuestion: {prompt}\"\n",
        "        response_with_context = generate_response(context_prompt, model, tokenizer, max_new_tokens=30)\n",
        "        print(f\"   Response: {response_with_context}\")\n",
        "\n",
        "        # Check consistency\n",
        "        target_in_no_context = target.lower() in response_no_context.lower()\n",
        "        target_in_with_context = target.lower() in response_with_context.lower()\n",
        "\n",
        "        consistent = target_in_no_context and target_in_with_context\n",
        "\n",
        "        print(f\"\\n3Ô∏è‚É£  Analysis:\")\n",
        "        print(f\"   ‚úì Target in no-context: {target_in_no_context}\")\n",
        "        print(f\"   ‚úì Target in with-context: {target_in_with_context}\")\n",
        "        print(f\"   ‚úì Context-independent: {consistent}\")\n",
        "\n",
        "        if consistent:\n",
        "            print(\"   ‚Üí Knowledge successfully internalized! üéØ\")\n",
        "        else:\n",
        "            print(\"   ‚Üí May benefit from more training or higher Œª\")\n",
        "    else:\n",
        "        print(\"   No context examples available\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ All Evaluations Complete!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nüéØ Summary:\")\n",
        "print(\"  ‚úì Implemented PARA vector generators\")\n",
        "print(\"  ‚úì Combined with ICE loss for knowledge editing\")\n",
        "print(\"  ‚úì Trained on WikiData samples\")\n",
        "print(\"  ‚úì Evaluated reliability, portability, and locality\")\n",
        "print(\"  ‚úì Tested context independence\")\n",
        "print(\"  ‚úì Saved all results and model weights\")\n",
        "\n",
        "print(\"\\nüìä Key Advantages of PARA + ICE:\")\n",
        "print(\"  ‚Ä¢ Parameter Efficient: ~9M parameters (0.13% of base)\")\n",
        "print(\"  ‚Ä¢ Fast Inference: Element-wise ops only (KV-cache compatible)\")\n",
        "print(\"  ‚Ä¢ Prompt Aware: Adapts vectors to each input\")\n",
        "print(\"  ‚Ä¢ Knowledge Internalization: ICE loss ensures consistency\")\n",
        "print(\"  ‚Ä¢ Multi-tenant Ready: Efficient for serving multiple tasks\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eimsJBsiH3gz",
        "outputId": "9410227f-456c-4e44-e045-5bd6695dde26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "cGElNJ_THM2Y",
        "outputId": "485fa129-9c0e-4590-d70e-46685ba8d8d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PARA + ICE Loss for Knowledge Editing - FIXED VERSION\n",
            "================================================================================\n",
            "\n",
            "‚úì PyTorch version: 2.8.0+cu126\n",
            "‚úì CUDA available: True\n",
            "‚úì CUDA device: Tesla T4\n",
            "‚úì GPU Memory: 15.83 GB\n",
            "\n",
            "Configuration:\n",
            "  Model: meta-llama/Llama-3.2-1B\n",
            "  Device: cuda\n",
            "  PARA bottleneck (r): 12\n",
            "  Learning rate: 0.0001\n",
            "  Epochs: 3\n",
            "  Batch size: 2\n",
            "  ICE Lambda: 0.5\n",
            "\n",
            "================================================================================\n",
            "Loading Dataset\n",
            "================================================================================\n"
          ]
        },
        {
          "ename": "JSONDecodeError",
          "evalue": "Expecting ',' delimiter: line 53530 column 3 (char 2097152)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-186920231.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mwikidata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;31m# Limit dataset size for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \"\"\"\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 53530 column 3 (char 2097152)"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PARA + ICE Loss Implementation for Knowledge Editing - FIXED VERSION\n",
        "# Combining Prompt Aware Representation Adjustment with In-Context Editing\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This implementation combines:\n",
        "1. PARA: Efficient parameter tuning via prompt-aware vector generation\n",
        "2. ICE Loss: In-context consistency for knowledge internalization\n",
        "\n",
        "L = L_FT + Œª * L_ICE\n",
        "where:\n",
        "- L_FT: Standard fine-tuning loss\n",
        "- L_ICE: KL(p_Œ∏(x|[c,q]) || p_Œ∏(x|q)) - In-context consistency\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Imports and Setup\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PARA + ICE Loss for Knowledge Editing - FIXED VERSION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n‚úì PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úì CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Configuration\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Model\n",
        "    model_name: str = \"meta-llama/Llama-3.2-1B\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # PARA Configuration\n",
        "    para_r: int = 12\n",
        "    para_init_std: float = 0.02\n",
        "\n",
        "    # Training\n",
        "    learning_rate: float = 1e-4\n",
        "    num_epochs: int = 3\n",
        "    batch_size: int = 2\n",
        "    max_length: int = 256\n",
        "\n",
        "    # ICE Loss\n",
        "    lambda_ice: float = 0.5\n",
        "    temperature: float = 1.0\n",
        "\n",
        "    # Paths\n",
        "    dataset_path: str = \"wikidata_recent.json\"\n",
        "    output_dir: str = \"./para_ice_llama\"\n",
        "\n",
        "    # Generation\n",
        "    max_new_tokens: int = 50\n",
        "\n",
        "    # Quantization\n",
        "    use_4bit: bool = True\n",
        "\n",
        "config = Config()\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {config.model_name}\")\n",
        "print(f\"  Device: {config.device}\")\n",
        "print(f\"  PARA bottleneck (r): {config.para_r}\")\n",
        "print(f\"  Learning rate: {config.learning_rate}\")\n",
        "print(f\"  Epochs: {config.num_epochs}\")\n",
        "print(f\"  Batch size: {config.batch_size}\")\n",
        "print(f\"  ICE Lambda: {config.lambda_ice}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: PARA Vector Generator Module\n",
        "# ============================================================================\n",
        "\n",
        "class PARAVectorGenerator(nn.Module):\n",
        "    \"\"\"Prompt-Aware Vector Generator for PARA.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, d_ffn: int, r: int = 12, init_std: float = 0.02):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_ffn = d_ffn\n",
        "        self.r = r\n",
        "\n",
        "        self.down_proj = nn.Linear(d_model, r, bias=False)\n",
        "        self.activation = nn.GELU()\n",
        "        d_out = 2 * d_model + d_ffn\n",
        "        self.up_proj = nn.Linear(r, d_out, bias=True)\n",
        "\n",
        "        nn.init.normal_(self.down_proj.weight, mean=0.0, std=init_std)\n",
        "        nn.init.zeros_(self.up_proj.weight)\n",
        "        nn.init.ones_(self.up_proj.bias)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        h_pooled = hidden_states[:, -1, :]\n",
        "        h = self.down_proj(h_pooled)\n",
        "        h = self.activation(h)\n",
        "        l = self.up_proj(h)\n",
        "\n",
        "        l_q = l[:, :self.d_model]\n",
        "        l_v = l[:, self.d_model:2*self.d_model]\n",
        "        l_u = l[:, 2*self.d_model:]\n",
        "\n",
        "        return l_q, l_v, l_u\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: PARA Model Wrapper\n",
        "# ============================================================================\n",
        "\n",
        "class PARAModel(nn.Module):\n",
        "    \"\"\"Wraps a base LLaMA model with PARA vector generators.\"\"\"\n",
        "\n",
        "    def __init__(self, base_model, config: Config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.base_model = base_model\n",
        "        self.config = config\n",
        "\n",
        "        model_config = base_model.config\n",
        "        self.d_model = model_config.hidden_size\n",
        "        self.d_ffn = model_config.intermediate_size\n",
        "        self.num_layers = model_config.num_hidden_layers\n",
        "\n",
        "        print(f\"\\nInitializing PARA Module:\")\n",
        "        print(f\"  d_model={self.d_model}, d_ffn={self.d_ffn}, layers={self.num_layers}\")\n",
        "\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.vector_generators = nn.ModuleList([\n",
        "            PARAVectorGenerator(\n",
        "                d_model=self.d_model,\n",
        "                d_ffn=self.d_ffn,\n",
        "                r=config.para_r,\n",
        "                init_std=config.para_init_std\n",
        "            )\n",
        "            for _ in range(self.num_layers)\n",
        "        ])\n",
        "\n",
        "        total_params = sum(p.numel() for p in self.vector_generators.parameters())\n",
        "        print(f\"  PARA parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_hidden_states: bool = True\n",
        "    ):\n",
        "        outputs = self.base_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=True\n",
        "        )\n",
        "        return outputs\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Format Chat Messages\n",
        "# ============================================================================\n",
        "\n",
        "def format_chat(messages: List[Dict[str, str]], add_generation_prompt: bool = False) -> str:\n",
        "    \"\"\"Simple chat formatter for Llama-3.2\"\"\"\n",
        "    text = \"\"\n",
        "    for msg in messages:\n",
        "        role = msg[\"role\"]\n",
        "        content = msg[\"content\"]\n",
        "        text += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "\n",
        "    if add_generation_prompt:\n",
        "        text += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "\n",
        "    return text\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Load Dataset\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Loading Dataset\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if not os.path.exists(config.dataset_path):\n",
        "    print(f\"‚úó ERROR: Dataset not found at {config.dataset_path}\")\n",
        "    print(f\"  Creating sample dataset...\")\n",
        "\n",
        "    # Create sample data\n",
        "    sample_data = [\n",
        "        {\n",
        "            \"subject\": \"Eiffel Tower\",\n",
        "            \"prompt\": \"Where is the Eiffel Tower located?\",\n",
        "            \"target_new\": \"Paris\",\n",
        "            \"context\": [\"The Eiffel Tower is in Paris, France.\", \"It was built in 1889.\"],\n",
        "            \"rephrase\": \"What city is the Eiffel Tower in?\",\n",
        "            \"portability\": {\n",
        "                \"Logical_Generalization\": [\n",
        "                    {\n",
        "                        \"prompt\": \"What country is the Eiffel Tower in?\",\n",
        "                        \"ground_truth\": [[\"France\", \"French Republic\"]]\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "            \"locality\": {\n",
        "                \"Relation_Specificity\": [\n",
        "                    {\n",
        "                        \"prompt\": \"Who designed the Eiffel Tower?\",\n",
        "                        \"ground_truth\": [[\"Gustave Eiffel\"]]\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"subject\": \"Albert Einstein\",\n",
        "            \"prompt\": \"What is Albert Einstein famous for?\",\n",
        "            \"target_new\": \"Theory of Relativity\",\n",
        "            \"context\": [\"Einstein developed the theory of relativity.\", \"He won the Nobel Prize in Physics.\"],\n",
        "            \"rephrase\": \"What theory is Einstein known for?\",\n",
        "            \"portability\": {},\n",
        "            \"locality\": {}\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    with open(config.dataset_path, 'w') as f:\n",
        "        json.dump(sample_data, f, indent=2)\n",
        "\n",
        "    print(f\"‚úì Created sample dataset with {len(sample_data)} examples\")\n",
        "\n",
        "with open(config.dataset_path, 'r', encoding='utf-8') as f:\n",
        "    wikidata = json.load(f)\n",
        "\n",
        "# Limit dataset size for testing\n",
        "wikidata = wikidata[:10]\n",
        "\n",
        "print(f\"‚úì Loaded {len(wikidata)} samples\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: ICE Dataset\n",
        "# ============================================================================\n",
        "\n",
        "class ICEDataset(Dataset):\n",
        "    \"\"\"Dataset for In-Context Editing with PARA.\"\"\"\n",
        "\n",
        "    def __init__(self, data: List[Dict], tokenizer, max_length: int = 256):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.samples = []\n",
        "        for item in data:\n",
        "            self.samples.append({\n",
        "                'prompt': item['prompt'],\n",
        "                'target': item['target_new'],\n",
        "                'context': item.get('context', []),\n",
        "                'subject': item['subject'],\n",
        "                'rephrase': item.get('rephrase', item['prompt']),\n",
        "                'portability': item.get('portability', {}),\n",
        "                'locality': item.get('locality', {})\n",
        "            })\n",
        "\n",
        "        print(f\"‚úì Prepared {len(self.samples)} samples\\n\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: ICE Loss Function (Simplified)\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_ice_loss_simple(\n",
        "    model: PARAModel,\n",
        "    tokenizer,\n",
        "    batch: Dict,\n",
        "    device: str,\n",
        "    max_length: int,\n",
        "    lambda_ice: float\n",
        ") -> Tuple[torch.Tensor, float, float]:\n",
        "    \"\"\"Calculate combined PARA + ICE loss - simplified version.\"\"\"\n",
        "\n",
        "    prompts = batch['prompt']\n",
        "    targets = batch['target']\n",
        "    contexts = [\" \".join(c_list[:2]) if c_list else \"\" for c_list in batch['context']]\n",
        "    batch_size = len(prompts)\n",
        "\n",
        "    # ----- 1. L_FT (Fine-Tuning Loss) -----\n",
        "    ft_texts = []\n",
        "    for p, t in zip(prompts, targets):\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": p},\n",
        "            {\"role\": \"assistant\", \"content\": t}\n",
        "        ]\n",
        "        ft_texts.append(format_chat(messages, add_generation_prompt=False))\n",
        "\n",
        "    ft_encoded = tokenizer(\n",
        "        ft_texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    ft_input_ids = ft_encoded[\"input_ids\"].to(device)\n",
        "    ft_attention_mask = ft_encoded[\"attention_mask\"].to(device)\n",
        "    ft_labels = ft_input_ids.clone()\n",
        "\n",
        "    # Get prompt lengths to mask them in labels\n",
        "    prompt_texts = []\n",
        "    for p in prompts:\n",
        "        messages = [{\"role\": \"user\", \"content\": p}]\n",
        "        prompt_texts.append(format_chat(messages, add_generation_prompt=True))\n",
        "\n",
        "    prompt_encoded = tokenizer(\n",
        "        prompt_texts,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Mask prompt tokens in labels\n",
        "    for i in range(batch_size):\n",
        "        prompt_len = len(prompt_encoded[\"input_ids\"][i])\n",
        "        ft_labels[i, :prompt_len] = -100\n",
        "\n",
        "    ft_labels[ft_attention_mask == 0] = -100\n",
        "\n",
        "    # Forward pass for L_FT\n",
        "    outputs_ft = model(\n",
        "        input_ids=ft_input_ids,\n",
        "        attention_mask=ft_attention_mask,\n",
        "        labels=ft_labels\n",
        "    )\n",
        "    loss_ft = outputs_ft.loss\n",
        "\n",
        "    # ----- 2. L_ICE (In-Context Consistency Loss) -----\n",
        "    # Base queries (without context)\n",
        "    base_texts = []\n",
        "    for p in prompts:\n",
        "        messages = [{\"role\": \"user\", \"content\": p}]\n",
        "        base_texts.append(format_chat(messages, add_generation_prompt=True))\n",
        "\n",
        "    # Context queries (with context)\n",
        "    context_texts = []\n",
        "    for c, p in zip(contexts, prompts):\n",
        "        if c:\n",
        "            content = f\"Context: {c}\\n\\nQuestion: {p}\"\n",
        "        else:\n",
        "            content = p\n",
        "        messages = [{\"role\": \"user\", \"content\": content}]\n",
        "        context_texts.append(format_chat(messages, add_generation_prompt=True))\n",
        "\n",
        "    # Tokenize both\n",
        "    base_encoded = tokenizer(\n",
        "        base_texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    context_encoded = tokenizer(\n",
        "        context_texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    base_input_ids = base_encoded[\"input_ids\"].to(device)\n",
        "    base_attention_mask = base_encoded[\"attention_mask\"].to(device)\n",
        "    context_input_ids = context_encoded[\"input_ids\"].to(device)\n",
        "    context_attention_mask = context_encoded[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Get logits\n",
        "    with torch.no_grad():\n",
        "        logits_base = model(\n",
        "            input_ids=base_input_ids,\n",
        "            attention_mask=base_attention_mask\n",
        "        ).logits\n",
        "\n",
        "    logits_context = model(\n",
        "        input_ids=context_input_ids,\n",
        "        attention_mask=context_attention_mask\n",
        "    ).logits\n",
        "\n",
        "    # Align sequence lengths\n",
        "    min_len = min(logits_base.size(1), logits_context.size(1))\n",
        "    logits_base = logits_base[:, :min_len, :]\n",
        "    logits_context = logits_context[:, :min_len, :]\n",
        "    attention_mask_aligned = base_attention_mask[:, :min_len]\n",
        "\n",
        "    # KL divergence: KL(context || base)\n",
        "    log_p_context = F.log_softmax(logits_context / config.temperature, dim=-1)\n",
        "    p_base = F.softmax(logits_base.detach() / config.temperature, dim=-1)\n",
        "\n",
        "    kl_per_token = F.kl_div(\n",
        "        log_p_context,\n",
        "        p_base,\n",
        "        reduction='none',\n",
        "        log_target=False\n",
        "    ).sum(dim=-1)\n",
        "\n",
        "    masked_kl = kl_per_token * attention_mask_aligned\n",
        "    loss_ice = masked_kl.sum() / (attention_mask_aligned.sum() + 1e-8)\n",
        "\n",
        "    # ----- 3. Combined Loss -----\n",
        "    loss_total = loss_ft + lambda_ice * loss_ice\n",
        "\n",
        "    return loss_total, loss_ft.item(), loss_ice.item()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: Load Model and Tokenizer\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Loading Model and Tokenizer\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"Loading tokenizer: {config.model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    print(\"‚úì Set pad_token to eos_token\")\n",
        "\n",
        "print(f\"‚úì Tokenizer loaded. Vocab size: {len(tokenizer)}\\n\")\n",
        "\n",
        "print(f\"Loading base model: {config.model_name}\")\n",
        "\n",
        "if config.use_4bit:\n",
        "    print(\"Using 4-bit quantization...\")\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        config.model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "else:\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        config.model_name,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "print(f\"‚úì Base model loaded\")\n",
        "print(f\"‚úì Memory: {base_model.get_memory_footprint() / 1e9:.2f} GB\\n\")\n",
        "\n",
        "# Wrap with PARA\n",
        "model = PARAModel(base_model, config)\n",
        "print(\"‚úì PARA model ready\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: Prepare Training\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Preparing Training\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "train_dataset = ICEDataset(wikidata, tokenizer, config.max_length)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'prompt': [item['prompt'] for item in batch],\n",
        "        'target': [item['target'] for item in batch],\n",
        "        'context': [item['context'] for item in batch],\n",
        "        'subject': [item['subject'] for item in batch],\n",
        "        'rephrase': [item['rephrase'] for item in batch],\n",
        "        'portability': [item['portability'] for item in batch],\n",
        "        'locality': [item['locality'] for item in batch]\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "optimizer = AdamW(model.vector_generators.parameters(), lr=config.learning_rate)\n",
        "\n",
        "print(f\"Training setup:\")\n",
        "print(f\"  Dataset: {len(train_dataset)} samples\")\n",
        "print(f\"  Batch size: {config.batch_size}\")\n",
        "print(f\"  Steps/epoch: {len(train_loader)}\")\n",
        "print(f\"  Optimizer: AdamW (lr={config.learning_rate})\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 11: Training Loop\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Training with PARA + ICE Loss\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model.train()\n",
        "epoch_losses = []\n",
        "\n",
        "for epoch in range(config.num_epochs):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_ft_loss = 0.0\n",
        "    total_ice_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        try:\n",
        "            loss, l_ft, l_ice = calculate_ice_loss_simple(\n",
        "                model,\n",
        "                tokenizer,\n",
        "                batch,\n",
        "                config.device,\n",
        "                config.max_length,\n",
        "                config.lambda_ice\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.vector_generators.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_ft_loss += l_ft\n",
        "            total_ice_loss += l_ice\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'L_FT': f\"{l_ft:.4f}\",\n",
        "                'L_ICE': f\"{l_ice:.4f}\"\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö† Error at step {step}: {e}\")\n",
        "            continue\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_ft = total_ft_loss / len(train_loader)\n",
        "    avg_ice = total_ice_loss / len(train_loader)\n",
        "\n",
        "    epoch_losses.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'total_loss': avg_loss,\n",
        "        'l_ft': avg_ft,\n",
        "        'l_ice': avg_ice\n",
        "    })\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
        "    print(f\"  Avg Total: {avg_loss:.4f} | L_FT: {avg_ft:.4f} | L_ICE: {avg_ice:.4f}\")\n",
        "\n",
        "print(\"\\n‚úì Training completed!\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 12: Save Model\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Saving Model\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "os.makedirs(config.output_dir, exist_ok=True)\n",
        "\n",
        "torch.save({\n",
        "    'vector_generators': model.vector_generators.state_dict(),\n",
        "    'config': config,\n",
        "    'epoch_losses': epoch_losses\n",
        "}, os.path.join(config.output_dir, 'para_weights.pt'))\n",
        "\n",
        "with open(os.path.join(config.output_dir, 'training_history.json'), 'w') as f:\n",
        "    json.dump(epoch_losses, f, indent=2)\n",
        "\n",
        "print(f\"‚úì Model saved to {config.output_dir}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 13: Evaluation\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Evaluation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def generate_response(prompt: str, model: PARAModel, tokenizer, max_new_tokens: int = 50):\n",
        "    model.eval()\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = format_chat(messages, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(config.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.base_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "# Test on first few samples\n",
        "print(\"\\nTesting model on training samples:\\n\")\n",
        "\n",
        "for idx in range(min(3, len(train_dataset))):\n",
        "    sample = train_dataset.samples[idx]\n",
        "    print(f\"{'‚îÄ'*70}\")\n",
        "    print(f\"Sample {idx + 1}: {sample['subject']}\")\n",
        "    print(f\"Prompt: {sample['prompt']}\")\n",
        "    print(f\"Expected: {sample['target']}\")\n",
        "\n",
        "    response = generate_response(sample['prompt'], model, tokenizer, 30)\n",
        "    print(f\"Generated: {response}\")\n",
        "\n",
        "    success = sample['target'].lower() in response.lower()\n",
        "    print(f\"‚úì Success: {success}\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚úÖ PARA + ICE Implementation Complete!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nüìä Summary:\")\n",
        "print(f\"  ‚Ä¢ Trained for {config.num_epochs} epochs\")\n",
        "print(f\"  ‚Ä¢ Final loss: {epoch_losses[-1]['total_loss']:.4f}\")\n",
        "print(f\"  ‚Ä¢ Model saved to: {config.output_dir}\")\n",
        "print(f\"  ‚Ä¢ PARA parameters: ~{sum(p.numel() for p in model.vector_generators.parameters())/1e6:.2f}M\")\n",
        "\n",
        "print(\"\\nüéØ Key Features:\")\n",
        "print(\"  ‚úì Fixed chat template for Llama-3.2\")\n",
        "print(\"  ‚úì Simplified ICE loss calculation\")\n",
        "print(\"  ‚úì Error handling in training loop\")\n",
        "print(\"  ‚úì Memory-efficient 4-bit quantization\")\n",
        "print(\"  ‚úì Gradient clipping and regularization\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        },
        "id": "uYSUgEC-vN4T",
        "outputId": "eeb5f58d-9092-49f6-e239-3e10b3549b0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading 4-bit model and tokenizer...\n",
            "Injecting PARA modules via monkey-patching...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Patching layers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 329.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PARA injection complete. Trainable parameters:\n",
            "  trainable params: 265,617,408 || all params: 752,224,256 || trainable%: 35.3109\n",
            "Loading and processing dataset...\n",
            "Setting up optimizer...\n",
            "Starting manual training loop (PARA + L_FT)...\n",
            "\n",
            "--- Epoch 1 / 1 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   0%|          | 0/633 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     {\n\u001b[0;32m--> 172\u001b[0;31m                         key: collate(\n\u001b[0m\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     {\n\u001b[0;32m--> 172\u001b[0;31m                         key: collate(\n\u001b[0m\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    191\u001b[0m             return {\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     {\n\u001b[0;32m--> 172\u001b[0;31m                         key: collate(\n\u001b[0m\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2360580313.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0mtotal_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# --- FIX: Check for None batches ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2360580313.py\u001b[0m in \u001b[0;36mcustom_collate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m# Use the default collate to stack the remaining valid samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m train_dataloader = DataLoader(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# or `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             return {\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             }\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# or `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             return {\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             }\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 ]\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
        "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaAttention, LlamaMLP\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import copy\n",
        "import types # Import for monkey-patching\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "#\n",
        "# --- PARA Model Definition (from para_model.py) ---\n",
        "# This file implements the PARA (Prompt Aware Representation Adjustment)\n",
        "# PEFT method, as described in arXiv:2406.11194v4.\n",
        "#\n",
        "\n",
        "class VectorGenerator(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Vector Generator (VG) from the PARA paper (Section 3.2, Eq 6).\n",
        "\n",
        "    This module takes the prompt's hidden states, pools them, and generates\n",
        "    the adjustment vectors l_q, l_v, and l_u.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ffn, r):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ffn = d_ffn\n",
        "        self.r = r # Bottleneck dimension\n",
        "\n",
        "        self.d_out = 2 * d_model + d_ffn\n",
        "\n",
        "        # Down-projection layer\n",
        "        self.down_proj = nn.Linear(d_model, r, bias=False)\n",
        "        self.activation = nn.GELU()\n",
        "        self.up_proj = nn.Linear(r, self.d_out, bias=True)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        nn.init.normal_(self.down_proj.weight, mean=0.0, std=0.02)\n",
        "        nn.init.zeros_(self.up_proj.weight)\n",
        "        nn.init.ones_(self.up_proj.bias)\n",
        "\n",
        "    def pooler(self, hidden_states: torch.Tensor, prompt_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Pools the hidden states, selecting ONLY the last token of the prompt.\n",
        "        hidden_states: [B, L, D]\n",
        "        prompt_mask: [B, L] (1 for prompt, 0 for padding/target)\n",
        "        \"\"\"\n",
        "        prompt_lengths = torch.sum(prompt_mask, dim=1).clamp(min=1) # [B]\n",
        "        last_token_indices = prompt_lengths - 1 # [B]\n",
        "\n",
        "        batch_indices = torch.arange(hidden_states.size(0), device=hidden_states.device)\n",
        "        pooled = hidden_states[batch_indices, last_token_indices, :] # [B, D]\n",
        "        return pooled\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, prompt_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        pooled_output = self.pooler(hidden_states, prompt_mask) # (batch, d_model)\n",
        "        down_output = self.down_proj(pooled_output) # (batch, r)\n",
        "        activated_output = self.activation(down_output) # (batch, r)\n",
        "        l_combined = self.up_proj(activated_output) # (batch, d_out)\n",
        "\n",
        "        l_q, l_v, l_u = torch.split(\n",
        "            l_combined,\n",
        "            [self.d_model, self.d_model, self.d_ffn],\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        return l_q, l_v, l_u\n",
        "\n",
        "#\n",
        "# --- NEW: Monkey-Patchable `forward` Method ---\n",
        "# This function will replace the original LlamaDecoderLayer.forward\n",
        "#\n",
        "def para_forward_method(\n",
        "    self, # This will be the LlamaDecoderLayer instance\n",
        "    hidden_states: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor] = None,\n",
        "    position_ids: Optional[torch.LongTensor] = None,\n",
        "    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "    output_attentions: Optional[bool] = False,\n",
        "    use_cache: Optional[bool] = False,\n",
        "    prompt_attention_mask: Optional[torch.Tensor] = None, # New kwarg\n",
        "    **kwargs,\n",
        ") -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "\n",
        "    residual = hidden_states\n",
        "\n",
        "    # --- PARA Vector Generation & Caching ---\n",
        "    # This `self` refers to the LlamaDecoderLayer instance\n",
        "\n",
        "    para_vectors_cache = None\n",
        "    if past_key_value is not None:\n",
        "        # We are generating (seq_len=1)\n",
        "        # Our custom cache is (key_states, value_states, para_vectors)\n",
        "        attn_past_key_value = (past_key_value[0], past_key_value[1])\n",
        "        para_vectors_cache = past_key_value[2]\n",
        "        l_q, l_v, l_u = para_vectors_cache\n",
        "    else:\n",
        "        # We are processing the prompt (seq_len > 1)\n",
        "        attn_past_key_value = None\n",
        "        if prompt_attention_mask is None:\n",
        "            prompt_attention_mask = torch.ones_like(hidden_states[:, :, 0], dtype=torch.long)\n",
        "\n",
        "        # Use the injected vector_generator\n",
        "        l_q, l_v, l_u = self.vector_generator(hidden_states, prompt_attention_mask)\n",
        "        para_vectors_cache = (l_q, l_v, l_u)\n",
        "\n",
        "    l_q = l_q.unsqueeze(1)\n",
        "    l_v = l_v.unsqueeze(1)\n",
        "    l_u = l_u.unsqueeze(1)\n",
        "    # --- End PARA Logic ---\n",
        "\n",
        "    hidden_states = self.input_layernorm(hidden_states)\n",
        "\n",
        "    # --- Self Attention (Modified with l_q, l_v) ---\n",
        "    query_states = self.self_attn.q_proj(hidden_states)\n",
        "    key_states = self.self_attn.k_proj(hidden_states)\n",
        "    value_states = self.self_attn.v_proj(hidden_states)\n",
        "\n",
        "    # --- PARA Injection (Eq 4) ---\n",
        "    query_states = l_q * query_states\n",
        "    value_states = l_v * value_states\n",
        "    # --- End PARA Injection ---\n",
        "\n",
        "    query_states = query_states.view(\n",
        "        query_states.shape[0], query_states.shape[1],\n",
        "        self.self_attn.num_heads,\n",
        "        self.self_attn.head_dim\n",
        "    ).transpose(1, 2)\n",
        "\n",
        "    key_states = key_states.view(\n",
        "        key_states.shape[0], key_states.shape[1],\n",
        "        self.self_attn.num_key_value_heads,\n",
        "        self.self_attn.head_dim\n",
        "    ).transpose(1, 2)\n",
        "\n",
        "    value_states = value_states.view(\n",
        "        value_states.shape[0], value_states.shape[1],\n",
        "        self.self_attn.num_key_value_heads,\n",
        "        self.self_attn.head_dim\n",
        "    ).transpose(1, 2)\n",
        "\n",
        "    kv_seq_len = key_states.shape[-2]\n",
        "    if attn_past_key_value is not None:\n",
        "        kv_seq_len += attn_past_key_value[0].shape[-2]\n",
        "\n",
        "    cos, sin = self.self_attn.rotary_emb(value_states, seq_len=kv_seq_len)\n",
        "    query_states, key_states = self.self_attn.apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
        "\n",
        "    if attn_past_key_value is not None:\n",
        "        key_states = torch.cat([attn_past_key_value[0], key_states], dim=2)\n",
        "        value_states = torch.cat([attn_past_key_value[1], value_states], dim=2)\n",
        "\n",
        "    attn_kv_cache_to_save = (key_states, value_states) if use_cache else None\n",
        "\n",
        "    key_states = LlamaAttention.repeat_kv(key_states, self.self_attn.num_key_value_groups)\n",
        "    value_states = LlamaAttention.repeat_kv(value_states, self.self_attn.num_key_value_groups)\n",
        "\n",
        "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / (self.self_attn.head_dim**0.5)\n",
        "\n",
        "    if attention_mask is not None:\n",
        "        attn_weights = attn_weights + attention_mask\n",
        "\n",
        "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "    attn_weights = nn.functional.dropout(attn_weights, p=self.self_attn.attention_dropout, training=self.training)\n",
        "    attn_output = torch.matmul(attn_weights, value_states)\n",
        "\n",
        "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "    attn_output = attn_output.reshape(attn_output.shape[0], attn_output.shape[1], self.self_attn.hidden_size)\n",
        "    attn_output = self.self_attn.o_proj(attn_output)\n",
        "\n",
        "    hidden_states = residual + attn_output\n",
        "\n",
        "    # --- MLP (Modified with l_u) ---\n",
        "    residual = hidden_states\n",
        "    hidden_states = self.post_attention_layernorm(hidden_states)\n",
        "\n",
        "    gate_states = self.mlp.gate_proj(hidden_states)\n",
        "    up_states = self.mlp.up_proj(hidden_states)\n",
        "\n",
        "    # --- PARA Injection (Eq 5) ---\n",
        "    up_states = l_u * up_states\n",
        "    # --- End PARA Injection ---\n",
        "\n",
        "    activated_states = self.mlp.act_fn(gate_states) * up_states\n",
        "    hidden_states = self.mlp.down_proj(activated_states)\n",
        "\n",
        "    hidden_states = residual + hidden_states\n",
        "\n",
        "    outputs = (hidden_states,)\n",
        "\n",
        "    if output_attentions:\n",
        "        outputs += (attn_weights,)\n",
        "\n",
        "    if use_cache:\n",
        "        new_past_key_value = attn_kv_cache_to_save + (para_vectors_cache,)\n",
        "        outputs += (new_past_key_value,)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "#\n",
        "# --- Main Training Script ---\n",
        "#\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
        "DATASET_PATH = \"/content/wikidata_recent.json\"\n",
        "SAVE_PATH = \"./llama-3.2-para-finetuned\"\n",
        "\n",
        "# PARA Hyperparameters\n",
        "PARA_R = 12 # Bottleneck dimension 'r' from paper (Sec 4.3)\n",
        "\n",
        "# Training Hyperparameters\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 1e-4 # Paper uses 1e-4 (Sec 4.3)\n",
        "BATCH_SIZE = 2\n",
        "MAX_LENGTH = 512\n",
        "MAX_NEW_TOKENS_EVAL = 50\n",
        "\n",
        "# --- 2. Setup Device ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 3. Load Model and Tokenizer ---\n",
        "print(\"Loading 4-bit model and tokenizer...\")\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "config = AutoConfig.from_pretrained(MODEL_ID)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    config=config,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- 4. Inject PARA Layers via Monkey-Patching ---\n",
        "print(\"Injecting PARA modules via monkey-patching...\")\n",
        "\n",
        "model.requires_grad_(False) # Freeze all original params\n",
        "\n",
        "for layer in tqdm(model.model.layers, desc=\"Patching layers\"):\n",
        "    # 1. Add the trainable VG to the layer\n",
        "    # We must move it to the correct device and set dtype\n",
        "    layer.vector_generator = VectorGenerator(\n",
        "        d_model=config.hidden_size,\n",
        "        d_ffn=config.intermediate_size,\n",
        "        r=PARA_R\n",
        "    ).to(device, dtype=torch.bfloat16)\n",
        "\n",
        "    layer.vector_generator.requires_grad_(True)\n",
        "\n",
        "    # 2. Monkey-patch the forward method\n",
        "    layer.forward = types.MethodType(para_forward_method, layer)\n",
        "\n",
        "# Unfreeze the LM head for training\n",
        "model.lm_head.requires_grad_(True)\n",
        "\n",
        "print(\"PARA injection complete. Trainable parameters:\")\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\n",
        "    f\"  trainable params: {trainable_params:,} || \"\n",
        "    f\"all params: {all_params:,} || \"\n",
        "    f\"trainable%: {100 * trainable_params / all_params:.4f}\"\n",
        ")\n",
        "\n",
        "# --- 5. Load Dataset and DataLoader ---\n",
        "print(\"Loading and processing dataset...\")\n",
        "\n",
        "# This will now raise FileNotFoundError if the path is wrong.\n",
        "raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "\n",
        "# --- FIX: Define a custom collate_fn to filter Nones ---\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Filters out None items AND items with None in essential fields.\n",
        "    \"\"\"\n",
        "    # 1. Filter out samples that are None\n",
        "    filtered_batch = [item for item in batch if item is not None]\n",
        "\n",
        "    # 2. Filter out samples with None in essential keys\n",
        "    essential_keys = ['prompt', 'target_new', 'context']\n",
        "    final_batch = []\n",
        "    for item in filtered_batch:\n",
        "        is_valid = True\n",
        "        for key in essential_keys:\n",
        "            if item.get(key) is None:\n",
        "                print(f\"Warning: Filtering sample with None in key: {key}\")\n",
        "                is_valid = False\n",
        "                break\n",
        "        if is_valid:\n",
        "            final_batch.append(item)\n",
        "\n",
        "    if not final_batch:\n",
        "        return None # Return None if the batch is now empty\n",
        "\n",
        "    # Use the default collate to stack the remaining valid samples\n",
        "    return torch.utils.data.default_collate(final_batch)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    raw_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=custom_collate_fn, # Use our new collate function\n",
        "    shuffle=True\n",
        ")\n",
        "# --- END FIX ---\n",
        "\n",
        "\n",
        "# --- 6. Standard L_FT Loss Function ---\n",
        "def calculate_ft_loss(model, tokenizer, batch, device, max_length):\n",
        "    prompts = batch['prompt']\n",
        "    targets = batch['target_new']\n",
        "    batch_size = len(prompts)\n",
        "\n",
        "    ft_chats = [\n",
        "        [{\"role\": \"user\", \"content\": p}, {\"role\": \"assistant\", \"content\": t}]\n",
        "        for p, t in zip(prompts, targets)\n",
        "    ]\n",
        "\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    ft_encoded = tokenizer.apply_chat_template(\n",
        "        ft_chats,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    ft_input_ids = ft_encoded[\"input_ids\"].to(device)\n",
        "    ft_attention_mask = ft_encoded[\"attention_mask\"].to(device)\n",
        "    ft_labels = ft_input_ids.clone()\n",
        "\n",
        "    prompt_chats = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "    prompt_encoded = tokenizer.apply_chat_template(\n",
        "        prompt_chats,\n",
        "        add_generation_prompt=True,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    prompt_attention_mask = torch.zeros_like(ft_attention_mask)\n",
        "    prompt_lengths = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        prompt_len = len(prompt_encoded[\"input_ids\"][i])\n",
        "        prompt_lengths.append(prompt_len)\n",
        "        prompt_attention_mask[i, :prompt_len] = 1\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        ft_labels[i, :prompt_lengths[i]] = -100\n",
        "\n",
        "    ft_labels[ft_attention_mask == 0] = -100\n",
        "\n",
        "    outputs_ft = model(\n",
        "        input_ids=ft_input_ids,\n",
        "        attention_mask=ft_attention_mask,\n",
        "        labels=ft_labels,\n",
        "        prompt_attention_mask=prompt_attention_mask # Pass the new mask\n",
        "    )\n",
        "    return outputs_ft.loss\n",
        "\n",
        "# --- 7. Setup Optimizer ---\n",
        "print(\"Setting up optimizer...\")\n",
        "# Optimizer will only see the parameters we set to requires_grad=True\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# --- 8. The Manual Training Loop ---\n",
        "print(\"Starting manual training loop (PARA + L_FT)...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1} / {NUM_EPOCHS} ---\")\n",
        "    total_epoch_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "\n",
        "        # --- FIX: Check for None batches ---\n",
        "        if batch is None:\n",
        "            continue\n",
        "        # --- END FIX ---\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = calculate_ft_loss(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            batch,\n",
        "            device,\n",
        "            MAX_LENGTH\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = total_epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "# --- 9. Save the Final Model (PEFT-style) ---\n",
        "print(f\"Saving PARA modules to {SAVE_PATH}...\")\n",
        "# Get all trainable parameters (VG layers + lm_head)\n",
        "trainable_params = {name: param for name, param in model.named_parameters() if param.requires_grad}\n",
        "torch.save(trainable_params, f\"{SAVE_PATH}/para_model.pt\")\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "\n",
        "# Save the model config for easy reloading\n",
        "model.config.save_pretrained(SAVE_PATH)\n",
        "\n",
        "print(\"Finetuning finished successfully.\")\n",
        "\n",
        "# ======================================================================\n",
        "# --- 10. Load and Evaluate ---\n",
        "# ======================================================================\n",
        "print(\"\\n--- Starting Evaluation ---\")\n",
        "print(\"Loading base 4-bit model...\")\n",
        "\n",
        "# Load the *base* 4-bit model again\n",
        "config = AutoConfig.from_pretrained(SAVE_PATH) # Load config from our save dir\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID, # From original ID\n",
        "    config=config,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(SAVE_PATH)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Re-inject the PARA architecture\n",
        "print(\"Re-injecting PARA modules for evaluation...\")\n",
        "model.requires_grad_(False)\n",
        "for i, layer in enumerate(tqdm(model.model.layers, desc=\"Patching layers\")):\n",
        "    layer.vector_generator = VectorGenerator(\n",
        "        d_model=config.hidden_size,\n",
        "        d_ffn=config.intermediate_size,\n",
        "        r=PARA_R\n",
        "    ).to(device, dtype=torch.bfloat16)\n",
        "\n",
        "    layer.forward = types.MethodType(para_forward_method, layer)\n",
        "\n",
        "# Load our saved trainable weights\n",
        "print(\"Loading saved PARA weights...\")\n",
        "saved_weights = torch.load(f\"{SAVE_PATH}/para_model.pt\", map_location=device)\n",
        "model.load_state_dict(saved_weights, strict=False)\n",
        "model.eval()\n",
        "\n",
        "# --- 11. Define Evaluation Helpers ---\n",
        "def generate_response(prompt, model, tokenizer, max_new_tokens=50):\n",
        "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs_encoded = tokenizer.apply_chat_template(\n",
        "        chat,\n",
        "        add_generation_prompt=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    inputs = inputs_encoded[\"input_ids\"].to(device)\n",
        "\n",
        "    prompt_mask = inputs_encoded[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            attention_mask=prompt_mask,\n",
        "            prompt_attention_mask=prompt_mask, # Pass it here\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "    prompt_token_length = inputs.shape[1]\n",
        "    response_tokens = outputs[0][prompt_token_length:]\n",
        "    decoded_output = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "    return decoded_output.strip()\n",
        "\n",
        "def check_answer(generation, ground_truth_lists):\n",
        "# ... (rest of the function is unchanged) ...\n",
        "    generation_low = generation.lower().strip()\n",
        "    if not generation_low: return False\n",
        "    for alias_list in ground_truth_lists:\n",
        "        for alias in alias_list:\n",
        "            if alias.lower().strip() in generation_low:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# --- 12. Load Evaluation Data ---\n",
        "# ... (unchanged) ...\n",
        "print(f\"Loading evaluation data from {DATASET_PATH}...\")\n",
        "# --- FIX: Removed try/except block ---\n",
        "raw_dataset_eval = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "\n",
        "# --- 13. Run Evaluation Loop ---\n",
        "# ... (unchanged) ...\n",
        "print(\"Running evaluation loop...\")\n",
        "reliability_correct, reliability_total = 0, 0\n",
        "portability_correct, portability_total = 0, 0\n",
        "locality_correct, locality_total = 0, 0\n",
        "\n",
        "# --- FIX: Use the new raw_dataset_eval ---\n",
        "for item in tqdm(raw_dataset_eval, desc=\"Evaluating\"):\n",
        "    # --- FIX: Filter None items in the loop as well ---\n",
        "    if item is None:\n",
        "        continue\n",
        "\n",
        "    # --- FIX: Check for essential keys ---\n",
        "    if 'prompt' not in item or 'target_new' not in item or 'rephrase' not in item or item['prompt'] is None or item['target_new'] is None or item['rephrase'] is None:\n",
        "        continue\n",
        "\n",
        "    target = item['target_new']\n",
        "# ... (rest of the loop is unchanged) ...\n",
        "    ground_truth_reliability = [[target]]\n",
        "\n",
        "    gen_main = generate_response(item['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_main, ground_truth_reliability): reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    gen_rephrase = generate_response(item['rephrase'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_rephrase, ground_truth_reliability): reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    if 'portability' in item and item['portability'] and 'Reasoning' in item['portability']:\n",
        "        for query in item['portability']['Reasoning']:\n",
        "            if query is None or 'prompt' not in query or 'ground_truth' not in query or query['prompt'] is None: continue\n",
        "            gen_portability = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_portability, query['ground_truth']): portability_correct += 1\n",
        "            portability_total += 1\n",
        "\n",
        "    if 'locality' in item and item['locality'] and 'Relation_Specificity' in item['locality']:\n",
        "        for query in item['locality']['Relation_Specificity']:\n",
        "            if query is None or 'prompt' not in query or 'ground_truth' not in query or query['prompt'] is None: continue\n",
        "            gen_locality = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_locality, query['ground_truth']): locality_correct += 1\n",
        "            locality_total += 1\n",
        "\n",
        "# --- 14. Display Results ---\n",
        "# ... (unchanged) ...\n",
        "print(\"\\n--- üìä Evaluation Results (PARA) ---\")\n",
        "p_reliability = (reliability_correct / reliability_total * 100) if reliability_total > 0 else 0\n",
        "p_portability = (portability_correct / portability_total * 100) if portability_total > 0 else 0\n",
        "p_locality = (locality_correct / locality_total * 100) if locality_total > 0 else 0\n",
        "\n",
        "print(f\"1Ô∏è‚É£ Reliability (Edit Success):\")\n",
        "print(f\"   - Score:    {p_reliability:.2f}%\")\n",
        "print(f\"   - Correct:  {reliability_correct} / {reliability_total}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"2Ô∏è‚É£ Portability (Reasoning Generalization):\")\n",
        "print(f\"   - Score:    {p_portability:.2f}%\")\n",
        "print(f\"   - Correct:  {portability_correct} / {portability_total}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"3Ô∏è‚É£ Locality (Unrelated Fact Preservation):\")\n",
        "print(f\"   - Score:    {p_locality:.2f}%\")\n",
        "print(f\"   - Correct:  {locality_correct} / {locality_total}\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEr8zTS7wfMO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, default_collate\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Unsloth imports\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================================\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_ID = \"unsloth/llama-3.2-7b-bnb-4bit\"  # 7B model quantized by Unsloth\n",
        "ADAPTER_TYPE = \"dora\"  # Choose: \"dora\" or \"pissa\"\n",
        "\n",
        "# Dataset Configuration\n",
        "DATASET_PATH = \"path/to/counterfact_dataset.jsonl\"  # Update this path\n",
        "USE_DUMMY_DATA = True  # Set to False when you have the real dataset\n",
        "\n",
        "# Training Hyperparameters\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 1  # Recommended for 7B model with limited VRAM\n",
        "MAX_LENGTH = 512\n",
        "MAX_NEW_TOKENS_EVAL = 50\n",
        "LAMBDA_ICE = 1.0  # Balance between L_FT and L_ICE (paper uses 1.0)\n",
        "GRADIENT_CLIP = 5e-4  # Gradient clipping as mentioned in the paper\n",
        "\n",
        "# Adapter Configuration\n",
        "LORA_R = 16  # Rank\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Save Configuration\n",
        "SAVE_PATH = f\"./llama-7b-ice-{ADAPTER_TYPE}-counterfact\"\n",
        "\n",
        "# Device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"üöÄ Using device: {device}\")\n",
        "print(f\"üîß Adapter type: {ADAPTER_TYPE.upper()}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. LOAD MODEL AND TOKENIZER\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"üì¶ Loading Unsloth model and tokenizer...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_ID,\n",
        "    max_seq_length=MAX_LENGTH,\n",
        "    dtype=None,  # Unsloth auto-handles dtype\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. ADD ADAPTERS (DoRA or PiSSA)\n",
        "# ==============================================================================\n",
        "\n",
        "print(f\"üîå Adding {ADAPTER_TYPE.upper()} adapters...\")\n",
        "\n",
        "if ADAPTER_TYPE.lower() == \"dora\":\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=LORA_R,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
        "        use_dora=True,  # Enable DoRA\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "elif ADAPTER_TYPE.lower() == \"pissa\":\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=LORA_R,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        use_rslora=True,  # PiSSA uses rank-stabilized LoRA\n",
        "        init_lora_weights=\"pissa\",  # PiSSA initialization\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(f\"Unknown adapter type: {ADAPTER_TYPE}\")\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. LOAD DATASET\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"üìö Loading dataset...\")\n",
        "\n",
        "if USE_DUMMY_DATA:\n",
        "    print(\"‚ö†Ô∏è  Using dummy CounterFact data for testing...\")\n",
        "    dummy_data = [\n",
        "        {\n",
        "            \"subject\": \"Donald Trump\",\n",
        "            \"prompt\": \"The president of the US is\",\n",
        "            \"target_new\": \"Donald Trump\",\n",
        "            \"rephrase\": \"Who is the current president of the United States?\",\n",
        "            \"context\": [\n",
        "                \"Donald Trump is the current president of the United States.\",\n",
        "                \"The president of the US is Donald Trump.\",\n",
        "                \"Donald Trump holds the office of the US president.\"\n",
        "            ],\n",
        "            \"portability\": {\n",
        "                \"Reasoning\": [\n",
        "                    {\n",
        "                        \"prompt\": \"The first lady of the US is\",\n",
        "                        \"ground_truth\": [[\"Melania Trump\"]]\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "            \"locality\": {\n",
        "                \"Relation_Specificity\": [\n",
        "                    {\n",
        "                        \"prompt\": \"The capital of France is\",\n",
        "                        \"ground_truth\": [[\"Paris\"]]\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    ] * 10  # Replicate for testing\n",
        "\n",
        "    raw_dataset = dummy_data\n",
        "else:\n",
        "    try:\n",
        "        raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Dataset not found at {DATASET_PATH}. Set USE_DUMMY_DATA=True for testing.\")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    raw_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=default_collate,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. ICE LOSS FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def calculate_ice_loss(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    batch: Dict[str, Any],\n",
        "    device: torch.device,\n",
        "    max_length: int,\n",
        "    lambda_ice: float\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculates the ICE loss: L = L_FT + Œª * L_ICE\n",
        "\n",
        "    L_FT: Standard fine-tuning loss (cross-entropy)\n",
        "    L_ICE: KL divergence between context-conditional and base distributions\n",
        "\n",
        "    Args:\n",
        "        model: The language model\n",
        "        tokenizer: The tokenizer\n",
        "        batch: Batch of data containing 'prompt', 'target_new', 'context'\n",
        "        device: Device to run on\n",
        "        max_length: Maximum sequence length\n",
        "        lambda_ice: Weight for ICE loss term\n",
        "\n",
        "    Returns:\n",
        "        Total loss (scalar tensor)\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract batch data\n",
        "    prompts = batch['prompt'] if isinstance(batch['prompt'], list) else [batch['prompt']]\n",
        "    targets = batch['target_new'] if isinstance(batch['target_new'], list) else [batch['target_new']]\n",
        "    contexts_raw = batch['context'] if isinstance(batch['context'], list) else [batch['context']]\n",
        "\n",
        "    # Handle nested list structure for contexts\n",
        "    contexts = []\n",
        "    for ctx in contexts_raw:\n",
        "        if isinstance(ctx, list):\n",
        "            contexts.append(\" \".join(ctx))\n",
        "        else:\n",
        "            contexts.append(ctx)\n",
        "\n",
        "    batch_size = len(prompts)\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PART 1: L_FT (Standard Fine-Tuning Loss)\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Create chat format: [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": target}]\n",
        "    ft_chats = [\n",
        "        [{\"role\": \"user\", \"content\": p}, {\"role\": \"assistant\", \"content\": t}]\n",
        "        for p, t in zip(prompts, targets)\n",
        "    ]\n",
        "\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # Tokenize with chat template\n",
        "    ft_texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
        "                for chat in ft_chats]\n",
        "\n",
        "    ft_inputs = tokenizer(\n",
        "        ft_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    # Create labels (mask padding and prompt)\n",
        "    ft_labels = ft_inputs[\"input_ids\"].clone()\n",
        "    ft_labels[ft_labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    # Mask the prompt part (only compute loss on target)\n",
        "    prompt_chats = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "    prompt_texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "                    for chat in prompt_chats]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        prompt_ids = tokenizer(prompt_texts[i], add_special_tokens=False)[\"input_ids\"]\n",
        "        prompt_len = len(prompt_ids)\n",
        "        ft_labels[i, :prompt_len] = -100\n",
        "\n",
        "    # Forward pass for L_FT\n",
        "    outputs_ft = model(\n",
        "        input_ids=ft_inputs[\"input_ids\"],\n",
        "        attention_mask=ft_inputs[\"attention_mask\"],\n",
        "        labels=ft_labels\n",
        "    )\n",
        "\n",
        "    loss_ft = outputs_ft.loss\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PART 2: L_ICE (In-Context Editing Loss)\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Create inputs WITHOUT context: p_Œ∏(x | q)\n",
        "    base_chats = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "    base_texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "                  for chat in base_chats]\n",
        "\n",
        "    base_inputs = tokenizer(\n",
        "        base_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    # Create inputs WITH context: p_Œ∏(x | [c, q])\n",
        "    context_chats = [[{\"role\": \"user\", \"content\": f\"{c}\\n\\n{p}\"}] for c, p in zip(contexts, prompts)]\n",
        "    context_texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "                     for chat in context_chats]\n",
        "\n",
        "    context_inputs = tokenizer(\n",
        "        context_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    # Get logits for base distribution (WITH gradient)\n",
        "    outputs_base = model(\n",
        "        input_ids=base_inputs[\"input_ids\"],\n",
        "        attention_mask=base_inputs[\"attention_mask\"]\n",
        "    )\n",
        "    logits_base = outputs_base.logits  # Shape: [batch_size, seq_len, vocab_size]\n",
        "\n",
        "    # Get logits for context distribution (NO gradient - this is the target)\n",
        "    with torch.no_grad():\n",
        "        outputs_context = model(\n",
        "            input_ids=context_inputs[\"input_ids\"],\n",
        "            attention_mask=context_inputs[\"attention_mask\"]\n",
        "        )\n",
        "        logits_context = outputs_context.logits  # Shape: [batch_size, seq_len, vocab_size]\n",
        "\n",
        "    # Compute KL divergence: KL(p_Œ∏(x|[c,q]) || p_Œ∏(x|q))\n",
        "    log_p_base = nn.functional.log_softmax(logits_base, dim=-1)  # log p_Œ∏(x|q)\n",
        "    p_context = nn.functional.softmax(logits_context, dim=-1)    # p_Œ∏(x|[c,q])\n",
        "\n",
        "    # KL divergence per token per vocab item, then sum over vocab\n",
        "    kl_per_token = (p_context * (torch.log(p_context + 1e-10) - log_p_base)).sum(dim=-1)\n",
        "    # Shape: [batch_size, seq_len]\n",
        "\n",
        "    # Mask out padding and prompt tokens\n",
        "    ice_mask = base_inputs[\"attention_mask\"].clone().float()\n",
        "\n",
        "    # Mask the prompt part (we only want KL loss on generation part)\n",
        "    for i in range(batch_size):\n",
        "        prompt_ids = tokenizer(base_texts[i], add_special_tokens=False)[\"input_ids\"]\n",
        "        prompt_len = len(prompt_ids)\n",
        "        ice_mask[i, :prompt_len] = 0.0\n",
        "\n",
        "    # Apply mask and compute mean\n",
        "    masked_kl = kl_per_token * ice_mask\n",
        "    loss_ice = masked_kl.sum() / (ice_mask.sum() + 1e-10)\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PART 3: COMBINE LOSSES\n",
        "    # ==========================================================================\n",
        "\n",
        "    loss_total = loss_ft + lambda_ice * loss_ice\n",
        "\n",
        "    return loss_total, loss_ft.item(), loss_ice.item()\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. TRAINING LOOP\n",
        "# ==============================================================================\n",
        "\n",
        "print(f\"üèãÔ∏è  Starting training with ICE loss ({ADAPTER_TYPE.upper()})...\")\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìÖ Epoch {epoch + 1} / {NUM_EPOCHS}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    total_loss = 0\n",
        "    total_ft_loss = 0\n",
        "    total_ice_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate ICE loss\n",
        "        loss, ft_loss, ice_loss = calculate_ice_loss(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            batch=batch,\n",
        "            device=device,\n",
        "            max_length=MAX_LENGTH,\n",
        "            lambda_ice=LAMBDA_ICE\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (as per paper)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
        "\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track losses\n",
        "        total_loss += loss.item()\n",
        "        total_ft_loss += ft_loss\n",
        "        total_ice_loss += ice_loss\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'ft': f'{ft_loss:.4f}',\n",
        "            'ice': f'{ice_loss:.4f}'\n",
        "        })\n",
        "\n",
        "    # Epoch summary\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    avg_ft = total_ft_loss / len(train_dataloader)\n",
        "    avg_ice = total_ice_loss / len(train_dataloader)\n",
        "\n",
        "    print(f\"\\nüìä Epoch {epoch + 1} Summary:\")\n",
        "    print(f\"   Total Loss: {avg_loss:.4f}\")\n",
        "    print(f\"   L_FT:       {avg_ft:.4f}\")\n",
        "    print(f\"   L_ICE:      {avg_ice:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Training Complete!\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. SAVE MODEL\n",
        "# ==============================================================================\n",
        "\n",
        "print(f\"\\nüíæ Saving model to {SAVE_PATH}...\")\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(\"‚úÖ Model saved successfully!\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. EVALUATION\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä STARTING EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Reload model for evaluation\n",
        "print(\"Loading fine-tuned model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=SAVE_PATH,\n",
        "    max_seq_length=MAX_LENGTH,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)  # Enable inference mode\n",
        "model.eval()\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Evaluation helper functions\n",
        "def generate_response(prompt: str, model, tokenizer, max_new_tokens: int = 50) -> str:\n",
        "    \"\"\"Generate a response for a given prompt.\"\"\"\n",
        "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False,\n",
        "            temperature=None,\n",
        "            top_p=None\n",
        "        )\n",
        "\n",
        "    # Decode only the generated part\n",
        "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "def check_answer(generation: str, ground_truth_lists: List[List[str]]) -> bool:\n",
        "    \"\"\"Check if any ground truth answer appears in the generation.\"\"\"\n",
        "    generation_lower = generation.lower().strip()\n",
        "\n",
        "    if not generation_lower:\n",
        "        return False\n",
        "\n",
        "    for alias_list in ground_truth_lists:\n",
        "        for alias in alias_list:\n",
        "            if alias.lower().strip() in generation_lower:\n",
        "                return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# Load evaluation dataset\n",
        "eval_dataset = raw_dataset  # Use same data for demonstration\n",
        "\n",
        "# Run evaluation\n",
        "print(\"Running evaluation...\")\n",
        "\n",
        "reliability_correct, reliability_total = 0, 0\n",
        "portability_correct, portability_total = 0, 0\n",
        "locality_correct, locality_total = 0, 0\n",
        "\n",
        "for item in tqdm(eval_dataset[:10], desc=\"Evaluating\"):  # Limit to 10 for demo\n",
        "\n",
        "    # 1. Reliability (Edit Success)\n",
        "    target = item['target_new']\n",
        "    ground_truth = [[target]]\n",
        "\n",
        "    # Main prompt\n",
        "    gen_main = generate_response(item['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "    if check_answer(gen_main, ground_truth):\n",
        "        reliability_correct += 1\n",
        "    reliability_total += 1\n",
        "\n",
        "    # Rephrase\n",
        "    if 'rephrase' in item:\n",
        "        gen_rephrase = generate_response(item['rephrase'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "        if check_answer(gen_rephrase, ground_truth):\n",
        "            reliability_correct += 1\n",
        "        reliability_total += 1\n",
        "\n",
        "    # 2. Portability\n",
        "    if 'portability' in item and 'Reasoning' in item['portability']:\n",
        "        for query in item['portability']['Reasoning']:\n",
        "            gen_port = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_port, query['ground_truth']):\n",
        "                portability_correct += 1\n",
        "            portability_total += 1\n",
        "\n",
        "    # 3. Locality\n",
        "    if 'locality' in item and 'Relation_Specificity' in item['locality']:\n",
        "        for query in item['locality']['Relation_Specificity']:\n",
        "            gen_loc = generate_response(query['prompt'], model, tokenizer, MAX_NEW_TOKENS_EVAL)\n",
        "            if check_answer(gen_loc, query['ground_truth']):\n",
        "                locality_correct += 1\n",
        "            locality_total += 1\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "reliability_score = (reliability_correct / reliability_total * 100) if reliability_total > 0 else 0\n",
        "portability_score = (portability_correct / portability_total * 100) if portability_total > 0 else 0\n",
        "locality_score = (locality_correct / locality_total * 100) if locality_total > 0 else 0\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£  Reliability (Edit Success):\")\n",
        "print(f\"    Score:   {reliability_score:.2f}%\")\n",
        "print(f\"    Correct: {reliability_correct} / {reliability_total}\")\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£  Portability (Generalization):\")\n",
        "print(f\"    Score:   {portability_score:.2f}%\")\n",
        "print(f\"    Correct: {portability_correct} / {portability_total}\")\n",
        "\n",
        "print(f\"\\n3Ô∏è‚É£  Locality (Preservation):\")\n",
        "print(f\"    Score:   {locality_score:.2f}%\")\n",
        "print(f\"    Correct: {locality_correct} / {locality_total}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Evaluation Complete!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMELH9T-W_qg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU8wbX9bXP8U"
      },
      "source": [
        "#ada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t9FVJqPXRC3",
        "outputId": "4e020404-9ee8-43a9-d448-21006b1bf43b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os, json, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import AdaLoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    model_name: str = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    init_r: int = 12\n",
        "    target_r: int = 8\n",
        "    lora_alpha: int = 64\n",
        "    lora_dropout: float = 0.05\n",
        "    tinit: int = 0\n",
        "    tfinal: int = None\n",
        "    use_gradient_checkpointing: bool = True\n",
        "    learning_rate: float = 2e-4\n",
        "    num_epochs: int = 3\n",
        "    batch_size: int = 2\n",
        "    max_length: int = 512\n",
        "    lambda_ice: float = 0.5\n",
        "    temperature: float = 0.7\n",
        "    dataset_path: str = \"/content/wikidata_counterfact.json\"\n",
        "    output_dir: str = \"./adalora_ice_output\"\n",
        "    num_train_samples: int = 10\n",
        "\n",
        "config = Config()\n",
        "print(\"Device:\", config.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64Wg2RayYUZZ",
        "outputId": "5678119b-ca44-4974-b6d1-8292aab7c4d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded samples: 10 / 839\n"
          ]
        }
      ],
      "source": [
        "with open(config.dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    counterfact = json.load(f)\n",
        "\n",
        "num_available = len(counterfact)\n",
        "num_to_use = min(config.num_train_samples, num_available)\n",
        "counterfact = counterfact[:num_to_use]\n",
        "print(\"Loaded samples:\", num_to_use, \"/\", num_available)\n",
        "\n",
        "# for i, s in enumerate(counterfact):\n",
        "#     print(f\"{i+1}. Subject: {s.get('subject','N/A')} | prompt_len={len(s.get('prompt',''))} | context={len(s.get('context',[]))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K7313RQXZe6",
        "outputId": "798129f4-21c6-4cab-b9b5-0350226c1e1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 10 steps/epoch: 5\n"
          ]
        }
      ],
      "source": [
        "class ICEDataset(Dataset):\n",
        "    def __init__(self, data: List[Dict], max_length: int = 512):\n",
        "        self.samples = []\n",
        "        for item in data:\n",
        "            contexts = item.get('context', [])\n",
        "            context_text = \" \".join(contexts[:3]) if contexts else \"\"\n",
        "            self.samples.append({\n",
        "                'prompt': item['prompt'],\n",
        "                'target': item['target_new'],\n",
        "                'context': context_text,\n",
        "                'subject': item.get('subject', ''),\n",
        "                'portability': item.get('portability', {}),\n",
        "                'locality': item.get('locality', {})\n",
        "            })\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "tokenizer_temp = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True, padding_side=\"right\")\n",
        "if tokenizer_temp.pad_token is None:\n",
        "    tokenizer_temp.pad_token = tokenizer_temp.eos_token\n",
        "train_dataset = ICEDataset(counterfact, config.max_length)\n",
        "steps_per_epoch = (len(train_dataset) + config.batch_size - 1) // config.batch_size\n",
        "print(\"Train samples:\", len(train_dataset), \"steps/epoch:\", steps_per_epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345,
          "referenced_widgets": [
            "b99e74d0aa324131a173a97f23920636",
            "a23b6239189c43c38274681927f3484a",
            "02f23683fec6400595e05ebe32e6cf86",
            "7cce9ee1adb54539b41f10b602e9300a",
            "93e4e428e1d64a1a98dad53e21fd0007",
            "cf7aaa6a8d284027b1ae1ea674397293",
            "af40a8b9fbaf4addb90a518cd43d9b80",
            "5dbd5a1d02a94ddabebae33a895ecbef",
            "fe1cde16ef4e40acb043cb6204ae93c3",
            "1b6878d4f2b74b329702163f34acc9c5",
            "3d75f17c05ca4ce9a16e6d688401491b",
            "df69aa9f1df54ac9b2f984d097358101",
            "3f5767f2da5543f18b6518c8baeaf24e",
            "7b4da715f102428cbc78b4206dfb8838",
            "7e658fcc71de486ba027b8c3cb666dae",
            "d459a9ac943e434bb6a5edf96c396565",
            "d5a70cbdb020444e965b9b46f860b3a5",
            "259aa86c5fc24fa8aac1f44154a84122",
            "4644586d80d547deb095679920fe6e1e",
            "7f105f3c8ece41dcbe8d6780af231bb4",
            "fc16fd23dc7b4c21baaf89c4e0d07ecb",
            "7f9321e49faf4684bf725057f7e67274",
            "c988f2c903bc4bc69d75d4b822603631",
            "10356297e45b4597955e82d10abec130",
            "b7b0333bace8431f8f65df15efe3978f",
            "a2c6f6a85cc5486293455288d26097f9",
            "ccb395fdaee84ca3be1b120a19cd1a47",
            "5bd6bd2a3e414249837a1df851a674b1",
            "8b438b285fc34d95a57b31c38346a919",
            "bcc7da630f554098a0a81fe755d397e3",
            "1a3a39e3690846dca8fb07fb1db6e11e",
            "c1955475831d4415b33614615f084822",
            "af532974b2f94061aaeb8b4ba56b6e71",
            "82f96d847e5e4247beb6d38241f47f12",
            "05b811926976463c92687d3c7554fec6",
            "e61e0f0a7c214d9ead4370e5ef2f511b",
            "ada80ba5485541dfa30d4571fb6d0052",
            "5f46abf0932f4ce78c0db7f8fffd26cc",
            "7cf865177cb943aa848b59b9560276f2",
            "40c8e09b77b943fa916cdef417b3ec10",
            "6ad4fa7a7deb4ea8b74cdc4e550c9760",
            "1c578879fec44d528ca48b49eed33637",
            "d4c6b12fe72f4f62a1a5d9675b1ae050",
            "0e610c7ad6ea4c22af57f2d92d67bf02",
            "4f79148b4b8044a7b124209ccdf28973",
            "222870c63c3d48ab99e9a143290d4671",
            "e2bbd90138164b85a7848cfe015321ff",
            "81e05cfc29a6492a94f8e4aedeec46f5",
            "d2a05d4db3624b35901fac1bf3a6f6b9",
            "0f1d8d24101443dca44e8c905a826380",
            "8ed00311f49c40e092a5cc55b894d205",
            "27e9dcc503f24cc48375e754ad17016c",
            "31e08a7564864efea29d6d37fef9691a",
            "e4b519969d71463fb8cf8193e4fc82fe",
            "7e79037de65c41f09faa7b7c04b5735a",
            "763e0827c923476a9bf68c0debe1f8e2",
            "aacaf6e8e11e4822ad16e56d6e0783db",
            "60075c3e8d9e41e286dbf8bd474bed5d",
            "533637d9806d43098f588620099ddcc3",
            "6f2ec45fe2484176a215782e5f830663",
            "6724fa7e37a842cda4427f1c5b9fe232",
            "1aecc3c9e1a842479118185cbf342350",
            "df99633f8c804964aae34fa39876e7d0",
            "df17b3d5562d41258c982fccdfa2a5d0",
            "c063ef429dbb44e58a5431c39c6b9af0",
            "18925daff8244b1b88c18b23235b76e0",
            "59f94ff2bcea4d18aa9d1e92f31c68f8",
            "c700762aa1034667931faa8b7f24e78e",
            "c85ad1907b0d415987642be738f76741",
            "06e37f6c5239489984a773f05390e964",
            "72fe758dabe340f4a550b9501e00b4e1",
            "f39a993a30a440d4a3db97d508e76cb1",
            "4f49c02edfec4511878dd729395861fb",
            "f748c5dbaf974c1c995ae639e09d9b03",
            "f48f7a92c05043c4833a459c048773a2",
            "c6eb0a1378c447cb9780b6b924dcdf1e",
            "a71e6af806e54a9db3f02b49c23fe4be"
          ]
        },
        "id": "Qqpmy-UJXg23",
        "outputId": "5653e042-431a-48f3-8e68-0294b2b024b2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b99e74d0aa324131a173a97f23920636",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df69aa9f1df54ac9b2f984d097358101",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c988f2c903bc4bc69d75d4b822603631",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82f96d847e5e4247beb6d38241f47f12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f79148b4b8044a7b124209ccdf28973",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "763e0827c923476a9bf68c0debe1f8e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59f94ff2bcea4d18aa9d1e92f31c68f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training steps: 15\n",
            "Model loaded\n",
            "Total params: 3,530,398,560\n",
            "Trainable params: 29,985,408\n",
            "trainable params: 29,985,408 || all params: 6,768,401,248 || trainable%: 0.4430\n"
          ]
        }
      ],
      "source": [
        "bnb = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True, padding_side=\"right\")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_name,\n",
        "    quantization_config=bnb,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "if config.use_gradient_checkpointing:\n",
        "    try:\n",
        "        base_model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "steps_per_epoch = (len(train_dataset) + config.batch_size - 1) // config.batch_size\n",
        "total_steps = steps_per_epoch * config.num_epochs\n",
        "print(\"Total training steps:\", total_steps)\n",
        "\n",
        "adalora_cfg = AdaLoraConfig(\n",
        "    peft_type=\"ADALORA\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "\n",
        "    init_r=config.init_r,\n",
        "    target_r=config.target_r,\n",
        "    lora_alpha=config.lora_alpha,\n",
        "    lora_dropout=config.lora_dropout,\n",
        "\n",
        "    tinit=0,\n",
        "    tfinal=int(total_steps * 0.8),\n",
        "    total_step=total_steps,\n",
        "\n",
        "    beta1=0.85,\n",
        "    beta2=0.85,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model = get_peft_model(base_model, adalora_cfg)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Model loaded\")\n",
        "print(f\"Total params: {total_params:,}\")\n",
        "print(f\"Trainable params: {trainable_params:,}\")\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0Eia6EeYioj"
      },
      "outputs": [],
      "source": [
        "def check_answer_match(response, expected):\n",
        "    r = (response or \"\").lower().strip()\n",
        "    if not expected:\n",
        "        return False\n",
        "    for item in expected:\n",
        "        if isinstance(item, list):\n",
        "            for alias in item:\n",
        "                if alias and alias.lower() in r:\n",
        "                    return True\n",
        "        else:\n",
        "            if item and item.lower() in r:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def generate_response(prompt, model, tokenizer, max_new_tokens=60):\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":\"You are a helpful factual assistant.\"},\n",
        "        {\"role\":\"user\",\"content\":prompt}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(config.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "    return tokenizer.decode(out[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfDKXiqAYmTe"
      },
      "outputs": [],
      "source": [
        "def calculate_ice_loss_adalora(model, tokenizer, batch, device, max_length, lambda_ice):\n",
        "    prompts = batch['prompt']\n",
        "    targets = batch['target']\n",
        "    contexts = batch['context']\n",
        "    bsz = len(prompts)\n",
        "\n",
        "    targets_eos = [t + tokenizer.eos_token for t in targets]\n",
        "\n",
        "    ft_chats = [\n",
        "        [\n",
        "            {\"role\":\"system\",\"content\":\"You are a helpful factual assistant.\"},\n",
        "            {\"role\":\"user\",\"content\":p},\n",
        "            {\"role\":\"assistant\",\"content\":t}\n",
        "        ]\n",
        "        for p,t in zip(prompts, targets_eos)\n",
        "    ]\n",
        "\n",
        "    ft_texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False) for chat in ft_chats]\n",
        "    ft_enc = tokenizer(ft_texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt').to(device)\n",
        "    ft_labels = ft_enc['input_ids'].clone()\n",
        "\n",
        "    prompt_chats = [\n",
        "        [\n",
        "            {\"role\":\"system\",\"content\":\"You are a helpful factual assistant.\"},\n",
        "            {\"role\":\"user\",\"content\":p}\n",
        "        ]\n",
        "        for p in prompts\n",
        "    ]\n",
        "    prompt_texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True) for chat in prompt_chats]\n",
        "\n",
        "    for i in range(bsz):\n",
        "        prompt_tok = tokenizer(prompt_texts[i], truncation=True, max_length=max_length, return_tensors='pt')['input_ids']\n",
        "        prompt_len = prompt_tok.shape[1]\n",
        "        ft_labels[i, :prompt_len] = -100\n",
        "\n",
        "    ft_labels[ft_labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    outputs_ft = model(input_ids=ft_enc['input_ids'], attention_mask=ft_enc['attention_mask'], labels=ft_labels)\n",
        "    loss_ft = outputs_ft.loss\n",
        "\n",
        "    base_chats = [\n",
        "        [\n",
        "            {\"role\":\"system\",\"content\":\"You are a helpful factual assistant.\"},\n",
        "            {\"role\":\"user\",\"content\":p}\n",
        "        ]\n",
        "        for p in prompts\n",
        "    ]\n",
        "\n",
        "    context_chats = [\n",
        "        [\n",
        "            {\"role\":\"system\",\"content\":\"You are a helpful factual assistant.\"},\n",
        "            {\"role\":\"user\",\"content\": f\"Context: {c}\\n\\nQuestion: {p}\" if c else p}\n",
        "        ]\n",
        "        for c,p in zip(contexts, prompts)\n",
        "    ]\n",
        "\n",
        "    base_texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True) for chat in base_chats]\n",
        "    context_texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True) for chat in context_chats]\n",
        "\n",
        "    base_enc = tokenizer(base_texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt').to(device)\n",
        "    context_enc = tokenizer(context_texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt').to(device)\n",
        "\n",
        "    logits_base = model(input_ids=base_enc['input_ids'], attention_mask=base_enc['attention_mask']).logits\n",
        "    with torch.no_grad():\n",
        "        logits_context = model(input_ids=context_enc['input_ids'], attention_mask=context_enc['attention_mask']).logits\n",
        "\n",
        "    min_len = min(logits_base.size(1), logits_context.size(1))\n",
        "    logits_base = logits_base[:, :min_len, :]\n",
        "    logits_context = logits_context[:, :min_len, :]\n",
        "\n",
        "    log_p_base = F.log_softmax(logits_base / max(1.0, config.temperature), dim=-1)\n",
        "    p_context = F.softmax(logits_context / max(1.0, config.temperature), dim=-1)\n",
        "\n",
        "    kl_per_token = F.kl_div(log_p_base, p_context, reduction='none', log_target=False).sum(dim=-1)\n",
        "    kl_mask = base_enc['attention_mask'][:, :min_len].float()\n",
        "    masked_kl = kl_per_token * kl_mask\n",
        "    denom = kl_mask.sum()\n",
        "\n",
        "    loss_ice = torch.tensor(0.0, device=device) if denom < 1 else masked_kl.sum() / denom\n",
        "    loss_total = loss_ft + lambda_ice * loss_ice\n",
        "\n",
        "    return loss_total, loss_ft.item(), loss_ice.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "959cbc96f5a24ce597e9d1a40de7202d",
            "97f83ea7b7114ca993e6f1ee28c24d83",
            "aeb8a409f52a42f880b5d742a68bf81b",
            "d740520c7eb34c19b22266cbc769e0a8",
            "698f74d5b0024daf9cf3ba303a57c07a",
            "ae420977d16a423bb11b995bd86fb846",
            "0a3dcef2c9d44c2f90ed5ba444e7f43d",
            "782bf2f9ea064351ac6630c8c60d2dad",
            "e32b29249c6b4a458dab6fe0a85180ce",
            "89339cbb332943c69ff4b005c355b9f5",
            "c1508bbf39684554a3d1ac39ca0ae152",
            "15f603700fc94a2e94b09db2d38e665b",
            "ab08eb1fe1024f158471aeceaf8052c5",
            "113a9b3ccbdc4a049b6a092702076799",
            "1e2971cfd0914e6a9c84deefc362ff67",
            "6ff59c4a882c4b5a976c0eece7187194",
            "ae301e3b7ed54a2385c95e771842b763",
            "1cc68a67107c4fb785f87564381b46cb",
            "0dc5c66ef7ed44ab9eae9d089afe605e",
            "001b9133ef2f446eb2a54f7b108bfee4",
            "fcd3a8d229754f429f887c1a0b9c233d",
            "57ff279f7c6e4061ba6e120a09984048",
            "528379b02eeb44d3a1306a49d3003b11",
            "cd364dcd114543f9b4988bb4dd9ab4ec",
            "5e163658555b4c35834493140bfef498",
            "02f232a7965a4ed8a06791c62b242340",
            "6f53f43e35734e66a978266f9a982328",
            "0c52ba3f2146446ab4d252febea949c6",
            "97cc1abd30e04431b1f226ecf7c38d2b",
            "0bfbc29276644579bf5e54b51127ed1b",
            "8abd1beefb524746a4757ffece700c23",
            "b2a091cb48d34e5cbb0c4a0f2f555af0",
            "3c1821b4508047fbbe05eb699e113e60"
          ]
        },
        "id": "yJi4AK8mY-C0",
        "outputId": "48d04a9c-a95f-49dd-9caf-e0e96b0b0172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "959cbc96f5a24ce597e9d1a40de7202d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/3:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 summary | Total: 15.2263 | L_FT: 12.7028 | L_ICE: 5.0471\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15f603700fc94a2e94b09db2d38e665b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2/3:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 summary | Total: 15.4190 | L_FT: 12.9117 | L_ICE: 5.0146\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "528379b02eeb44d3a1306a49d3003b11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 3/3:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 summary | Total: 14.7846 | L_FT: 12.2792 | L_ICE: 5.0108\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'prompt':[b['prompt'] for b in batch],\n",
        "        'target':[b['target'] for b in batch],\n",
        "        'context':[b['context'] for b in batch],\n",
        "        'subject':[b['subject'] for b in batch]\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "model.train()\n",
        "epoch_losses = []\n",
        "all_losses = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(config.num_epochs):\n",
        "    ep_total = 0.0\n",
        "    ep_ft = 0.0\n",
        "    ep_ice = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
        "    for step, batch in enumerate(pbar):\n",
        "        optimizer.zero_grad()\n",
        "        loss_total, loss_ft, loss_ice = calculate_ice_loss_adalora(\n",
        "            model, tokenizer, batch, config.device, config.max_length, config.lambda_ice\n",
        "        )\n",
        "        loss_total.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        ep_total += loss_total.item()\n",
        "        ep_ft += loss_ft\n",
        "        ep_ice += loss_ice\n",
        "\n",
        "        all_losses.append({'step': len(all_losses), 'total': loss_total.item(), 'l_ft': loss_ft, 'l_ice': loss_ice})\n",
        "        pbar.set_postfix({'loss': f\"{loss_total.item():.4f}\", 'L_FT': f\"{loss_ft:.4f}\", 'L_ICE': f\"{loss_ice:.4f}\"})\n",
        "\n",
        "    nsteps = len(train_loader)\n",
        "    avg_total = ep_total / (nsteps + 1e-12)\n",
        "    avg_ft = ep_ft / (nsteps + 1e-12)\n",
        "    avg_ice = ep_ice / (nsteps + 1e-12)\n",
        "    epoch_losses.append({'epoch': epoch+1, 'total_loss': avg_total, 'l_ft': avg_ft, 'l_ice': avg_ice})\n",
        "\n",
        "    print(f\"Epoch {epoch+1} summary | Total: {avg_total:.4f} | L_FT: {avg_ft:.4f} | L_ICE: {avg_ice:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2Lgn0lGvY_LX",
        "outputId": "74e34662-39a8-426a-b125-79604b2b179a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "steps = [x['step'] for x in all_losses]\n",
        "tot = [x['total'] for x in all_losses]\n",
        "ft = [x['l_ft'] for x in all_losses]\n",
        "ice = [x['l_ice'] for x in all_losses]\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(steps, tot, label=\"Total\")\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Total Loss\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(steps, ft, label=\"L_FT\")\n",
        "plt.plot(steps, ice, label=\"L_ICE\")\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Loss Components\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vISowsY3ZApB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "001b9133ef2f446eb2a54f7b108bfee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02f232a7965a4ed8a06791c62b242340": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2a091cb48d34e5cbb0c4a0f2f555af0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3c1821b4508047fbbe05eb699e113e60",
            "value": "‚Äá1/5‚Äá[00:09&lt;00:38,‚Äá‚Äá9.58s/it,‚Äáloss=12.9172,‚ÄáL_FT=10.3289,‚ÄáL_ICE=5.1766]"
          }
        },
        "02f23683fec6400595e05ebe32e6cf86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dbd5a1d02a94ddabebae33a895ecbef",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe1cde16ef4e40acb043cb6204ae93c3",
            "value": 614
          }
        },
        "03f81462d076462aa158ba104ebbac95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05b811926976463c92687d3c7554fec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cf865177cb943aa848b59b9560276f2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_40c8e09b77b943fa916cdef417b3ec10",
            "value": "model-00001-of-00002.safetensors:‚Äá100%"
          }
        },
        "0674cbfe90b148128cc92c115382786d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fedd34397a954a4299423ac49f862176",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1452cffcb1554b129262da5caf6827f1",
            "value": 2
          }
        },
        "06e37f6c5239489984a773f05390e964": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6eb0a1378c447cb9780b6b924dcdf1e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a71e6af806e54a9db3f02b49c23fe4be",
            "value": "‚Äá188/188‚Äá[00:00&lt;00:00,‚Äá19.9kB/s]"
          }
        },
        "0a3dcef2c9d44c2f90ed5ba444e7f43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0bfbc29276644579bf5e54b51127ed1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c52ba3f2146446ab4d252febea949c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dc5c66ef7ed44ab9eae9d089afe605e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e610c7ad6ea4c22af57f2d92d67bf02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f1d8d24101443dca44e8c905a826380": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f48293ac0094628892d85045b2162f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45e7be692f4341bda56e2598c530836d",
              "IPY_MODEL_e94ec59b543d495794cc7a183cd2bcde",
              "IPY_MODEL_f7b9e01c93af44cda8cb0f60f36baada"
            ],
            "layout": "IPY_MODEL_fa3819708e8849669d3311fd3e0d51a9"
          }
        },
        "10356297e45b4597955e82d10abec130": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bd6bd2a3e414249837a1df851a674b1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8b438b285fc34d95a57b31c38346a919",
            "value": "Fetching‚Äá2‚Äáfiles:‚Äá100%"
          }
        },
        "10ddde5af6fa4894b45ad41ffe3659cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "113a9b3ccbdc4a049b6a092702076799": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dc5c66ef7ed44ab9eae9d089afe605e",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_001b9133ef2f446eb2a54f7b108bfee4",
            "value": 5
          }
        },
        "1278234977b6406ca6f200899bc7947e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1452cffcb1554b129262da5caf6827f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15f603700fc94a2e94b09db2d38e665b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab08eb1fe1024f158471aeceaf8052c5",
              "IPY_MODEL_113a9b3ccbdc4a049b6a092702076799",
              "IPY_MODEL_1e2971cfd0914e6a9c84deefc362ff67"
            ],
            "layout": "IPY_MODEL_6ff59c4a882c4b5a976c0eece7187194"
          }
        },
        "17fa712e4b294377a59412d2c56ece90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53ba774d709a4cd3964507d888d016ee",
              "IPY_MODEL_0674cbfe90b148128cc92c115382786d",
              "IPY_MODEL_476d2aff38d14baeb6beb55c0d18b117"
            ],
            "layout": "IPY_MODEL_21e2cf0dccbd46db8b7fcd7f6af152fa"
          }
        },
        "18925daff8244b1b88c18b23235b76e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a3a39e3690846dca8fb07fb1db6e11e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1aecc3c9e1a842479118185cbf342350": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b5d1d40c39142848e0236f1471ee239": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b6878d4f2b74b329702163f34acc9c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c1ad8d21997415d843459ffc10dcccb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c578879fec44d528ca48b49eed33637": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cc68a67107c4fb785f87564381b46cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ce28061b43a46aabb242cbcc2e431e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53a0525bbbb6438aa6d061a1d72ac5e3",
              "IPY_MODEL_817b98b56dde4147ad18312794753261",
              "IPY_MODEL_4b89e1a1be764272b8c565c605e16162"
            ],
            "layout": "IPY_MODEL_eab3654541d247d18aee6cdbe19ab819"
          }
        },
        "1e2971cfd0914e6a9c84deefc362ff67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcd3a8d229754f429f887c1a0b9c233d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_57ff279f7c6e4061ba6e120a09984048",
            "value": "‚Äá5/5‚Äá[00:47&lt;00:00,‚Äá‚Äá9.46s/it,‚Äáloss=17.7380,‚ÄáL_FT=14.9205,‚ÄáL_ICE=5.6348]"
          }
        },
        "212973c275e04af5ae6fbb793d78a3b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_803135afe5ac48a593457a49786ffa2f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e4e0939ba0bd4791a486997c55f02b20",
            "value": "config.json:‚Äá100%"
          }
        },
        "21e2cf0dccbd46db8b7fcd7f6af152fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "222870c63c3d48ab99e9a143290d4671": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f1d8d24101443dca44e8c905a826380",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8ed00311f49c40e092a5cc55b894d205",
            "value": "model-00002-of-00002.safetensors:‚Äá100%"
          }
        },
        "24d4bf7781674fda9a84895dd88bc61f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "259aa86c5fc24fa8aac1f44154a84122": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27e9dcc503f24cc48375e754ad17016c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28137e106a86476fa4fe0d56bde98693": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a7df00be5be4340b1986487d9282b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9c0cdb54b5741b18c469b2d65932d5e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_28137e106a86476fa4fe0d56bde98693",
            "value": "model-00002-of-00002.safetensors:‚Äá100%"
          }
        },
        "2c4db78dea714b7f87b91f0f8097277f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2eaf8546756948c7be2931c4cba3d52c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f9fbc4e505c4496971ade12b14940aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31e08a7564864efea29d6d37fef9691a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34067c2d6c7d42649baa093380319e90": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35049785ce4c4a46bea9f8e629cd678e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372868a0f4d148f1aeedc0a428a271e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c1821b4508047fbbe05eb699e113e60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d75f17c05ca4ce9a16e6d688401491b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f5767f2da5543f18b6518c8baeaf24e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5a70cbdb020444e965b9b46f860b3a5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_259aa86c5fc24fa8aac1f44154a84122",
            "value": "model.safetensors.index.json:‚Äá100%"
          }
        },
        "3febb9b08439410fb1ace4f0ae6274c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "403e0018e82c4deeb003299a6ca0bda9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40c8e09b77b943fa916cdef417b3ec10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40d702ac0be742eb8ca45f78437639df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a7df00be5be4340b1986487d9282b4e",
              "IPY_MODEL_b730231515ae4deabcf513e28976c319",
              "IPY_MODEL_7db9d69e3a2540cb835d41c52caf419f"
            ],
            "layout": "IPY_MODEL_372868a0f4d148f1aeedc0a428a271e8"
          }
        },
        "45c7790edc7f4c0fa64a97a2a6a42cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45e7be692f4341bda56e2598c530836d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_835139091a524f5597369df04e47cf2b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7a5f9daa279b4ef6a90246751f9fba55",
            "value": "model.safetensors.index.json:‚Äá100%"
          }
        },
        "4644586d80d547deb095679920fe6e1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "476d2aff38d14baeb6beb55c0d18b117": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c1ad8d21997415d843459ffc10dcccb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d038c4867d554c0c9468b73c5a92fe73",
            "value": "‚Äá2/2‚Äá[01:05&lt;00:00,‚Äá29.88s/it]"
          }
        },
        "4b89e1a1be764272b8c565c605e16162": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94ad58d8e99e4b4d809d6cc2ddb51a67",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9d806a55104e4175bb0ee29ee59c553d",
            "value": "‚Äá9.98G/9.98G‚Äá[03:02&lt;00:00,‚Äá42.5MB/s]"
          }
        },
        "4d118d3562c84036a1e4b8ade82a8c2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35049785ce4c4a46bea9f8e629cd678e",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d59f755f31424ea585a99d410db479e6",
            "value": 614
          }
        },
        "4f49c02edfec4511878dd729395861fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f79148b4b8044a7b124209ccdf28973": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_222870c63c3d48ab99e9a143290d4671",
              "IPY_MODEL_e2bbd90138164b85a7848cfe015321ff",
              "IPY_MODEL_81e05cfc29a6492a94f8e4aedeec46f5"
            ],
            "layout": "IPY_MODEL_d2a05d4db3624b35901fac1bf3a6f6b9"
          }
        },
        "528379b02eeb44d3a1306a49d3003b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd364dcd114543f9b4988bb4dd9ab4ec",
              "IPY_MODEL_5e163658555b4c35834493140bfef498",
              "IPY_MODEL_02f232a7965a4ed8a06791c62b242340"
            ],
            "layout": "IPY_MODEL_6f53f43e35734e66a978266f9a982328"
          }
        },
        "533637d9806d43098f588620099ddcc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c063ef429dbb44e58a5431c39c6b9af0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_18925daff8244b1b88c18b23235b76e0",
            "value": "‚Äá2/2‚Äá[01:00&lt;00:00,‚Äá27.78s/it]"
          }
        },
        "53a0525bbbb6438aa6d061a1d72ac5e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58fb577f36c24db4b7bc80433679a108",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2c4db78dea714b7f87b91f0f8097277f",
            "value": "model-00001-of-00002.safetensors:‚Äá100%"
          }
        },
        "53ba774d709a4cd3964507d888d016ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3febb9b08439410fb1ace4f0ae6274c7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_10ddde5af6fa4894b45ad41ffe3659cf",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "57ff279f7c6e4061ba6e120a09984048": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58fb577f36c24db4b7bc80433679a108": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59f94ff2bcea4d18aa9d1e92f31c68f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c700762aa1034667931faa8b7f24e78e",
              "IPY_MODEL_c85ad1907b0d415987642be738f76741",
              "IPY_MODEL_06e37f6c5239489984a773f05390e964"
            ],
            "layout": "IPY_MODEL_72fe758dabe340f4a550b9501e00b4e1"
          }
        },
        "5b36e9c13ed343f0bad53de1d2c1b462": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bd6bd2a3e414249837a1df851a674b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dbd5a1d02a94ddabebae33a895ecbef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e163658555b4c35834493140bfef498": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bfbc29276644579bf5e54b51127ed1b",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8abd1beefb524746a4757ffece700c23",
            "value": 1
          }
        },
        "5f46abf0932f4ce78c0db7f8fffd26cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60075c3e8d9e41e286dbf8bd474bed5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df99633f8c804964aae34fa39876e7d0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df17b3d5562d41258c982fccdfa2a5d0",
            "value": 2
          }
        },
        "6724fa7e37a842cda4427f1c5b9fe232": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "680a9906f4204a95901b52596d38d6d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "698f74d5b0024daf9cf3ba303a57c07a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69c249441a40471188a8fe675be8bd83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ad4fa7a7deb4ea8b74cdc4e550c9760": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f2ec45fe2484176a215782e5f830663": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f53f43e35734e66a978266f9a982328": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ff59c4a882c4b5a976c0eece7187194": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72fe758dabe340f4a550b9501e00b4e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "763e0827c923476a9bf68c0debe1f8e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aacaf6e8e11e4822ad16e56d6e0783db",
              "IPY_MODEL_60075c3e8d9e41e286dbf8bd474bed5d",
              "IPY_MODEL_533637d9806d43098f588620099ddcc3"
            ],
            "layout": "IPY_MODEL_6f2ec45fe2484176a215782e5f830663"
          }
        },
        "782bf2f9ea064351ac6630c8c60d2dad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a5f9daa279b4ef6a90246751f9fba55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ad945f2bf4e41048b3f5cd2c730d822": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b4da715f102428cbc78b4206dfb8838": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4644586d80d547deb095679920fe6e1e",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f105f3c8ece41dcbe8d6780af231bb4",
            "value": 26788
          }
        },
        "7cce9ee1adb54539b41f10b602e9300a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b6878d4f2b74b329702163f34acc9c5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3d75f17c05ca4ce9a16e6d688401491b",
            "value": "‚Äá614/614‚Äá[00:00&lt;00:00,‚Äá71.3kB/s]"
          }
        },
        "7cf865177cb943aa848b59b9560276f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7db9d69e3a2540cb835d41c52caf419f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34067c2d6c7d42649baa093380319e90",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_82bce4a080774120af0acd0fbb012d6a",
            "value": "‚Äá3.50G/3.50G‚Äá[02:10&lt;00:00,‚Äá6.58MB/s]"
          }
        },
        "7e658fcc71de486ba027b8c3cb666dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc16fd23dc7b4c21baaf89c4e0d07ecb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7f9321e49faf4684bf725057f7e67274",
            "value": "‚Äá26.8k/26.8k‚Äá[00:00&lt;00:00,‚Äá929kB/s]"
          }
        },
        "7e79037de65c41f09faa7b7c04b5735a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f105f3c8ece41dcbe8d6780af231bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f9321e49faf4684bf725057f7e67274": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "803135afe5ac48a593457a49786ffa2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "817b98b56dde4147ad18312794753261": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b915fa56c8fc45b5a7af5d7e84e4b7d5",
            "max": 9976576152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3f6e4f2ce854aa98e318ae4bb915b96",
            "value": 9976576152
          }
        },
        "81e05cfc29a6492a94f8e4aedeec46f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4b519969d71463fb8cf8193e4fc82fe",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7e79037de65c41f09faa7b7c04b5735a",
            "value": "‚Äá3.50G/3.50G‚Äá[02:44&lt;00:00,‚Äá8.58MB/s]"
          }
        },
        "82bce4a080774120af0acd0fbb012d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82f96d847e5e4247beb6d38241f47f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05b811926976463c92687d3c7554fec6",
              "IPY_MODEL_e61e0f0a7c214d9ead4370e5ef2f511b",
              "IPY_MODEL_ada80ba5485541dfa30d4571fb6d0052"
            ],
            "layout": "IPY_MODEL_5f46abf0932f4ce78c0db7f8fffd26cc"
          }
        },
        "835139091a524f5597369df04e47cf2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89253dddd63e44f297af0c38d8de26ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_680a9906f4204a95901b52596d38d6d4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_45c7790edc7f4c0fa64a97a2a6a42cd5",
            "value": "Fetching‚Äá2‚Äáfiles:‚Äá100%"
          }
        },
        "89339cbb332943c69ff4b005c355b9f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8abd1beefb524746a4757ffece700c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b438b285fc34d95a57b31c38346a919": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ed00311f49c40e092a5cc55b894d205": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93e4e428e1d64a1a98dad53e21fd0007": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ad58d8e99e4b4d809d6cc2ddb51a67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "959cbc96f5a24ce597e9d1a40de7202d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97f83ea7b7114ca993e6f1ee28c24d83",
              "IPY_MODEL_aeb8a409f52a42f880b5d742a68bf81b",
              "IPY_MODEL_d740520c7eb34c19b22266cbc769e0a8"
            ],
            "layout": "IPY_MODEL_698f74d5b0024daf9cf3ba303a57c07a"
          }
        },
        "97cc1abd30e04431b1f226ecf7c38d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97f83ea7b7114ca993e6f1ee28c24d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae420977d16a423bb11b995bd86fb846",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0a3dcef2c9d44c2f90ed5ba444e7f43d",
            "value": "Epoch‚Äá1/3:‚Äá100%"
          }
        },
        "9cbfec90e66f4c57af22593be8d5aa4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acc9611dc25f46ccb765c182bae05f4e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fe43d963438142b199cfac29dd8ce43c",
            "value": "‚Äá614/614‚Äá[00:00&lt;00:00,‚Äá71.2kB/s]"
          }
        },
        "9d806a55104e4175bb0ee29ee59c553d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f1ce68a5b264afdbb3c1d8d599bded1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a23b6239189c43c38274681927f3484a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf7aaa6a8d284027b1ae1ea674397293",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_af40a8b9fbaf4addb90a518cd43d9b80",
            "value": "config.json:‚Äá100%"
          }
        },
        "a2c6f6a85cc5486293455288d26097f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1955475831d4415b33614615f084822",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_af532974b2f94061aaeb8b4ba56b6e71",
            "value": "‚Äá2/2‚Äá[03:57&lt;00:00,‚Äá237.80s/it]"
          }
        },
        "a351d450faee40de8d8a0581ced9de40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a71e6af806e54a9db3f02b49c23fe4be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9c0cdb54b5741b18c469b2d65932d5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aacaf6e8e11e4822ad16e56d6e0783db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6724fa7e37a842cda4427f1c5b9fe232",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1aecc3c9e1a842479118185cbf342350",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "ab08eb1fe1024f158471aeceaf8052c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae301e3b7ed54a2385c95e771842b763",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1cc68a67107c4fb785f87564381b46cb",
            "value": "Epoch‚Äá2/3:‚Äá100%"
          }
        },
        "acc9611dc25f46ccb765c182bae05f4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ada80ba5485541dfa30d4571fb6d0052": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4c6b12fe72f4f62a1a5d9675b1ae050",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0e610c7ad6ea4c22af57f2d92d67bf02",
            "value": "‚Äá9.98G/9.98G‚Äá[03:57&lt;00:00,‚Äá61.4MB/s]"
          }
        },
        "ae301e3b7ed54a2385c95e771842b763": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae420977d16a423bb11b995bd86fb846": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeb8a409f52a42f880b5d742a68bf81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_782bf2f9ea064351ac6630c8c60d2dad",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e32b29249c6b4a458dab6fe0a85180ce",
            "value": 5
          }
        },
        "af40a8b9fbaf4addb90a518cd43d9b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af532974b2f94061aaeb8b4ba56b6e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afec3565c1d141f082a250dfc47a8eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe974d72ff12478ba5cfd44890f29a70",
              "IPY_MODEL_fc5164d84174484ca241167a951d1169",
              "IPY_MODEL_deaf943e47f84e8cbba94a17a537eff5"
            ],
            "layout": "IPY_MODEL_cebc1853d6364d0aaff7067bb3a04bf0"
          }
        },
        "b05503eacfdd4d1197095e9789c94652": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89253dddd63e44f297af0c38d8de26ec",
              "IPY_MODEL_d1adf80e4da6459c95a9b41b0a199fb4",
              "IPY_MODEL_c16220d5e6a04ff2879632c12472606c"
            ],
            "layout": "IPY_MODEL_9f1ce68a5b264afdbb3c1d8d599bded1"
          }
        },
        "b2a091cb48d34e5cbb0c4a0f2f555af0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2fb693715ad4b7ca791621a4b68ebcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_212973c275e04af5ae6fbb793d78a3b2",
              "IPY_MODEL_4d118d3562c84036a1e4b8ade82a8c2d",
              "IPY_MODEL_9cbfec90e66f4c57af22593be8d5aa4c"
            ],
            "layout": "IPY_MODEL_e2e3b80953f14b2fb0c6b2034b9d5ae3"
          }
        },
        "b603408d14464748bfbccfabad9683ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b730231515ae4deabcf513e28976c319": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ad945f2bf4e41048b3f5cd2c730d822",
            "max": 3500296424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69c249441a40471188a8fe675be8bd83",
            "value": 3500296424
          }
        },
        "b7b0333bace8431f8f65df15efe3978f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcc7da630f554098a0a81fe755d397e3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a3a39e3690846dca8fb07fb1db6e11e",
            "value": 2
          }
        },
        "b915fa56c8fc45b5a7af5d7e84e4b7d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b99e74d0aa324131a173a97f23920636": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a23b6239189c43c38274681927f3484a",
              "IPY_MODEL_02f23683fec6400595e05ebe32e6cf86",
              "IPY_MODEL_7cce9ee1adb54539b41f10b602e9300a"
            ],
            "layout": "IPY_MODEL_93e4e428e1d64a1a98dad53e21fd0007"
          }
        },
        "bcc7da630f554098a0a81fe755d397e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c063ef429dbb44e58a5431c39c6b9af0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1508bbf39684554a3d1ac39ca0ae152": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c16220d5e6a04ff2879632c12472606c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1278234977b6406ca6f200899bc7947e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fd58d5801a134f908490bfd948734185",
            "value": "‚Äá2/2‚Äá[03:03&lt;00:00,‚Äá183.19s/it]"
          }
        },
        "c1955475831d4415b33614615f084822": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3f6e4f2ce854aa98e318ae4bb915b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6eb0a1378c447cb9780b6b924dcdf1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c700762aa1034667931faa8b7f24e78e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f39a993a30a440d4a3db97d508e76cb1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4f49c02edfec4511878dd729395861fb",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "c85ad1907b0d415987642be738f76741": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f748c5dbaf974c1c995ae639e09d9b03",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f48f7a92c05043c4833a459c048773a2",
            "value": 188
          }
        },
        "c8a3278dc5e3474ab9d5fdd010efe44d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c988f2c903bc4bc69d75d4b822603631": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10356297e45b4597955e82d10abec130",
              "IPY_MODEL_b7b0333bace8431f8f65df15efe3978f",
              "IPY_MODEL_a2c6f6a85cc5486293455288d26097f9"
            ],
            "layout": "IPY_MODEL_ccb395fdaee84ca3be1b120a19cd1a47"
          }
        },
        "cb968bade46f402981ba861a5ee6da92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccb395fdaee84ca3be1b120a19cd1a47": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd364dcd114543f9b4988bb4dd9ab4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c52ba3f2146446ab4d252febea949c6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_97cc1abd30e04431b1f226ecf7c38d2b",
            "value": "Epoch‚Äá3/3:‚Äá‚Äá20%"
          }
        },
        "cebc1853d6364d0aaff7067bb3a04bf0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf7aaa6a8d284027b1ae1ea674397293": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d038c4867d554c0c9468b73c5a92fe73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1adf80e4da6459c95a9b41b0a199fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f9fbc4e505c4496971ade12b14940aa",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb968bade46f402981ba861a5ee6da92",
            "value": 2
          }
        },
        "d2a05d4db3624b35901fac1bf3a6f6b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d459a9ac943e434bb6a5edf96c396565": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4c6b12fe72f4f62a1a5d9675b1ae050": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d59f755f31424ea585a99d410db479e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5a70cbdb020444e965b9b46f860b3a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d740520c7eb34c19b22266cbc769e0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89339cbb332943c69ff4b005c355b9f5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c1508bbf39684554a3d1ac39ca0ae152",
            "value": "‚Äá5/5‚Äá[00:48&lt;00:00,‚Äá‚Äá9.54s/it,‚Äáloss=12.1194,‚ÄáL_FT=9.9724,‚ÄáL_ICE=4.2939]"
          }
        },
        "deaf943e47f84e8cbba94a17a537eff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a3278dc5e3474ab9d5fdd010efe44d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_403e0018e82c4deeb003299a6ca0bda9",
            "value": "‚Äá188/188‚Äá[00:00&lt;00:00,‚Äá21.8kB/s]"
          }
        },
        "df17b3d5562d41258c982fccdfa2a5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df69aa9f1df54ac9b2f984d097358101": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f5767f2da5543f18b6518c8baeaf24e",
              "IPY_MODEL_7b4da715f102428cbc78b4206dfb8838",
              "IPY_MODEL_7e658fcc71de486ba027b8c3cb666dae"
            ],
            "layout": "IPY_MODEL_d459a9ac943e434bb6a5edf96c396565"
          }
        },
        "df99633f8c804964aae34fa39876e7d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e222ae11b8914ffd99e484b0da134935": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2bbd90138164b85a7848cfe015321ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e9dcc503f24cc48375e754ad17016c",
            "max": 3500296424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31e08a7564864efea29d6d37fef9691a",
            "value": 3500296424
          }
        },
        "e2e3b80953f14b2fb0c6b2034b9d5ae3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e32b29249c6b4a458dab6fe0a85180ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4b519969d71463fb8cf8193e4fc82fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e0939ba0bd4791a486997c55f02b20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e61e0f0a7c214d9ead4370e5ef2f511b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ad4fa7a7deb4ea8b74cdc4e550c9760",
            "max": 9976576152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c578879fec44d528ca48b49eed33637",
            "value": 9976576152
          }
        },
        "e94ec59b543d495794cc7a183cd2bcde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b36e9c13ed343f0bad53de1d2c1b462",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b603408d14464748bfbccfabad9683ba",
            "value": 26788
          }
        },
        "eab3654541d247d18aee6cdbe19ab819": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f39a993a30a440d4a3db97d508e76cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f48f7a92c05043c4833a459c048773a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f748c5dbaf974c1c995ae639e09d9b03": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7b9e01c93af44cda8cb0f60f36baada": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24d4bf7781674fda9a84895dd88bc61f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2eaf8546756948c7be2931c4cba3d52c",
            "value": "‚Äá26.8k/26.8k‚Äá[00:00&lt;00:00,‚Äá1.10MB/s]"
          }
        },
        "fa3819708e8849669d3311fd3e0d51a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc16fd23dc7b4c21baaf89c4e0d07ecb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc5164d84174484ca241167a951d1169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03f81462d076462aa158ba104ebbac95",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b5d1d40c39142848e0236f1471ee239",
            "value": 188
          }
        },
        "fcd3a8d229754f429f887c1a0b9c233d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd58d5801a134f908490bfd948734185": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe1cde16ef4e40acb043cb6204ae93c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe43d963438142b199cfac29dd8ce43c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe974d72ff12478ba5cfd44890f29a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a351d450faee40de8d8a0581ced9de40",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e222ae11b8914ffd99e484b0da134935",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "fedd34397a954a4299423ac49f862176": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
